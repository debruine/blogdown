<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lisa DeBruine</title>
    <link>https://debruine.github.io/</link>
      <atom:link href="https://debruine.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Lisa DeBruine</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 23 Sep 2022 09:00:00 +0000</lastBuildDate>
    <image>
      <url>https://debruine.github.io/media/icon_hud41de7153c7fa400a999f8d222dc5c78_8091_512x512_fill_lanczos_center_3.png</url>
      <title>Lisa DeBruine</title>
      <link>https://debruine.github.io/</link>
    </image>
    
    <item>
      <title>Python basics</title>
      <link>https://debruine.github.io/courses/example/python/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/courses/example/python/</guid>
      <description>&lt;p&gt;Build a foundation in Python.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/rfscVS0vtbw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;quiz&#34;&gt;Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;What is the difference between lists and tuples?&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;Lists&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lists are mutable - they can be changed&lt;/li&gt;
&lt;li&gt;Slower than tuples&lt;/li&gt;
&lt;li&gt;Syntax: &lt;code&gt;a_list = [1, 2.0, &#39;Hello world&#39;]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tuples&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tuples are immutable - they can&amp;rsquo;t be changed&lt;/li&gt;
&lt;li&gt;Tuples are faster than lists&lt;/li&gt;
&lt;li&gt;Syntax: &lt;code&gt;a_tuple = (1, 2.0, &#39;Hello world&#39;)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Is Python case-sensitive?&lt;/summary&gt;
  &lt;p&gt;Yes&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Visualization</title>
      <link>https://debruine.github.io/courses/example/visualization/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/courses/example/visualization/</guid>
      <description>&lt;p&gt;Learn how to visualize data with Plotly.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/hSPmj7mK6ng&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;quiz&#34;&gt;Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;When is a heatmap useful?&lt;/summary&gt;
  &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit.&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Write Plotly code to render a bar chart&lt;/summary&gt;
  &lt;p&gt;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import plotly.express as px
data_canada = px.data.gapminder().query(&amp;quot;country == &#39;Canada&#39;&amp;quot;)
fig = px.bar(data_canada, x=&#39;year&#39;, y=&#39;pop&#39;)
fig.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Statistics</title>
      <link>https://debruine.github.io/courses/example/stats/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/courses/example/stats/</guid>
      <description>&lt;p&gt;Introduction to statistics for data science.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;
&lt;p&gt;The general form of the &lt;strong&gt;normal&lt;/strong&gt; probability density function is:&lt;/p&gt;
&lt;p&gt;$$
f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The parameter $\mu$ is the mean or expectation of the distribution.
$\sigma$ is its standard deviation.
The variance of the distribution is $\sigma^{2}$.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;quiz&#34;&gt;Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;What is the parameter $\mu$?&lt;/summary&gt;
  &lt;p&gt;The parameter $\mu$ is the mean or expectation of the distribution.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>How to set up a big team science project</title>
      <link>https://debruine.github.io/talk/how-to-set-up-a-big-team-science-project/</link>
      <pubDate>Fri, 23 Sep 2022 09:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/talk/how-to-set-up-a-big-team-science-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Loops</title>
      <link>https://debruine.github.io/post/loops/</link>
      <pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/loops/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tictoc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tutorial will introduce a few ways you can iterate your code. We’ll use the {tictoc} package to time each method to show how they differ.&lt;/p&gt;
&lt;div id=&#34;task-specifics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Task specifics&lt;/h2&gt;
&lt;p&gt;Let’s say you want to do a power analysis by simulation. You’ll need to simulate some data, run the analysis, and record the relevant p-value. And you’ll need to repeat this procedure a number of times.&lt;/p&gt;
&lt;p&gt;Here are all of the simulation parameters you will need for the examples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predicted data parameters
n1 &amp;lt;- 50
m1 &amp;lt;- 100
sd1 &amp;lt;- 10
n2 &amp;lt;- 45
m2 &amp;lt;- 105
sd2 &amp;lt;- 11

# critical alpha for calculating power
# doesn&amp;#39;t always have to be 0.05 (justify your alpha)
alpha &amp;lt;- 0.05 

# number of simulation replications
n_reps &amp;lt;- 10000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;one-iteration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One iteration&lt;/h2&gt;
&lt;p&gt;Your first task when iterating is to sort out the code for a single iteration. Once that is bug-free and doing what you want, you can repeat it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulate data
data1 &amp;lt;- rnorm(n1, m1, sd1)
data2 &amp;lt;- rnorm(n2, m2, sd2)

# analyse it  
test &amp;lt;- t.test(data1, data2)

# get the relevant p-value
test$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.000158984&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;for-loop&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;For loop&lt;/h2&gt;
&lt;p&gt;Repeat the code above &lt;code&gt;n_reps&lt;/code&gt; times using a for loop. You have to assign the variable &lt;code&gt;i&lt;/code&gt; (or whatever you want to call it) to each item in the vector &lt;code&gt;1:n_reps&lt;/code&gt;, but you don’t necessarily have to use &lt;code&gt;i&lt;/code&gt; in the code. Here, we use it to add the p-value to the vector &lt;code&gt;p&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;for loop&amp;quot;)

p &amp;lt;- c()

for (i in 1:n_reps) {
  data1 &amp;lt;- rnorm(n1, m1, sd1)
  data2 &amp;lt;- rnorm(n2, m2, sd2)
  
  test &amp;lt;- t.test(data1, data2)
  
  p[i] &amp;lt;- test$p.value
}

# calculcate power as % &amp;lt; alpha
power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## for loop: 1.456 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you pre-allocate the vector &lt;code&gt;p&lt;/code&gt;, this can speed up your loops. This means defining an empty vector with a length of &lt;code&gt;n_reps&lt;/code&gt; before you start the loop. This saves the time it takes to delete the vector and make a new, larger one on each iteration. It doesn’t really make much difference for our simple code here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;pre-allocate&amp;quot;)

# pre-allocate vector for p-values
p &amp;lt;- vector(&amp;quot;numeric&amp;quot;, length = n_reps)

for (i in 1:n_reps) {
  data1 &amp;lt;- rnorm(n1, m1, sd1)
  data2 &amp;lt;- rnorm(n2, m2, sd2)
  
  test &amp;lt;- t.test(data1, data2)
  
  p[i] &amp;lt;- test$p.value
}

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## pre-allocate: 1.399 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;replicate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;replicate&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;replicate()&lt;/code&gt; function lets you iterate the exact same code and collect the output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;replicate&amp;quot;)

p &amp;lt;- replicate(n_reps, {
  data1 &amp;lt;- rnorm(n1, m1, sd1)
  data2 &amp;lt;- rnorm(n2, m2, sd2)
  
  test &amp;lt;- t.test(data1, data2)
  
  test$p.value
})

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## replicate: 1.499 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can enclose the relevant code in a named function and replicate that. This can be useful if you need to run different values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;replicate-function&amp;quot;)

# define a simulation function
sim_func &amp;lt;- function(n1, m1, sd1, n2, m2, sd2) {
  data1 &amp;lt;- rnorm(n1, m1, sd1)
  data2 &amp;lt;- rnorm(n2, m2, sd2)
  
  test &amp;lt;- t.test(data1, data2)
  
  test$p.value
}

# repeat the sim function with different sd1 and m2 values
p &amp;lt;- replicate(n_reps, sim_func(50, 100, 9.5, 45, 102, 11))

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## replicate-function: 1.676 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;apply&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;apply&lt;/h2&gt;
&lt;p&gt;The apply functions can be used to iterate over a vector or list. Here, we’re iterating over the vector &lt;code&gt;1:n_reps&lt;/code&gt;. The function needs to have an argument &lt;code&gt;i&lt;/code&gt; for these values, but we don’t actually need to use &lt;code&gt;i&lt;/code&gt; in the function code. Here, we don’t assign the p-value to a vector &lt;code&gt;p&lt;/code&gt; inside the function, but rather return the p-value from the function and the &lt;code&gt;sapply()&lt;/code&gt; function simplifies this into a vector, which is then assigned to &lt;code&gt;p&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;apply&amp;quot;)
p &amp;lt;- sapply(1:n_reps, function(i) {
  data1 &amp;lt;- rnorm(n1, m1, sd1)
  data2 &amp;lt;- rnorm(n2, m2, sd2)
  
  test &amp;lt;- t.test(data1, data2)
  
  test$p.value
})

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## apply: 1.756 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to use the apply functions with a function that takes arguments, either the first argument should be &lt;code&gt;i&lt;/code&gt; for the replication index, or you can repeat the first argument &lt;code&gt;n_reps&lt;/code&gt; times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;apply-function&amp;quot;)

sim_func &amp;lt;- function(i, n1, m1, sd1, n2, m2, sd2) {
  data1 &amp;lt;- rnorm(n1, m1, sd1)
  data2 &amp;lt;- rnorm(n2, m2, sd2)
  
  test &amp;lt;- t.test(data1, data2)
  
  test$p.value
}

# you can only iterate over 1 argument
# the rest must be single values
p &amp;lt;- sapply(1:n_reps, sim_func, 
            n1 = 50, m1 = 100, sd1 = 9.5, 
            n2 = 45, m2 = 102, sd2 = 11)

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## apply-function: 1.687 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also split up the data simulation and analysis steps into two different apply functions. In the first one, you iterate over the vector &lt;code&gt;1:n_reps&lt;/code&gt; and in the second, you iterate over each item in the &lt;code&gt;data&lt;/code&gt; list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;apply2&amp;quot;)

# simulate data as a list of lists
data &amp;lt;- lapply(1:n_reps, function(i) {
  list(
    data1 = rnorm(n1, m1, sd1), 
    data2 = rnorm(n2, m2, sd2)
  )
})

# iterate over list items
p &amp;lt;- sapply(data, function(d) {
  test &amp;lt;- t.test(d$data1, d$data2)
  
  test$p.value
})

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## apply2: 2.098 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you use &lt;code&gt;sapply()&lt;/code&gt; for the data simulation, it will simplify the result into a matrix. Then you need to use &lt;code&gt;apply()&lt;/code&gt; to iterate over the columns of the matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;apply3&amp;quot;)

# simulate data as a matrix
data &amp;lt;- sapply(1:n_reps, function(i) {
  list(
    data1 = rnorm(n1, m1, sd1), 
    data2 = rnorm(n2, m2, sd2)
  )
})

# iterate over columns
p &amp;lt;- apply(data, MARGIN = 2, function(d) {
  test &amp;lt;- t.test(d$data1, d$data2)
  
  test$p.value
})

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## apply3: 1.908 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;mapply()&lt;/code&gt; function takes any number of arguments (the function goes first). You can use this to iterate over different values of the arguments, but this can get a little tricky to keep track of, so I like to organise each iteration in a row of a data frame first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;mapply&amp;quot;)

# make a data frame with 1 row for each replicate
params &amp;lt;- tidyr::crossing(
  rep = 1:2000,
  n1 = 50,
  m1 = 100,
  sd1 = 10,
  n2 = 45,
  m2 = 100:110,
  sd2 = 11
)

sim_func &amp;lt;- function(n1, m1, sd1, n2, m2, sd2) {
  data1 &amp;lt;- rnorm(n1, m1, sd1)
  data2 &amp;lt;- rnorm(n2, m2, sd2)
  
  test &amp;lt;- t.test(data1, data2)
  
  test$p.value
}

p &amp;lt;- mapply(sim_func, 
            n1 = params$n1, m1 = params$m1, sd1 = params$sd1, 
            n2 = params$n2, m2 = params$m2, sd2 = params$sd2)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## mapply: 3.284 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add p to the params table and 
# calculate power for each param combo
power &amp;lt;- params |&amp;gt;
  mutate(p = p) |&amp;gt;
  group_by(n1, n2, m1, m2, sd1, sd2) |&amp;gt;
  summarise(power = mean(p &amp;lt; alpha),
            .groups = &amp;quot;drop&amp;quot;)

# plot power by m2 - m1
ggplot(power, aes(x = m2 - m1, y = power)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(name = &amp;quot;Raw Effect Size&amp;quot;, breaks = 0:10) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_minimal(base_size = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/loops_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;purrr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;purrr&lt;/h2&gt;
&lt;p&gt;The {purrr} package has iteration functions that work a lot like the apply functions, with some extra helpful features (most of which we won’t explore here).&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;map_dbl()&lt;/code&gt; is like &lt;code&gt;sapply()&lt;/code&gt; in that it returns a vector, but more specific in that it requires the results of the function be a double.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;map&amp;quot;)
p &amp;lt;- map_dbl(1:n_reps, function(i) {
  data1 &amp;lt;- rnorm(n1, m1, sd1)
  data2 &amp;lt;- rnorm(n2, m2, sd2)
  
  test &amp;lt;- t.test(data1, data2)
  
  test$p.value
})

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## map: 2.142 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also split up the data simulation and analysis like above. The &lt;code&gt;map()&lt;/code&gt; function returns a list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;map-split&amp;quot;)

# simulate data as a list of lists
data &amp;lt;- map(1:n_reps, function(i) {
  list(
    data1 = rnorm(n1, m1, sd1), 
    data2 = rnorm(n2, m2, sd2)
  )
})

# iterate over list items
p &amp;lt;- map_dbl(data, function(d) {
  test &amp;lt;- t.test(d$data1, d$data2)
  
  test$p.value
})

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## map-split: 1.763 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;pmap()&lt;/code&gt; function is really useful if you want to run a simulation across a lot of different parameters. Use &lt;code&gt;tidyr::crossing()&lt;/code&gt; to make a data frame with one row for each replicate. Add &lt;code&gt;...&lt;/code&gt; to the arguments in the function inside &lt;code&gt;pmap()&lt;/code&gt; so it can ignore any unused columns in the &lt;code&gt;params&lt;/code&gt; table (e.g., &lt;code&gt;rep&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;pmap&amp;quot;)

# make a data frame with 1 row for each replicate
params &amp;lt;- tidyr::crossing(
  rep = 1:n_reps,
  n1 = n1,
  m1 = m1,
  sd1 = sd1,
  n2 = n2,
  m2 = m2,
  sd2 = sd2
)

# simulate data as a list of lists
data &amp;lt;- pmap(params, function(n1, m1, sd1, n2, m2, sd2, ...) {
  list(
    data1 = rnorm(n1, m1, sd1), 
    data2 = rnorm(n2, m2, sd2)
  )
})

# iterate over list items
p &amp;lt;- map_dbl(data, function(d) {
  test &amp;lt;- t.test(d$data1, d$data2)
  
  test$p.value
})

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## pmap: 1.826 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this way, we could explore a range of values, such as how the results change as m2 varies from 100 to 110.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make a data frame with 1 row for each replicate
params &amp;lt;- tidyr::crossing(
  rep = 1:2000,
  n1 = 50,
  m1 = 100,
  sd1 = 10,
  n2 = 45,
  m2 = 100:110,
  sd2 = 11
)


p &amp;lt;- pmap_dbl(params, function(n1, m1, sd1, n2, m2, sd2,...) {
  data1 = rnorm(n1, m1, sd1)
  data2 = rnorm(n2, m2, sd2)

  test &amp;lt;- t.test(data1, data2)
  
  test$p.value
})

# add p to the params table and 
# calculate power for each param combo
power &amp;lt;- params |&amp;gt;
  mutate(p = p) |&amp;gt;
  group_by(n1, n2, m1, m2, sd1, sd2) |&amp;gt;
  summarise(power = mean(p &amp;lt; alpha),
            .groups = &amp;quot;drop&amp;quot;)

# plot power by m2 - m1
ggplot(power, aes(x = m2 - m1, y = power)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(name = &amp;quot;Raw Effect Size&amp;quot;, breaks = 0:10) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_minimal(base_size = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/loops_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;foreach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;foreach&lt;/h2&gt;
&lt;p&gt;If you have a ton of iterations or each is slow, you might want to run the iterations in parallel. These functions use the {foreach} package.&lt;/p&gt;
&lt;p&gt;First, set up the basic loop using the &lt;code&gt;foreach()&lt;/code&gt; function and &lt;code&gt;%do%&lt;/code&gt; syntax. We’ll up the number of replications to 1e5.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(foreach)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;foreach&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:purrr&amp;#39;:
## 
##     accumulate, when&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_reps &amp;lt;- 1e5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;foreach&amp;quot;)

sim_func &amp;lt;- function(i) {
  data1 = rnorm(n1, m1, sd1)
  data2 = rnorm(n2, m2, sd2)

  test &amp;lt;- t.test(data1, data2)
  
  test$p.value
}

p &amp;lt;- foreach(i= 1:n_reps) %do% sim_func(i)

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## foreach: 49.59 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Than change &lt;code&gt;%do%&lt;/code&gt; to &lt;code&gt;%dopar%&lt;/code&gt; to take advantage of parallelisation if your computer has the capacity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set up parallelisation
library(doParallel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: iterators&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: parallel&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;registerDoParallel()
getDoParWorkers() # find out how many workers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;foreach-parallel&amp;quot;)

sim_func &amp;lt;- function(i) {
  data1 = rnorm(n1, m1, sd1)
  data2 = rnorm(n2, m2, sd2)

  test &amp;lt;- t.test(data1, data2)
  
  test$p.value
}

p &amp;lt;- foreach(i= 1:n_reps) %dopar% sim_func(i)

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## foreach-parallel: 26.711 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, you can use &lt;code&gt;times()&lt;/code&gt; like &lt;code&gt;replicate()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic(&amp;quot;times-parallel&amp;quot;)

sim_func &amp;lt;- function() {
  data1 = rnorm(n1, m1, sd1)
  data2 = rnorm(n2, m2, sd2)

  test &amp;lt;- t.test(data1, data2)
  
  test$p.value
}

p &amp;lt;- times(n_reps) %dopar% sim_func()

power &amp;lt;- mean(p &amp;lt; alpha)

toc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## times-parallel: 20.183 sec elapsed&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Simulation Workshops</title>
      <link>https://debruine.github.io/project/datasim/</link>
      <pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/project/datasim/</guid>
      <description>&lt;h2 id=&#34;data-simulation-for-factorial-designs&#34;&gt;Data simulation for factorial designs&lt;/h2&gt;
&lt;p&gt;This session will cover the basics of simulation using {faux}. We will simulate data with factorial designs by specifying the within and between-subjects factor structure, each cell mean and standard deviation, and correlations between cells where appropriate. This can be used to create simulated data sets to be used in preparing the analysis code for pre-registrations or registered reports. We will also create data sets for simulation-based power analyses. Students will need to have very basic knowledge of R and R Markdown, and have installed {faux}, {afex} and {tidyverse}.&lt;/p&gt;
&lt;h2 id=&#34;data-simulation-for-mixed-designs&#34;&gt;Data simulation for mixed designs&lt;/h2&gt;
&lt;p&gt;This session will cover simulating data for a mixed design, where trials are crossed with subjects. We will learn how to analyse this using {lme4}, with a focus on understanding how the simulation parameters correspond to the output. Finally, we will learn how to use simulation to calculate power. Students will need to have basic knowledge of R and R Markdown, some familiarity with mixed designs (even if they don’t currently analyse them with mixed models) and have installed {faux}, {afex}, {tidyverse}, and {lme4}.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Applied Data Skills</title>
      <link>https://debruine.github.io/publication/ads/</link>
      <pubDate>Sat, 07 May 2022 14:49:41 +0100</pubDate>
      <guid>https://debruine.github.io/publication/ads/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Open-source tutorials benefit the field</title>
      <link>https://debruine.github.io/publication/tutorials/</link>
      <pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/publication/tutorials/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data Simulation Workshops</title>
      <link>https://debruine.github.io/talk/data-simulation-workshops/</link>
      <pubDate>Wed, 27 Apr 2022 09:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/talk/data-simulation-workshops/</guid>
      <description></description>
    </item>
    
    <item>
      <title>30 Day Chart Challenge</title>
      <link>https://debruine.github.io/project/30dcc/</link>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/project/30dcc/</guid>
      <description>&lt;p&gt;This is my first year participating in the 30-day chart challenge. I&amp;rsquo;m making a book of tutorials for each chart.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ManyFaces</title>
      <link>https://debruine.github.io/project/manyfaces/</link>
      <pubDate>Wed, 05 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/project/manyfaces/</guid>
      <description>&lt;p&gt;ManyFaces is a new big team science group of face researchers working on collaborative projects. The first two working groups are focussing on building a database of face stimuli and developing protocols for face image capture to facilitate collaborative image sets. There are also topic groups that discuss things like ethics in face research and mixed model methodology.&lt;/p&gt;
&lt;p&gt;See the &lt;a href=&#34;https://docs.google.com/document/d/1X6b3am7zwDYXT2-TUR-a85-xG-cdrE8_vz9GICbwAl0/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;brainstorming doc&lt;/a&gt; for more info and how to join the Slack.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Everything is cool when you&#39;re part of a team</title>
      <link>https://debruine.github.io/talk/everything-is-cool-when-youre-part-of-a-team/</link>
      <pubDate>Tue, 19 Oct 2021 16:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/talk/everything-is-cool-when-youre-part-of-a-team/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data from Images</title>
      <link>https://debruine.github.io/post/data-from-images/</link>
      <pubDate>Sun, 17 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/data-from-images/</guid>
      <description>
&lt;script src=&#34;https://debruine.github.io/post/data-from-images/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://debruine.github.io/post/data-from-images/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://debruine.github.io/post/data-from-images/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;I saw this a few days ago and sympathised, because I’ve often wanted to use data that is trapped in images or PDFs. If it’s not too much, I’ll manually transcribe it, but that’s so tedious!&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
crucial data being published as images … 😭😭😭😭 &lt;a href=&#34;https://t.co/9CKg9NhYtA&#34;&gt;https://t.co/9CKg9NhYtA&lt;/a&gt;
&lt;/p&gt;
— Shel 🇰🇪 (&lt;span class=&#34;citation&#34;&gt;@Shel_Kariuki&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/Shel_Kariuki/status/1449029768763084814?ref_src=twsrc%5Etfw&#34;&gt;October 15, 2021&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;A tweet by &lt;a href=&#34;https://twitter.com/abiyugiday/status/1449257787821789184?s=20&#34;&gt;Abiyu Giday&lt;/a&gt; reminded me that the &lt;code&gt;magick&lt;/code&gt; package has OCR (optical character recognition), so I decided to try it out.&lt;/p&gt;
&lt;div id=&#34;required-packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Required packages&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magick)    # for image processing
library(tesseract) # for OCR image reading
library(dplyr)     # for data wrangling and pipes
library(tidyr)     # for data wrangling
library(stringr)   # for string manipulation
library(ggplot2)   # for plots&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;read-the-image&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Read the image&lt;/h3&gt;
&lt;p&gt;First, you need to read in the image and convert it to text using two functions from the &lt;a href=&#34;https://cran.r-project.org/web/packages/magick/vignettes/intro.html&#34;&gt;&lt;code&gt;magick&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;I use &lt;a href=&#34;https://imagemagick.org/&#34;&gt;ImageMagick&lt;/a&gt; for webmorph development, so had it installed previously. I’m not sure if installing the R package also sets up the ImageMagick installation. If you’re on a Windows machine, the easiest way is to use &lt;code&gt;installr::install.imagemagick()&lt;/code&gt;. On a Mac, you can install it with &lt;a href=&#34;https://brew.sh/&#34;&gt;Homebrew&lt;/a&gt; using &lt;code&gt;brew install imagemagick@6&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now You can read in the image with &lt;code&gt;image_read()&lt;/code&gt; and run OCR on it with &lt;code&gt;image_ocr()&lt;/code&gt;. If you haven’t installed the &lt;code&gt;tesseract&lt;/code&gt; package yet, the second function will prompt you to.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# original image source:
# https://pbs.twimg.com/media/FBv8P8XXEBgCBvS?format=jpg&amp;amp;name=medium
imgtext &amp;lt;- magick::image_read(&amp;quot;data_image.jpg&amp;quot;) %&amp;gt;%
  magick::image_ocr()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;process-the-text&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Process the text&lt;/h3&gt;
&lt;p&gt;The result is a single &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A data type representing strings of text.&#39; href=&#39;https://psyteachr.github.io/glossary/c#character&#39;&gt;character&lt;/a&gt; string that looks like this, so we’re going to need to do quite a bit of processing to get it into a &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;Data in a rectangular table format, where each row has an entry for each column.&#39; href=&#39;https://psyteachr.github.io/glossary/t#tabular-data&#39;&gt;tabular&lt;/a&gt; format.&lt;/p&gt;
&lt;pre&gt;
rs
COUNTY VACCINES AL |

1. Nairobi 957,147 (31.4%) 25. Homa Bay 33,290 (5.5%)
2. Kiambu 298,723 (18.4%) 26. Migori 32,670 (5.9%)
3. Nakuru 170,684 (13.4%) 27. Kilifi 31,611 (4.0%)
4. Nyeri 134,166 (26.3%) 28. Kisii 30,204 (4.3%)
5. Murang’a 110,825 (16.4%) 29. Nyamira 29,142 (8.5%)
6. Machakos 100,671 (11.1%) 30. Busia 26,792 (5.8%)
7. Uasin Gishu 92,142 (13.3%) 31. Vihiga 25,172 (7.6%)
8. Kisumu 90,495 (13.8%) 32. Tharaka Nithi 24,386 (9.9%)
9. Mombasa 82,814 (10.3%) 33. Baringo 21,176 (6.2%)
10. Kirinyaga 81,233 (19.6%) 34. Bomet 20,885 (4.5%)
ll. Kajiado 75,960 (11.5%) 35. Elgeyo Marakwet 17,574 (7.2%)
12. Bungoma 66,688 (7.9%) 36. Kwale 17,185 (3.8%)
13. Meru 66,270 (7.0%) 37. Narok 15,410 (2.8%)
14. Kakamega 62,043 (6.3%) 38. Turkana 9,249 (2.0%)
15. Nyandarua 60,574 (16.1%) 39. West Pokot 8,207 (2.9%)
16. Laikipia 58,141 (19.0%) 40. Garissa 7,694 (1.9%)
17. Makueni 57,435 (9.8%) 41. Samburu 6,686 (4.6%)
18. Embu 56,082 (14.2%) 42. Mandera 6,220 (1.8%)
19. Trans Nzoia 45,228 (8.7%) 43. Isiolo 5,653 (4.2%)
20. Kitui 40,663 (6.5%) 44. Wajir 5,003 (1.5%)
21. Kericho 38,497 (7.6%) 45. Lamu 4,692 (5.6%)
22. Siaya 38,313 (7.1%) 46. Tana River 3,440 (2.3%)
23. Nandi 38,243 (7.8%) 47. Marsabit 2,953 ( 1.3%)
24. Taita Taveta 34,478 (16.2%)

&lt;/pre&gt;
&lt;p&gt;First, I’ll get rid of the first three lines.&lt;/p&gt;
&lt;p&gt;You need to put two backslashes before the &lt;code&gt;&#34;\\|&#34;&lt;/code&gt; to mark it as a literal &lt;code&gt;|&lt;/code&gt;, since &lt;code&gt;|&lt;/code&gt; has a special meaning in regular expressions (it means “or”). This is called &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;Include special characters like &#34; inside of a string by prefacing them with a backslash.&#39; href=&#39;https://psyteachr.github.io/glossary/e#escape&#39;&gt;escaping&lt;/a&gt; the character. The combination &lt;code&gt;&#34;\n&#34;&lt;/code&gt; represents a line break.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trimmed &amp;lt;- gsub(&amp;quot;rs\nCOUNTY VACCINES AL \\|\n\n&amp;quot;, &amp;quot;&amp;quot;, imgtext)

trimmed&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1. Nairobi 957,147 (31.4%) 25. Homa Bay 33,290 (5.5%)\n2. Kiambu 298,723 (18.4%) 26. Migori 32,670 (5.9%)\n3. Nakuru 170,684 (13.4%) 27. Kilifi 31,611 (4.0%)\n4. Nyeri 134,166 (26.3%) 28. Kisii 30,204 (4.3%)\n5. Murang’a 110,825 (16.4%) 29. Nyamira 29,142 (8.5%)\n6. Machakos 100,671 (11.1%) 30. Busia 26,792 (5.8%)\n7. Uasin Gishu 92,142 (13.3%) 31. Vihiga 25,172 (7.6%)\n8. Kisumu 90,495 (13.8%) 32. Tharaka Nithi 24,386 (9.9%)\n9. Mombasa 82,814 (10.3%) 33. Baringo 21,176 (6.2%)\n10. Kirinyaga 81,233 (19.6%) 34. Bomet 20,885 (4.5%)\nll. Kajiado 75,960 (11.5%) 35. Elgeyo Marakwet 17,574 (7.2%)\n12. Bungoma 66,688 (7.9%) 36. Kwale 17,185 (3.8%)\n13. Meru 66,270 (7.0%) 37. Narok 15,410 (2.8%)\n14. Kakamega 62,043 (6.3%) 38. Turkana 9,249 (2.0%)\n15. Nyandarua 60,574 (16.1%) 39. West Pokot 8,207 (2.9%)\n16. Laikipia 58,141 (19.0%) 40. Garissa 7,694 (1.9%)\n17. Makueni 57,435 (9.8%) 41. Samburu 6,686 (4.6%)\n18. Embu 56,082 (14.2%) 42. Mandera 6,220 (1.8%)\n19. Trans Nzoia 45,228 (8.7%) 43. Isiolo 5,653 (4.2%)\n20. Kitui 40,663 (6.5%) 44. Wajir 5,003 (1.5%)\n21. Kericho 38,497 (7.6%) 45. Lamu 4,692 (5.6%)\n22. Siaya 38,313 (7.1%) 46. Tana River 3,440 (2.3%)\n23. Nandi 38,243 (7.8%) 47. Marsabit 2,953 ( 1.3%)\n24. Taita Taveta 34,478 (16.2%)\n&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;split-into-rows&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Split into rows&lt;/h3&gt;
&lt;p&gt;Since there are two rows of data on each row, I’ll convert all of the line breaks (&lt;code&gt;&#34;\n&#34;&lt;/code&gt;) into spaces and then split the result wherever there is 0 or 1 spaces (&lt;code&gt;&#34; ?&#34;&lt;/code&gt;), followed by 1 or more digits (&lt;code&gt;&#34;[0-9]+&#34;&lt;/code&gt;), followed by a full stop and a space (&lt;code&gt;&#34;\\. &#34;&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;split_base &amp;lt;- gsub(&amp;quot;\n&amp;quot;, &amp;quot; &amp;quot;, trimmed) %&amp;gt;%
  strsplit(&amp;quot; ?[0-9]+\\. &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you prefer &lt;code&gt;stringr&lt;/code&gt; functions to base functions, you can do it this way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;split_stringr &amp;lt;- trimmed %&amp;gt;%
  stringr::str_replace(&amp;quot;\n&amp;quot;, &amp;quot; &amp;quot;) %&amp;gt;%
  stringr::str_split(&amp;quot; ?s[0-9]+\\. &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fix-encoding-errors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fix encoding errors&lt;/h3&gt;
&lt;p&gt;Make sure you look through all of your data at this point. The first time I did this, I didn’t notice that &lt;code&gt;11.&lt;/code&gt; got read in as &lt;code&gt;ll.&lt;/code&gt;, so line 21 didn’t split.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;split_base[[1]][21]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Bomet 20,885 (4.5%) ll. Kajiado 75,960 (11.5%)&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can fix that by replacing &lt;code&gt;&#34;ll. &#34;&lt;/code&gt; with &lt;code&gt;&#34;11. &#34;&lt;/code&gt; before you split the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;split_base &amp;lt;- trimmed %&amp;gt;%
  gsub(&amp;quot;ll. &amp;quot;, &amp;quot;11. &amp;quot;, .) %&amp;gt;%
  gsub(&amp;quot;\n&amp;quot;, &amp;quot; &amp;quot;, .) %&amp;gt;%
  strsplit(&amp;quot; ?[0-9]+\\. &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tabular-format&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tabular format&lt;/h3&gt;
&lt;p&gt;Now we need to get this into a tabular format. The objects &lt;code&gt;split_base&lt;/code&gt; and &lt;code&gt;split_stringr&lt;/code&gt; are 1-item &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A container data type that allows items with different data types to be grouped together.&#39; href=&#39;https://psyteachr.github.io/glossary/l#list&#39;&gt;lists&lt;/a&gt; where the first item contains the &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A type of data structure that collects values with the same data type, like T/F values, numbers, or strings.&#39; href=&#39;https://psyteachr.github.io/glossary/v#vector&#39;&gt;vector&lt;/a&gt; of our rows. The first row is blank (the content before the first split at &lt;code&gt;1.&lt;/code&gt;) so we have to omit that. The code below creates a data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data1 &amp;lt;- data.frame(x = split_base[[1]][-1]) &lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
x
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nairobi 957,147 (31.4%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Homa Bay 33,290 (5.5%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kiambu 298,723 (18.4%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Migori 32,670 (5.9%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nakuru 170,684 (13.4%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kilifi 31,611 (4.0%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;separate-columns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Separate columns&lt;/h3&gt;
&lt;p&gt;Now we have to separate the columns out. There are several ways to do this. If you’re a regex wizard, you don’t need the rest of this tutorial, so I’m going to break it into smaller steps instead. Using &lt;code&gt;gsub()&lt;/code&gt; to create new columns by replacing parts of the original column is a straightforward way (HT &lt;a href=&#34;https://twitter.com/_TanHo&#34;&gt;Tan Ho&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Create the &lt;code&gt;county&lt;/code&gt; column by replacing all characters from the space before the first digit (&lt;code&gt;[0-9]&lt;/code&gt;) plus any characters after that (&lt;code&gt;.*&lt;/code&gt;) until the end of the string (&lt;code&gt;$&lt;/code&gt;). Create the &lt;code&gt;number&lt;/code&gt; column by replacing from the start of the string (&lt;code&gt;^&lt;/code&gt;) plus any non-numbers (&lt;code&gt;[^0-9]+&lt;/code&gt;) that follow it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data2 &amp;lt;- data1 %&amp;gt;%
  mutate(county = gsub(&amp;quot; [0-9].*$&amp;quot;, &amp;quot;&amp;quot;, x),
         number = gsub(&amp;quot;^[^0-9]+&amp;quot;, &amp;quot;&amp;quot;, x))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
x
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
county
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
number
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nairobi 957,147 (31.4%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nairobi
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
957,147 (31.4%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Homa Bay 33,290 (5.5%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Homa Bay
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
33,290 (5.5%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kiambu 298,723 (18.4%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kiambu
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
298,723 (18.4%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Migori 32,670 (5.9%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Migori
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
32,670 (5.9%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nakuru 170,684 (13.4%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nakuru
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
170,684 (13.4%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kilifi 31,611 (4.0%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kilifi
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
31,611 (4.0%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The &lt;code&gt;county&lt;/code&gt; column looks fine, but the &lt;code&gt;number&lt;/code&gt; column needs to be split into the number of vaccinations and the percent. Use the &lt;code&gt;separate()&lt;/code&gt; function to split this column on the left parenthesis (remember to &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;Include special characters like &#34; inside of a string by prefacing them with a backslash.&#39; href=&#39;https://psyteachr.github.io/glossary/e#escape&#39;&gt;escape&lt;/a&gt; it).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data3 &amp;lt;- data2 %&amp;gt;%
  separate(col = number, 
           into = c(&amp;quot;number&amp;quot;, &amp;quot;percent&amp;quot;), 
           sep = &amp;quot;\\(&amp;quot;,
           extra = &amp;quot;drop&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
x
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
county
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
number
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
percent
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nairobi 957,147 (31.4%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nairobi
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
957,147
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
31.4%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Homa Bay 33,290 (5.5%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Homa Bay
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
33,290
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5.5%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kiambu 298,723 (18.4%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kiambu
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
298,723
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
18.4%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Migori 32,670 (5.9%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Migori
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
32,670
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5.9%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nakuru 170,684 (13.4%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nakuru
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
170,684
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
13.4%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kilifi 31,611 (4.0%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kilifi
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
31,611
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
4.0%)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-up-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Clean up the data&lt;/h3&gt;
&lt;p&gt;Now we just need to clean up some extra characters in the &lt;code&gt;number&lt;/code&gt; and &lt;code&gt;percent&lt;/code&gt; columns. Get rid of the comma in the &lt;code&gt;number&lt;/code&gt; column and the percent sign and right parenthesis in the &lt;code&gt;percent&lt;/code&gt; column (remember to escape the parenthesis).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data4 &amp;lt;- data3 %&amp;gt;%
  mutate(number = gsub(&amp;quot;,&amp;quot;, &amp;quot;&amp;quot;, number),
         percent = gsub(&amp;quot;%\\)&amp;quot;, &amp;quot;&amp;quot;, percent)) %&amp;gt;%
  select(-x)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
county
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
number
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
percent
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nairobi
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
957147
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
31.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Homa Bay
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
33290
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kiambu
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
298723
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
18.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Migori
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
32670
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nakuru
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
170684
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
13.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kilifi
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
31611
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
4.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;check-data-types&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Check data types&lt;/h3&gt;
&lt;p&gt;This looks good, but there is still a problem. We can’t do anything useful with this data set because the &lt;code&gt;number&lt;/code&gt; and &lt;code&gt;percent&lt;/code&gt; columns are actually still character &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;The kind of data represented by an object.&#39; href=&#39;https://psyteachr.github.io/glossary/d#data-type&#39;&gt;data types&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(data4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     county             number            percent         
##  Length:47          Length:47          Length:47         
##  Class :character   Class :character   Class :character  
##  Mode  :character   Mode  :character   Mode  :character&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- mean(data4$number)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in mean.default(data4$number): argument is not numeric or logical:
## returning NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Convert the &lt;code&gt;number&lt;/code&gt; column to an &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A data type representing whole numbers.&#39; href=&#39;https://psyteachr.github.io/glossary/i#integer&#39;&gt;integer&lt;/a&gt; and the &lt;code&gt;percent&lt;/code&gt; column to a &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A data type representing a real decimal number&#39; href=&#39;https://psyteachr.github.io/glossary/d#double&#39;&gt;double&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- data4 %&amp;gt;%
  mutate(number = as.integer(number),
         percent = as.double(percent))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
county
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
number
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
percent
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nairobi
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
957147
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Homa Bay
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33290
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kiambu
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
298723
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Migori
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
32670
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nakuru
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
170684
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kilifi
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31611
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now you’re ready to plot the data or use it in analyses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data, aes(x = number, y = percent)) +
  geom_point(color = &amp;quot;dodgerblue&amp;quot;) +
  scale_x_continuous(&amp;quot;Number of Vaccinated People&amp;quot;,
                     breaks = seq(0, 1e6, 2e5),
                     labels = c(paste0(seq(0, 800, 200), &amp;quot;K&amp;quot;), &amp;quot;1M&amp;quot;),
                     limits = c(0, 1e6)) +
  scale_y_continuous(&amp;quot;Percent of County&amp;quot;,
                     breaks = seq(0, 40, 10),
                     labels = paste0(seq(0, 40, 10), &amp;quot;%&amp;quot;)) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/data-from-images/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;glossary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Glossary&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
term
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
definition
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://psyteachr.github.io/glossary/c.html#character&#34; class=&#34;glossary&#34; target=&#34;_blank&#34;&gt;character&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A data type representing strings of text.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://psyteachr.github.io/glossary/d.html#data-type&#34; class=&#34;glossary&#34; target=&#34;_blank&#34;&gt;data type&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
The kind of data represented by an object.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://psyteachr.github.io/glossary/d.html#double&#34; class=&#34;glossary&#34; target=&#34;_blank&#34;&gt;double&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A data type representing a real decimal number
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://psyteachr.github.io/glossary/e.html#escape&#34; class=&#34;glossary&#34; target=&#34;_blank&#34;&gt;escape&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Include special characters like &#34; inside of a string by prefacing them with a backslash.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://psyteachr.github.io/glossary/i.html#integer&#34; class=&#34;glossary&#34; target=&#34;_blank&#34;&gt;integer&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A data type representing whole numbers.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://psyteachr.github.io/glossary/l.html#list&#34; class=&#34;glossary&#34; target=&#34;_blank&#34;&gt;list&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A container data type that allows items with different data types to be grouped together.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://psyteachr.github.io/glossary/t.html#tabular-data&#34; class=&#34;glossary&#34; target=&#34;_blank&#34;&gt;tabular data&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Data in a rectangular table format, where each row has an entry for each column.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;a href=&#34;https://psyteachr.github.io/glossary/v.html#vector&#34; class=&#34;glossary&#34; target=&#34;_blank&#34;&gt;vector&lt;/a&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A type of data structure that collects values with the same data type, like T/F values, numbers, or strings.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An ounce of prevention is worth a pound of cure</title>
      <link>https://debruine.github.io/talk/an-ounce-of-prevention-is-worth-a-pound-of-cure/</link>
      <pubDate>Wed, 13 Oct 2021 15:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/talk/an-ounce-of-prevention-is-worth-a-pound-of-cure/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mixed effects models for designs with randomly sampled stimuli</title>
      <link>https://debruine.github.io/talk/mixed-effects-models-for-designs-with-randomly-sampled-stimuli/</link>
      <pubDate>Tue, 05 Oct 2021 16:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/talk/mixed-effects-models-for-designs-with-randomly-sampled-stimuli/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Faux</title>
      <link>https://debruine.github.io/project/faux/</link>
      <pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/project/faux/</guid>
      <description>&lt;p&gt;It is useful to be able to simulate data with a specified structure. The faux package provides some functions to make this process easier for factorial designs and single- or multilevel data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Skills for Reproducible Research</title>
      <link>https://debruine.github.io/publication/reprores/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/publication/reprores/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data visualisation using R, for researchers who don&#39;t use R</title>
      <link>https://debruine.github.io/publication/dataviz/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/publication/dataviz/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Improving Transparency, Falsifiability, and Rigor by Making Hypothesis Tests Machine-Readable</title>
      <link>https://debruine.github.io/publication/scienceverse/</link>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/publication/scienceverse/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Understanding mixed effects models through data simulation</title>
      <link>https://debruine.github.io/publication/lmem-ampps/</link>
      <pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/publication/lmem-ampps/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Testing for normality</title>
      <link>https://debruine.github.io/post/normality/</link>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/normality/</guid>
      <description>


&lt;p&gt;You’ve probably been directed here because you asked someone about how to test the normality of predictors in an analysis. However, statistical tests like t-tests, ANOVAs, and other &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A mathematical model comparing how one or more independent variables affect a continuous dependent variable&#39; href=&#39;https://psyteachr.github.io/glossary/g#general-linear-model&#39;&gt;GLM&lt;/a&gt;-based tests assume that the &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;That part of an observation that cannot be captured by the statistical model, and thus is assumed to reflect unknown factors.&#39; href=&#39;https://psyteachr.github.io/glossary/r#residual-error&#39;&gt;residuals&lt;/a&gt; will be normally distributed and it doesn’t matter at all if the &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A variable whose value is used (in a model) to predict the value of a response variable.&#39; href=&#39;https://psyteachr.github.io/glossary/p#predictor-variable&#39;&gt;predictors&lt;/a&gt; and even the &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;The target variable that is being analyzed, whose value is assumed to depend on other variables.&#39; href=&#39;https://psyteachr.github.io/glossary/d#dependent-variable&#39;&gt;dependent variable&lt;/a&gt; aren’t.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse) # for data wrangling
library(faux)      # for data simulation
library(afex)      # for anova
library(cowplot)   # for dataviz
set.seed(8675309)  # to make sure simulation values don&amp;#39;t vary between runs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this blog post, I’m going to use data simulation to show you how you can visualise the normality of residuals with &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A scatterplot created by plotting two sets of quantiles against each other, used to check if data come from a specified distribution&#39; href=&#39;https://psyteachr.github.io/glossary/q#q-q-plot&#39;&gt;QQ plots&lt;/a&gt;. We’re going to simulate data from a totally hypothetical population of ferrets and cats. We’re going to try to predict the energy levels of these pets from their weight. In my limited experience, tiny ferrets are way more energetic than big ferrets. I know nothing about cats.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://debruine.github.io/images/darwin_oy.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Tiny, energetic Darwin and her big, lazy brother, Oy&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulate-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulate Data&lt;/h2&gt;
&lt;p&gt;We’ll use &lt;a href=&#34;https://debruine.github.io/faux/&#34;&gt;faux&lt;/a&gt; to simulate data based on data parameters like means, SDs and correlations for each group. At the moment, faux can only simulate multivariate normal distributions and then you can convert them to other distributions. So we’ll simulate weights from a &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A symmetric distribution of data where values near the centre are most probable.&#39; href=&#39;https://psyteachr.github.io/glossary/n#normal-distribution&#39;&gt;normal distribution&lt;/a&gt; with a mean of 0 and SD of 1, and then convert these to a &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;All numbers in the range have an equal probability of being sampled&#39; href=&#39;https://psyteachr.github.io/glossary/u#uniform-distribution&#39;&gt;uniform distribution&lt;/a&gt; for each pet type based on ranges I found online. Energy will be simulated from normal distributions with different means and SDs for cats and ferrets. Energy will be uncorrelated with weight for cats and negatively correlated for ferrets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- faux::sim_design(
  within = list(vars = c(&amp;quot;weight&amp;quot;, &amp;quot;energy&amp;quot;)),
  between = list(species = c(&amp;quot;cat&amp;quot;, &amp;quot;ferret&amp;quot;)),
  n = 50,
  mu = list(weight = c(cat = 0, ferret = 0),
            energy = c(cat = 50, energy = 100)),
  sd = list(weight = c(cat = 1, ferret = 1),
            energy = c(cat = 15, energy = 20)),
  r = list(cat = 0, ferret = -0.5),
  plot = FALSE
) %&amp;gt;%
  mutate(weight = case_when(
    species == &amp;quot;cat&amp;quot; ~ norm2unif(weight, 3.6, 4.5),
    species == &amp;quot;ferret&amp;quot; ~ norm2unif(weight, 0.7, 2.0)
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;N.B. If you’re more used to &lt;a href=&#34;https://debruine.github.io/tutorials/sim-data.html#intercept-model&#34;&gt;simulating data using model parameters&lt;/a&gt;, this way might make more sense to you, but it’s often difficult to figure out what the &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A value that describes a distribution, such as the mean or SD&#39; href=&#39;https://psyteachr.github.io/glossary/p#parameter&#39;&gt;parameters&lt;/a&gt; should be if you don’t already have pilot data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 50

# values approximated from an lm analysis
b_0  &amp;lt;-  92 # intercept
b_w  &amp;lt;- -13 # fixed effect of weight
b_s  &amp;lt;-  85 # fixed effect of species
b_ws &amp;lt;- -26 # weight*species interaction
err_sd &amp;lt;- 16 # SD of error term

# simulate populations of cats and ferrets 
# with weights from uniform distributions
cat &amp;lt;- data.frame(
  id = paste0(&amp;quot;C&amp;quot;, 1:n),
  species = &amp;quot;cat&amp;quot;,
  weight = runif(n, 3.6, 4.5)
)

ferret &amp;lt;- data.frame(
  id = paste0(&amp;quot;F&amp;quot;, 1:n),
  species = &amp;quot;ferret&amp;quot;,
  weight = runif(n, 0.7, 2.0)
)

# join data and calculate DV based on GLM
data &amp;lt;- bind_rows(cat, ferret) %&amp;gt;%
  mutate(
    # effect-code species
    species.e = recode(species, cat = -0.5, ferret = 0.5),
    # simulate error term
    err = rnorm(2*n, 0, err_sd),
    # calculate DV
    energy = b_0 + species.e*b_s + weight*b_w + 
             species.e*weight*b_ws + err
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/normality_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So weight is bimodal and made of two uniform distributions, while energy is bimodal and made of two normal distributions.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://debruine.github.io/posts/normality_files/figure-html/unnamed-chunk-4-1.png&#34; alt=&#34;Distibutions overall and within species.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Distibutions overall and within species.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you run a Shapiro-Wilk test on these variables, you’d conclude they are &lt;em&gt;definitely&lt;/em&gt; not normally distributed, but this doesn’t matter at all!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shapiro.test(data$energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  data$energy
## W = 0.95486, p-value = 0.001759&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shapiro.test(data$weight)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  data$weight
## W = 0.82694, p-value = 1.821e-09&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-residuals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calculate Residuals&lt;/h2&gt;
&lt;p&gt;We will predict energy from weight, species, and their interaction using a &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A mathematical model comparing how one or more independent variables affect a continuous dependent variable&#39; href=&#39;https://psyteachr.github.io/glossary/g#general-linear-model&#39;&gt;linear model&lt;/a&gt;. We’ll &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A coding scheme for categorical variables that contrasts each group mean with the mean of all the group means.&#39; href=&#39;https://psyteachr.github.io/glossary/e#effect-code&#39;&gt;effect code&lt;/a&gt; species to make the output more similar to what you’d get from ANOVA (and it doesn’t really make sense to &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A coding scheme for categorical variables that creates (n_levels -1) dichotomous variables where each level of the categorical variable is contrasted to a reference level.&#39; href=&#39;https://psyteachr.github.io/glossary/t#treatment-code&#39;&gt;treatment code&lt;/a&gt; them, since neither cats nor ferrets are a meaningful “baseline”).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# effect-code species
data$species.e &amp;lt;- recode(data$species, cat = -0.5, ferret = 0.5)

m1 &amp;lt;- lm(energy ~ weight*species.e, data = data)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std.error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;85.414&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16.204&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.271&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;weight&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-7.928&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.674&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.696&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.093&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;species.e&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;76.821&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32.407&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.370&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.020&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;weight:species.e&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-18.107&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.348&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.937&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.056&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can use the &lt;code&gt;resid()&lt;/code&gt; function to get the &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;That part of an observation that cannot be captured by the statistical model, and thus is assumed to reflect unknown factors.&#39; href=&#39;https://psyteachr.github.io/glossary/r#residual-error&#39;&gt;residual error&lt;/a&gt; term from your model. This is the difference between the predicted value (based on the weight and species for each subject and the model parameters) and the actual value. Those values should be normally distributed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;err &amp;lt;- resid(m1)

ggplot() + geom_density(aes(err))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/normality_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shapiro-wilk&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shapiro-Wilk&lt;/h2&gt;
&lt;p&gt;I don’t recommend using statistical tests for normality. Essentially, they are underpowered in small samples and overpowered in large samples. &lt;a href=&#34;https://towardsdatascience.com/stop-testing-for-normality-dba96bb73f90&#34;&gt;Robert Greener has a good discussion of this.&lt;/a&gt;. However, the residuals do “pass” the Shapiro-Wilk normality test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shapiro.test(err)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  err
## W = 0.99579, p-value = 0.9905&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;qq-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;QQ plots&lt;/h2&gt;
&lt;p&gt;It’s better to assess normality visually, but it’s quite hard to judge normality from a density plot, especially when you have small samples, so we can use a &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;A scatterplot created by plotting two sets of quantiles against each other, used to check if data come from a specified distribution&#39; href=&#39;https://psyteachr.github.io/glossary/q#q-q-plot&#39;&gt;QQ plot&lt;/a&gt; to visualise how close a distribution is to normal. This is a scatterplot created by plotting two sets of &lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; title=&#39;Cutoffs dividing the range of a distribution into continuous intervals with equal probabilities.&#39; href=&#39;https://psyteachr.github.io/glossary/q#quantile&#39;&gt;quantiles&lt;/a&gt; against each other, used to check if data come from a specified distribution (here the normal distribution).&lt;/p&gt;
&lt;p&gt;These data are simulated, so will show an almost perfect straight line. Real data are always a bit messier. But even here, the points at the extremes are often not exactly on the line. It takes practice to tell if a QQ-plot shows clear signs of non-normality.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot function for more customisation
qplot(sample = err) + 
  stat_qq_line(colour = &amp;quot;dodgerblue&amp;quot;) +
  labs(x = &amp;quot;Theoretical distribution&amp;quot;,
       y = &amp;quot;Sample distribution&amp;quot;,
       title = &amp;quot;QQ Plot for Residual Error&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/normality_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our bimodal energy data are a good example of a QQ plot showing a non-normal distribution (see how the points move away from the line at the ends), but that doesn’t matter for your model at all.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data, aes(sample = energy)) +
  stat_qq() +
  stat_qq_line(colour = &amp;quot;dodgerblue&amp;quot;) +
  labs(x = &amp;quot;Theoretical distribution&amp;quot;,
       y = &amp;quot;Sample distribution&amp;quot;,
       title = &amp;quot;QQ Plot for Energy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/normality_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other tests&lt;/h2&gt;
&lt;p&gt;So how do you get the residuals for other tests? All functions that return models in R should have a &lt;code&gt;resid()&lt;/code&gt; function. T-tests are a little trickier, but you can just convert them to their GLM equivalents (&lt;a href=&#34;https://lindeloev.github.io/tests-as-linear/&#34;&gt;Jonas Lindeløv has a great tutorial&lt;/a&gt;) or use the formulas below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulated data to use below
A &amp;lt;- rnorm(50, 0, 1)
B &amp;lt;- rnorm(50, 0.5, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;one-sample-t-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;One-sample t-test&lt;/h3&gt;
&lt;p&gt;The residuals for a one-samples t-test are the scores minus the mean difference. (You don’t &lt;em&gt;have to&lt;/em&gt; subtract the mean difference, since the distribution won’t change if you add a constant value.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# one-sample t-test against 0
mu = 0
t_o &amp;lt;- t.test(A, mu = mu)
err_t &amp;lt;- A - mean(A)
plot_t &amp;lt;- qplot(sample = err_t) + stat_qq_line()

# lm equivalent to one-sample t-test
m_o &amp;lt;- lm(A - mu ~ 1)
err_lm &amp;lt;- resid(m_o)
plot_lm &amp;lt;- qplot(sample = err_lm) + stat_qq_line()

cowplot::plot_grid(plot_t, plot_lm, labels = c(&amp;quot;t&amp;quot;, &amp;quot;lm&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/normality_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;paired-samples-t-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Paired samples t-test&lt;/h3&gt;
&lt;p&gt;The residuals for a paired-samples t-test are the difference between the paired values, minus the mean difference.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# paired samples t-test
t_p &amp;lt;- t.test(A, B, paired = TRUE)
diff &amp;lt;- A - B
err_t &amp;lt;- diff - mean(diff)
plot_t &amp;lt;- qplot(sample = err_t) + stat_qq_line()

# lm equivalent to paired-samples t-test
m_p &amp;lt;- lm(A-B ~ 1)
err_lm &amp;lt;- resid(m_p)
plot_lm &amp;lt;- qplot(sample = err_lm) + stat_qq_line()

cowplot::plot_grid(plot_t, plot_lm, labels = c(&amp;quot;t&amp;quot;, &amp;quot;lm&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/normality_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;independent-samples-t-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Independent-samples t-test&lt;/h3&gt;
&lt;p&gt;The residuals for an independent-samples t-test are the scores minus their own group mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# independent-sample t-test
t_i &amp;lt;- t.test(A, B)
err_t &amp;lt;- c(A-mean(A), B-mean(B))
plot_t &amp;lt;- qplot(sample = err_t) + stat_qq_line()

# lm equivalent to one-sample t-test
dat &amp;lt;- data.frame(
  val = c(A, B),
  grp = rep(0:1, each = 50)
)

m_o &amp;lt;- lm(val ~ 1 + grp, dat)
err_lm &amp;lt;- resid(m_o)
plot_lm &amp;lt;- qplot(sample = err_lm) + stat_qq_line()

cowplot::plot_grid(plot_t, plot_lm, labels = c(&amp;quot;t&amp;quot;, &amp;quot;lm&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/normality_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;anova&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ANOVA&lt;/h3&gt;
&lt;p&gt;You can use the &lt;code&gt;resid()&lt;/code&gt; function on the models output by ANOVAs or ANCOVAs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m_aov &amp;lt;- afex::aov_4(energy ~ weight*species.e + (1|id),
  data = data,
  factorize = FALSE
)
plot_aov &amp;lt;- qplot(sample = resid(m_aov)) + stat_qq_line()

m_lm &amp;lt;- lm(energy ~ weight*species.e, data = data)
plot_lm &amp;lt;- qplot(sample = resid(m_lm)) + stat_qq_line()

cowplot::plot_grid(plot_aov, plot_lm, labels = c(&amp;quot;aov&amp;quot;, &amp;quot;lm&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/normality_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Dale Barr has a great blog post on &lt;a href=&#34;https://datahowler.wordpress.com/2018/08/04/checking-model-assumptions-look-at-the-residuals-not-the-raw-data/&#34;&gt;checking assumptions for multilevel data&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;glossary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Glossary&lt;/h2&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;42%&#34; /&gt;
&lt;col width=&#34;57%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;term&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;definition&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; href=&#39;https://psyteachr.github.io/glossary/d#dependent-variable&#39;&gt;dependent-variable&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;The target variable that is being analyzed, whose value is assumed to depend on other variables.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; href=&#39;https://psyteachr.github.io/glossary/e#effect-code&#39;&gt;effect-code&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A coding scheme for categorical variables that contrasts each group mean with the mean of all the group means.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; href=&#39;https://psyteachr.github.io/glossary/g#general-linear-model&#39;&gt;general-linear-model&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A mathematical model comparing how one or more independent variables affect a continuous dependent variable&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; href=&#39;https://psyteachr.github.io/glossary/n#normal-distribution&#39;&gt;normal-distribution&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A symmetric distribution of data where values near the centre are most probable.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; href=&#39;https://psyteachr.github.io/glossary/p#parameter&#39;&gt;parameter&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A value that describes a distribution, such as the mean or SD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; href=&#39;https://psyteachr.github.io/glossary/p#predictor-variable&#39;&gt;predictor-variable&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A variable whose value is used (in a model) to predict the value of a response variable.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; href=&#39;https://psyteachr.github.io/glossary/q#q-q-plot&#39;&gt;q-q-plot&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A scatterplot created by plotting two sets of quantiles against each other, used to check if data come from a specified distribution&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; href=&#39;https://psyteachr.github.io/glossary/q#quantile&#39;&gt;quantile&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cutoffs dividing the range of a distribution into continuous intervals with equal probabilities.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; href=&#39;https://psyteachr.github.io/glossary/r#residual-error&#39;&gt;residual-error&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;That part of an observation that cannot be captured by the statistical model, and thus is assumed to reflect unknown factors.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; href=&#39;https://psyteachr.github.io/glossary/t#treatment-code&#39;&gt;treatment-code&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A coding scheme for categorical variables that creates (n_levels -1) dichotomous variables where each level of the categorical variable is contrasted to a reference level.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a class=&#39;glossary&#39; target=&#39;_blank&#39; href=&#39;https://psyteachr.github.io/glossary/u#uniform-distribution&#39;&gt;uniform-distribution&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;All numbers in the range have an equal probability of being sampled&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Experimentum Data Wrangling Demo</title>
      <link>https://debruine.github.io/post/experimentum-data/</link>
      <pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/experimentum-data/</guid>
      <description>
&lt;script src=&#34;https://debruine.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://debruine.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://debruine.github.io/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;(updated 2021-01-21)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/debruine/experimentum&#34;&gt;Experimentum&lt;/a&gt; studies require that you download data from questionnaires and experiments separately, since the data have different formats. You can &lt;a href=&#34;https://exp.psy.gla.ac.uk/project?demodata&amp;amp;autond&#34; target=&#34;_blank&#34;&gt;participate anonymously in the demo study&lt;/a&gt; (the median completion time is 3.8 minutes). The links below update dynamically, so your data will be available immediately.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://exp.psy.gla.ac.uk/include/scripts/public_download?id=520&amp;amp;download=exp&#34; target=&#34;_blank&#34;&gt;questionnaire data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://exp.psy.gla.ac.uk/include/scripts/public_download?id=520&amp;amp;download=quest&#34; target=&#34;_blank&#34;&gt;experiment data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;data/project_520_structure.json&#34;&gt;project structure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The project structure file above is a JSON-formatted file that contains all of the information needed to run a study in Experimentum. In future versions of Experimentum, you will be able to directly edit this file, for example translating the questions into another language, and set up a study by simply uploading the file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;proj &amp;lt;- jsonlite::read_json(&amp;quot;data/project_520_structure.json&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;questionnaire-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Questionnaire Data&lt;/h2&gt;
&lt;p&gt;The study has two questionnaires: &lt;strong&gt;Groups&lt;/strong&gt;, a few questions you can use to divide the participants into groups of varying sizes, and &lt;strong&gt;BMIS&lt;/strong&gt;, the &lt;a href=&#34;https://mypages.unh.edu/jdmayer/brief-mood-introspection-scale-bmis&#34;&gt;Brief Mood Introspection Scale&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;load-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quest_data &amp;lt;- read_csv(&amp;quot;data/Demo-Data-quests_2021-01-21.csv&amp;quot;)

glimpse(quest_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 2,807
## Columns: 14
## $ session_id  &amp;lt;dbl&amp;gt; 60481, 60481, 60481, 60481, 60481, 60481, 60481, 60481, 60…
## $ project_id  &amp;lt;dbl&amp;gt; 520, 520, 520, 520, 520, 520, 520, 520, 520, 520, 520, 520…
## $ quest_name  &amp;lt;chr&amp;gt; &amp;quot;Groups&amp;quot;, &amp;quot;Groups&amp;quot;, &amp;quot;Groups&amp;quot;, &amp;quot;Groups&amp;quot;, &amp;quot;Groups&amp;quot;, &amp;quot;Groups&amp;quot;…
## $ quest_id    &amp;lt;dbl&amp;gt; 2921, 2921, 2921, 2921, 2921, 2921, 2921, 2920, 2920, 2920…
## $ user_id     &amp;lt;dbl&amp;gt; 43533, 43533, 43533, 43533, 43533, 43533, 43533, 43533, 43…
## $ user_sex    &amp;lt;chr&amp;gt; &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;…
## $ user_status &amp;lt;chr&amp;gt; &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;r…
## $ user_age    &amp;lt;dbl&amp;gt; 26.6, 26.6, 26.6, 26.6, 26.6, 26.6, 26.6, 26.6, 26.6, 26.6…
## $ q_name      &amp;lt;chr&amp;gt; &amp;quot;fiber_arts&amp;quot;, &amp;quot;native_english&amp;quot;, &amp;quot;hats&amp;quot;, &amp;quot;pets&amp;quot;, &amp;quot;colour&amp;quot;, …
## $ q_id        &amp;lt;dbl&amp;gt; 92833810, 92833809, 92833814, 92833811, 92833813, 92833815…
## $ order       &amp;lt;dbl&amp;gt; 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…
## $ dv          &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;0&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;Yes&amp;quot;, &amp;quot;Red&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;today&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, …
## $ starttime   &amp;lt;dttm&amp;gt; 2021-01-19 17:48:15, 2021-01-19 17:48:15, 2021-01-19 17:4…
## $ endtime     &amp;lt;dttm&amp;gt; 2021-01-19 17:48:31, 2021-01-19 17:48:31, 2021-01-19 17:4…&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;session_id&lt;/code&gt; a unique ID generated each time someone starts a study&lt;/li&gt;
&lt;li&gt;&lt;code&gt;project_id&lt;/code&gt; a unique ID for this study&lt;/li&gt;
&lt;li&gt;&lt;code&gt;quest_name&lt;/code&gt; the name of each questionnaire (not guaranteed to be unique)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;quest_id&lt;/code&gt; uniquely identifies each questionnaire&lt;/li&gt;
&lt;li&gt;&lt;code&gt;user_id&lt;/code&gt; registered participants have a unique ID that is the same across logins, while guest participants get a new ID each time they log in&lt;/li&gt;
&lt;li&gt;&lt;code&gt;user_sex&lt;/code&gt; gender from the options “male”, “female”, “nonbinary”, “na” (specifically chose not to answer), or &lt;code&gt;NA&lt;/code&gt; (did not complete the question)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;user_status&lt;/code&gt; whether use is a researcher (“admin”, “super”, “res”, “student”), a “registered” user, a “guest” user, or “test”&lt;/li&gt;
&lt;li&gt;&lt;code&gt;user_age&lt;/code&gt; the user’s age; registered accounts are asked for their birthdate, and their age is calculated to the nearest 0.1 years; guest users may be asked their age in years&lt;/li&gt;
&lt;li&gt;&lt;code&gt;q_name&lt;/code&gt; name of the question&lt;/li&gt;
&lt;li&gt;&lt;code&gt;q_id&lt;/code&gt; uniquely identifies each question&lt;/li&gt;
&lt;li&gt;&lt;code&gt;order&lt;/code&gt; the order the question was presented in (not necessarily answered in) or questionnaires with randomised order&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dv&lt;/code&gt; the response&lt;/li&gt;
&lt;li&gt;&lt;code&gt;starttime&lt;/code&gt; the time that the questionnaire was started&lt;/li&gt;
&lt;li&gt;&lt;code&gt;endtime&lt;/code&gt; the time that the questionnaire was submitted&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;removing-duplicate-answers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Removing duplicate answers&lt;/h3&gt;
&lt;p&gt;Although Experimentum tries to prevent people accidentally using the back button during a study, there are some ways around this, so sometimes a person will submit the same questionnaire twice in a row. You can filter your data down to only the first time each person completed each question with the following code (do not use this if your study actually gives people the same questionnaire more than once).&lt;/p&gt;
&lt;p&gt;The questions are recorded in the order that they were answered, so we can just group by participant (&lt;code&gt;user_id&lt;/code&gt;) and question (&lt;code&gt;q_id&lt;/code&gt;) and choose the first answer. If you have a longitudinal study, group by &lt;code&gt;session_id&lt;/code&gt; to allow multiple sessions per user.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quest_distinct &amp;lt;- quest_data %&amp;gt;%
  group_by(user_id, q_id) %&amp;gt;% # or add session_id 
  # chooses the first time each user answered each question
  filter(row_number() == 1) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check how many duplicate rows were excluded.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setdiff(quest_data, quest_distinct) %&amp;gt;% nrow()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Groups&lt;/h3&gt;
&lt;p&gt;Here, we select just the data from the Groups questionnaire and keep only the session_id, user_id, q_name, and dv columns, and convert the data to wide format. If you restricted your data to only one session per user, as above, then &lt;code&gt;user_id&lt;/code&gt; and &lt;code&gt;session_id&lt;/code&gt; are redundant. The code below works for both types of data, though.&lt;/p&gt;
&lt;p&gt;If the &lt;code&gt;dv&lt;/code&gt; column contains both numeric and character data, the new columns will all be characters, so add &lt;code&gt;convert = TRUE&lt;/code&gt; if you are using &lt;code&gt;spread()&lt;/code&gt;. If you use the &lt;code&gt;pivot_wider()&lt;/code&gt; function, it doesn’t have a &lt;code&gt;convert&lt;/code&gt; argument, so you have to pipe the data frame to a separate function, &lt;code&gt;type.convert()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;groups &amp;lt;- quest_distinct %&amp;gt;%
  filter(quest_name == &amp;quot;Groups&amp;quot;) %&amp;gt;%
  select(session_id, user_id, q_name, dv) %&amp;gt;%
  spread(q_name, dv, convert = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
session_id
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
user_id
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
colour
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
exercise
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
fiber_arts
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
hats
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
native_english
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
pets
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
spiders
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Red
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
today
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60495
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31625
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Green
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
today
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60509
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53422
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
black
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
more
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60550
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53458
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
purplE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
today
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60552
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53460
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
green
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
today
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60553
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53461
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Blue
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
today
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;questionnaire-options&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Questionnaire options&lt;/h3&gt;
&lt;p&gt;I usually recommend recording the actual text chosen from drop-down menus, rather than integers that you will have to remember how you mapped onto the answers. If you need to check how you set up the coding, you can look at the info page on Experimentum or check the project json file that we loaded above. It’s a nested list and contains all the info, but can be a little tricky to parse (I’ll work on making an R package to make this easier in the future).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get question names, text and type, plus options if select
qs &amp;lt;- proj$quest_2921$question %&amp;gt;%
  map(~{
    x &amp;lt;- .x[c(&amp;quot;name&amp;quot;, &amp;quot;question&amp;quot;, &amp;quot;type&amp;quot;)]
    if (!is.null(.x$options)) {
      x$options &amp;lt;- sapply(.x$options, `[[`, &amp;quot;display&amp;quot;)
      names(x$options) &amp;lt;- sapply(.x$options, `[[`, &amp;quot;opt_value&amp;quot;)
    }
    x
  })&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;name: native_english&lt;/li&gt;
&lt;li&gt;question: Is English your native language?&lt;/li&gt;
&lt;li&gt;type: select&lt;/li&gt;
&lt;li&gt;options:
&lt;ul&gt;
&lt;li&gt;1: Yes&lt;/li&gt;
&lt;li&gt;0: No&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;name: fiber_arts&lt;/li&gt;
&lt;li&gt;question: Do you know how to knit or crochet?&lt;/li&gt;
&lt;li&gt;type: select&lt;/li&gt;
&lt;li&gt;options:
&lt;ul&gt;
&lt;li&gt;1: Yes&lt;/li&gt;
&lt;li&gt;0: No&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;name: pets&lt;/li&gt;
&lt;li&gt;question: Do you have a pet?&lt;/li&gt;
&lt;li&gt;type: select&lt;/li&gt;
&lt;li&gt;options:
&lt;ul&gt;
&lt;li&gt;Yes: Yes&lt;/li&gt;
&lt;li&gt;No: No&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;name: exercise&lt;/li&gt;
&lt;li&gt;question: When was the last time you exercised?&lt;/li&gt;
&lt;li&gt;type: select&lt;/li&gt;
&lt;li&gt;options:
&lt;ul&gt;
&lt;li&gt;today: today or yesterday&lt;/li&gt;
&lt;li&gt;week: in the past week&lt;/li&gt;
&lt;li&gt;more: more than a week ago&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;name: colour&lt;/li&gt;
&lt;li&gt;question: What is your favourite colour?&lt;/li&gt;
&lt;li&gt;type: text&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;name: hats&lt;/li&gt;
&lt;li&gt;question: How many hats do you own (approximately)?&lt;/li&gt;
&lt;li&gt;type: selectnum&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;name: spiders&lt;/li&gt;
&lt;li&gt;question: Are you afraid of spiders?&lt;/li&gt;
&lt;li&gt;type: radioanchor&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;plots-and-tables&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Plots and tables&lt;/h4&gt;
&lt;p&gt;Plot your data or create summary tables to help you spot any problems. The &lt;code&gt;count()&lt;/code&gt; function is useful for variables with a small number of options.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# count a single column
count(groups, exercise)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
exercise
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
more
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
24
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
today
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
69
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
week
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# count multiple columns
count(groups, fiber_arts, native_english, pets)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
fiber_arts
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
native_english
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
pets
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Histograms or density plots are best for columns with many continuous values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(groups, aes(hats)) +
  geom_histogram(binwidth = 1, 
                 fill = &amp;quot;dodgerblue&amp;quot;,
                 color = &amp;quot;white&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/demodata_files/figure-html/hats-plot-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(groups, aes(spiders)) +
  geom_histogram(binwidth = 1, 
                 fill = &amp;quot;violetred&amp;quot;,
                 color = &amp;quot;white&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/demodata_files/figure-html/spiders-plot-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;recode-variables&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Recode variables&lt;/h4&gt;
&lt;p&gt;You might want to do some recoding of variables here. The &lt;code&gt;pets&lt;/code&gt; column contains the words “Yes” and “No”; maybe you want to code this as 1s and 0s.The column &lt;code&gt;fiber_arts&lt;/code&gt; has a 1 if a person knows how to knit or crochet, and a 0 if they don’t. You might want to change this to the words “Yes” and “No”. The &lt;code&gt;recode()&lt;/code&gt; function is useful for this. I like to give the binary-coded version of a variable the suffix “.b”.&lt;/p&gt;
&lt;p&gt;Note that in the &lt;code&gt;recode()&lt;/code&gt; function, numbers that are on the left side of an equal sign need to be in quotes. This just has to do with the way R treats argument names and doesn’t mean that the recoded column has to be a character type.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;groups_coded &amp;lt;- groups %&amp;gt;%
  mutate(
    pets.b = recode(pets, &amp;quot;Yes&amp;quot; = 1, &amp;quot;No&amp;quot; = 0),
    fiber_arts.b = fiber_arts,
    fiber_arts = recode(fiber_arts, &amp;quot;1&amp;quot; = &amp;quot;Yes&amp;quot;, &amp;quot;0&amp;quot; = &amp;quot;No&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
session_id
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
user_id
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
colour
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
exercise
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
fiber_arts
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
hats
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
native_english
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
pets
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
spiders
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
pets.b
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
fiber_arts.b
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Red
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
today
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60495
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31625
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Green
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
today
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60509
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53422
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
black
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
more
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60550
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53458
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
purplE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
today
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60552
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53460
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
green
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
today
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60553
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53461
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Blue
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
today
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;free-text&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Free text&lt;/h4&gt;
&lt;p&gt;If you have any free-text responses, you will probably need to code them. I always start by looking at all of the possible values after transforming the value to lowercase and getting rid of spaces around the text.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colours &amp;lt;- groups_coded %&amp;gt;%
  mutate(colour = tolower(colour) %&amp;gt;% trimws()) %&amp;gt;%
  count(colour) %&amp;gt;%
  arrange(n, colour) %&amp;gt;%
  group_by(n) %&amp;gt;%
  summarise(colours = paste(colour, collapse = &amp;quot;, &amp;quot;),
            .groups = &amp;quot;drop&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
colours
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
amber, cornflower, grey, indian orange, magenta, monochrome grey/white tones, petroleum blue, teal, violet, white, yes.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
pink, turquoise
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
orange
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
black
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
yellow
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
red
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
purple
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
green
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
blue
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can then decide to recode some colours to fix misspellings, etc. One tricky part of using &lt;code&gt;recode()&lt;/code&gt; is that all replaced values have to be the same data type, so use &lt;code&gt;NA_character_&lt;/code&gt; if you are replacing values with strings, &lt;code&gt;NA_real_&lt;/code&gt; for doubles, and &lt;code&gt;NA_integer_&lt;/code&gt; for integers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;groups_colours &amp;lt;- groups_coded %&amp;gt;%
  mutate(colour = tolower(colour) %&amp;gt;% trimws()) %&amp;gt;%
  mutate(colours_fixed = recode(colour,
                                &amp;quot;amber&amp;quot; = &amp;quot;yellow&amp;quot;, 
                                &amp;quot;yes.&amp;quot; = NA_character_,
                                &amp;quot;violet&amp;quot; = &amp;quot;purple&amp;quot;,
                                &amp;quot;petroleum blue&amp;quot; = &amp;quot;blue&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bmis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;BMIS&lt;/h3&gt;
&lt;p&gt;The second questionnaire is the Brief Mood Introspection Scale (BMIS). The BMIS has 16 questions divided into positive and negative adjectives. The question names are all in the format &lt;code&gt;valence_adjective&lt;/code&gt;, so you can easily separate the question name into two columns.&lt;/p&gt;
&lt;p&gt;Because the original &lt;code&gt;quest_data&lt;/code&gt; had both character and numeric values in the &lt;code&gt;dv&lt;/code&gt; column, it is still a character type even now that the dv column only contains numbers. You can fix this using the &lt;code&gt;type_convert()&lt;/code&gt; function. Set col_types manually or use &lt;code&gt;cols()&lt;/code&gt; to automatically guess.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bmis_raw &amp;lt;- quest_distinct %&amp;gt;%
  filter(quest_name == &amp;quot;BMIS&amp;quot;) %&amp;gt;%
  select(session_id, user_id, q_name, dv) %&amp;gt;%
  separate(q_name, c(&amp;quot;valence&amp;quot;, &amp;quot;adjective&amp;quot;)) %&amp;gt;%
  type_convert(col_types = cols())&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
session_id
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
user_id
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
valence
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
adjective
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
dv
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
neg
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
nervous
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
pos
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
caring
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
pos
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
peppy
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
neg
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
tired
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
neg
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
grouchy
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
neg
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
sad
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;summary-scores&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Summary scores&lt;/h4&gt;
&lt;p&gt;The BMIS is coded as the sum of the forward-coded scores for all the positive adjectives and the reverse-coded scores for all negative adjectives. Experimentum has a function to reverse code selected items in the “radiopage” questionnaire type, but we didn’t do that here so you can see how to manually recode. The scores are 1 to 4, so subjtract them from 5 to get the reverse-coded value. Make sure to look at a few of your recoded values to make sure it’s doing what you expect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bmis_coded &amp;lt;- bmis_raw %&amp;gt;%
  mutate(recoded_dv = case_when(
    valence == &amp;quot;pos&amp;quot; ~ dv,
    valence == &amp;quot;neg&amp;quot; ~ 5 - dv
  ))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
session_id
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
user_id
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
valence
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
adjective
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
dv
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
recoded_dv
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
neg
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
nervous
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
pos
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
caring
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
pos
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
peppy
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
neg
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
tired
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
neg
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
grouchy
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
neg
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
sad
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Create the summary score by grouping by user_id and session_id and summing the responses. We use &lt;code&gt;sum()&lt;/code&gt; here because the BMIS is not valid if people skipped any questions, so we want the result to be &lt;code&gt;NA&lt;/code&gt; if they did. Some questionnaire scoring allows you to calculate an average score omitting missed questions, so you could use &lt;code&gt;mean(recoded_dv, na.rm = TRUE)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bmis &amp;lt;- bmis_coded %&amp;gt;%
  group_by(session_id, user_id) %&amp;gt;%
  summarise(bmis = sum(recoded_dv),
            .groups = &amp;quot;drop&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plots&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Plots&lt;/h4&gt;
&lt;p&gt;Always plot your summary scores. This helps you to double-check your logic and identify outliers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(bmis, aes(bmis)) + 
  geom_histogram(binwidth = 1, 
                 fill = &amp;quot;dodgerblue&amp;quot;,
                 color = &amp;quot;white&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/demodata_files/figure-html/bmis-hist-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;user-demographics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;User Demographics&lt;/h3&gt;
&lt;p&gt;Experimentum data always contains user demographic data, which is collected when the user signs up for a registered account or logs in as a guest. This study did not ask users for their age or sex, so that info is only available from registered users.&lt;/p&gt;
&lt;p&gt;First, we select the session_id and all of the user variables, then make sure we have only one row for each participant.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;user &amp;lt;- quest_distinct %&amp;gt;%
  select(session_id, starts_with(&amp;quot;user_&amp;quot;)) %&amp;gt;%
  distinct()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
session_id
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
user_id
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
user_sex
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
user_status
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
user_age
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43533
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
res
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60495
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31625
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
student
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60509
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53422
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
guest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
61.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60550
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53458
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
guest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60552
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53460
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
guest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60553
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
53461
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
guest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;rejoining&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rejoining&lt;/h3&gt;
&lt;p&gt;Now you can rejoin your questionnaire data. Start with the user table and only join on matching data from the individual questionnaires. Use &lt;code&gt;session_id&lt;/code&gt; and &lt;code&gt;user_id&lt;/code&gt; to join.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;q_data &amp;lt;- user %&amp;gt;%
  left_join(bmis, by = c(&amp;quot;session_id&amp;quot;, &amp;quot;user_id&amp;quot;)) %&amp;gt;%
  left_join(groups_colours, by = c(&amp;quot;session_id&amp;quot;, &amp;quot;user_id&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exclusions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exclusions&lt;/h3&gt;
&lt;p&gt;You will usually want to exclude participants with user_status that are not “registered” or “guest”. Statuses “admin”, “super”, “res”, and “student” refer to different types of researchers (with different privileges on the Experimentum platform). The status “test” is for test runs with different user demographics. You can also apply other exclusion criteria here like age restrictions or requiring that summary score not be missing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;q_data_excl &amp;lt;- q_data %&amp;gt;%
  filter(user_status %in% c(&amp;quot;guest&amp;quot;, &amp;quot;registered&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
user_status
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
guest
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
121
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;experiment-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Experiment Data&lt;/h2&gt;
&lt;p&gt;Our study has one rating experiment with two between-subject conditions: cuteness ratings and appropriateness as pet ratings.&lt;/p&gt;
&lt;div id=&#34;load-data-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp_data &amp;lt;- read_csv(&amp;quot;data/Demo-Data-exps_2021-01-21.csv&amp;quot;)

glimpse(exp_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1,476
## Columns: 15
## $ session_id  &amp;lt;dbl&amp;gt; 60481, 60481, 60481, 60481, 60481, 60481, 60481, 60481, 60…
## $ project_id  &amp;lt;dbl&amp;gt; 520, 520, 520, 520, 520, 520, 520, 520, 520, 520, 520, 520…
## $ exp_name    &amp;lt;chr&amp;gt; &amp;quot;Animals: Cuteness&amp;quot;, &amp;quot;Animals: Cuteness&amp;quot;, &amp;quot;Animals: Cutene…
## $ exp_id      &amp;lt;dbl&amp;gt; 707, 707, 707, 707, 707, 707, 707, 707, 707, 707, 707, 707…
## $ user_id     &amp;lt;dbl&amp;gt; 43533, 43533, 43533, 43533, 43533, 43533, 43533, 43533, 43…
## $ user_sex    &amp;lt;chr&amp;gt; &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;…
## $ user_status &amp;lt;chr&amp;gt; &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;r…
## $ user_age    &amp;lt;dbl&amp;gt; 26.6, 26.6, 26.6, 26.6, 26.6, 26.6, 26.6, 26.6, 26.6, 26.6…
## $ trial_name  &amp;lt;chr&amp;gt; &amp;quot;animal-967657_640&amp;quot;, &amp;quot;surprised-3786845_640&amp;quot;, &amp;quot;penguins-42…
## $ trial_n     &amp;lt;dbl&amp;gt; 2, 12, 9, 3, 8, 10, 5, 6, 7, 11, 4, 1, 4, 10, 7, 11, 12, 9…
## $ order       &amp;lt;dbl&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7…
## $ dv          &amp;lt;dbl&amp;gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 3, 4, 2, 7, 4…
## $ rt          &amp;lt;dbl&amp;gt; 1756, 1103, 842, 735, 755, 845, 878, 842, 812, 1421, 1003,…
## $ side        &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ dt          &amp;lt;dttm&amp;gt; 2021-01-19 17:48:35, 2021-01-19 17:48:36, 2021-01-19 17:4…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Experiment data have the same session and user columns as questionnaire data, plus columns for the experiment name (&lt;code&gt;exp_name&lt;/code&gt;) and unique id (&lt;code&gt;exp_id&lt;/code&gt;). The remaining columns give data specific to each trial:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;trial_name&lt;/code&gt; not necessarily unique&lt;/li&gt;
&lt;li&gt;&lt;code&gt;trial_n&lt;/code&gt; uniquely identifies each trial within an experiment&lt;/li&gt;
&lt;li&gt;&lt;code&gt;order&lt;/code&gt; (the order the trial was shown to that participant&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dv&lt;/code&gt; the response&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rt&lt;/code&gt; the rough reaction time in milliseconds (web data have many sources of possible bias so do not use Experimentum to do RT experiments that require millisecond precision)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;side&lt;/code&gt; if the experiment has multiple images, the order of the images if side is set to random (not relevant here)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dt&lt;/code&gt; the timestamp of the response&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(exp_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1,476
## Columns: 15
## $ session_id  &amp;lt;dbl&amp;gt; 60481, 60481, 60481, 60481, 60481, 60481, 60481, 60481, 60…
## $ project_id  &amp;lt;dbl&amp;gt; 520, 520, 520, 520, 520, 520, 520, 520, 520, 520, 520, 520…
## $ exp_name    &amp;lt;chr&amp;gt; &amp;quot;Animals: Cuteness&amp;quot;, &amp;quot;Animals: Cuteness&amp;quot;, &amp;quot;Animals: Cutene…
## $ exp_id      &amp;lt;dbl&amp;gt; 707, 707, 707, 707, 707, 707, 707, 707, 707, 707, 707, 707…
## $ user_id     &amp;lt;dbl&amp;gt; 43533, 43533, 43533, 43533, 43533, 43533, 43533, 43533, 43…
## $ user_sex    &amp;lt;chr&amp;gt; &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;…
## $ user_status &amp;lt;chr&amp;gt; &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;res&amp;quot;, &amp;quot;r…
## $ user_age    &amp;lt;dbl&amp;gt; 26.6, 26.6, 26.6, 26.6, 26.6, 26.6, 26.6, 26.6, 26.6, 26.6…
## $ trial_name  &amp;lt;chr&amp;gt; &amp;quot;animal-967657_640&amp;quot;, &amp;quot;surprised-3786845_640&amp;quot;, &amp;quot;penguins-42…
## $ trial_n     &amp;lt;dbl&amp;gt; 2, 12, 9, 3, 8, 10, 5, 6, 7, 11, 4, 1, 4, 10, 7, 11, 12, 9…
## $ order       &amp;lt;dbl&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7…
## $ dv          &amp;lt;dbl&amp;gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 3, 4, 2, 7, 4…
## $ rt          &amp;lt;dbl&amp;gt; 1756, 1103, 842, 735, 755, 845, 878, 842, 812, 1421, 1003,…
## $ side        &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ dt          &amp;lt;dttm&amp;gt; 2021-01-19 17:48:35, 2021-01-19 17:48:36, 2021-01-19 17:4…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most researchers don’t want all that data, so we can select just the important columns. The exp_name contains info we don’t need, so we’ll also process that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp_selected &amp;lt;- exp_data %&amp;gt;%
  select(session_id, user_id, exp_name, trial_name, dv) %&amp;gt;%
  mutate(exp_name = sub(&amp;quot;Animals: &amp;quot;, &amp;quot;&amp;quot;, exp_name))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count(exp_selected, exp_name)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
exp_name
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Appropriate
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
696
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cuteness
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
780
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count(exp_selected, trial_name)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
trial_name
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
adorable-5059091_640
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
animal-967657_640
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bird-349035_640
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
dolphin-203875_640
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
frog-3312038_640
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
hedgehog-468228_640
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
kitty-2948404_640
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
meerkat-459171_640
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
penguins-429134_640
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
pug-690566_640
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
spider-2313079_640
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
surprised-3786845_640
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
123
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-trial-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding trial data&lt;/h3&gt;
&lt;p&gt;It’s common that you need to add some data about each trial.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-20&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;cute/adorable-5059091_640.jpg&#34; alt=&#34;images from the experiment&#34; width=&#34;24%&#34; /&gt;&lt;img src=&#34;cute/animal-967657_640.jpg&#34; alt=&#34;images from the experiment&#34; width=&#34;24%&#34; /&gt;&lt;img src=&#34;cute/bird-349035_640.jpg&#34; alt=&#34;images from the experiment&#34; width=&#34;24%&#34; /&gt;&lt;img src=&#34;cute/dolphin-203875_640.jpg&#34; alt=&#34;images from the experiment&#34; width=&#34;24%&#34; /&gt;&lt;img src=&#34;cute/frog-3312038_640.jpg&#34; alt=&#34;images from the experiment&#34; width=&#34;24%&#34; /&gt;&lt;img src=&#34;cute/hedgehog-468228_640.jpg&#34; alt=&#34;images from the experiment&#34; width=&#34;24%&#34; /&gt;&lt;img src=&#34;cute/kitty-2948404_640.jpg&#34; alt=&#34;images from the experiment&#34; width=&#34;24%&#34; /&gt;&lt;img src=&#34;cute/meerkat-459171_640.jpg&#34; alt=&#34;images from the experiment&#34; width=&#34;24%&#34; /&gt;&lt;img src=&#34;cute/penguins-429134_640.jpg&#34; alt=&#34;images from the experiment&#34; width=&#34;24%&#34; /&gt;&lt;img src=&#34;cute/pug-690566_640.jpg&#34; alt=&#34;images from the experiment&#34; width=&#34;24%&#34; /&gt;&lt;img src=&#34;cute/spider-2313079_640.jpg&#34; alt=&#34;images from the experiment&#34; width=&#34;24%&#34; /&gt;&lt;img src=&#34;cute/surprised-3786845_640.jpg&#34; alt=&#34;images from the experiment&#34; width=&#34;24%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: images from the experiment
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;You will probably have you trial information in a separate table, so you can load that here. In this case, we’ll use the &lt;code&gt;tribble()&lt;/code&gt; function to create a table by rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trial_info &amp;lt;- tribble(
  ~photo,                  ~name,    ~is_baby, ~mammal,
  &amp;quot;adorable-5059091_640&amp;quot;,  &amp;quot;kitten1&amp;quot;,   1,        1,
  &amp;quot;animal-967657_640&amp;quot;,     &amp;quot;fox&amp;quot;,       0,        1,
  &amp;quot;bird-349035_640&amp;quot;,       &amp;quot;chicken&amp;quot;,   1,        0,
  &amp;quot;dolphin-203875_640&amp;quot;,    &amp;quot;dolphin&amp;quot;,   0,        1,
  &amp;quot;frog-3312038_640&amp;quot;,      &amp;quot;frog&amp;quot;,      0,        0,
  &amp;quot;hedgehog-468228_640&amp;quot;,   &amp;quot;hedgehog&amp;quot;,  0,        1,
  &amp;quot;kitty-2948404_640&amp;quot;,     &amp;quot;kitten2&amp;quot;,   1,        1,
  &amp;quot;meerkat-459171_640&amp;quot;,    &amp;quot;meerkat&amp;quot;,   0,        1,
  &amp;quot;penguins-429134_640&amp;quot;,   &amp;quot;penguin&amp;quot;,   1,        0,
  &amp;quot;pug-690566_640&amp;quot;,        &amp;quot;dog&amp;quot;,       1,        1,
  &amp;quot;spider-2313079_640&amp;quot;,    &amp;quot;spider&amp;quot;,    0,        0,
  &amp;quot;surprised-3786845_640&amp;quot;, &amp;quot;squirrel&amp;quot;,  0,        1
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you can join it to the experiment data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp_full &amp;lt;- exp_selected %&amp;gt;%
  left_join(trial_info, by = c(&amp;quot;trial_name&amp;quot; = &amp;quot;photo&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And create some plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(exp_full, aes(dv, colour = exp_name)) +
  geom_freqpoly(binwidth = 1) +
  facet_wrap(~name) +
  scale_x_continuous(breaks = 1:7) +
  labs(colour = &amp;quot;Rating Type&amp;quot;, x = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/demodata_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;demographs-exclusions-and-rejoin&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Demographs, exclusions and rejoin&lt;/h3&gt;
&lt;p&gt;As for the questionnaire data above, you can pull out the user demographics, apply exclusions, and rejoin to the experiment data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# user table with exclusions
user_excl &amp;lt;- exp_data %&amp;gt;%
  select(session_id, starts_with(&amp;quot;user_&amp;quot;)) %&amp;gt;%
  distinct() %&amp;gt;%
  filter(user_status %in% c(&amp;quot;guest&amp;quot;, &amp;quot;registered&amp;quot;))

e_data_excl &amp;lt;- user_excl %&amp;gt;%
  left_join(exp_full, by = c(&amp;quot;session_id&amp;quot;, &amp;quot;user_id&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(e_data_excl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1,440
## Columns: 11
## $ session_id  &amp;lt;dbl&amp;gt; 60509, 60509, 60509, 60509, 60509, 60509, 60509, 60509, 60…
## $ user_id     &amp;lt;dbl&amp;gt; 53422, 53422, 53422, 53422, 53422, 53422, 53422, 53422, 53…
## $ user_sex    &amp;lt;chr&amp;gt; &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;…
## $ user_status &amp;lt;chr&amp;gt; &amp;quot;guest&amp;quot;, &amp;quot;guest&amp;quot;, &amp;quot;guest&amp;quot;, &amp;quot;guest&amp;quot;, &amp;quot;guest&amp;quot;, &amp;quot;guest&amp;quot;, &amp;quot;gue…
## $ user_age    &amp;lt;dbl&amp;gt; 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 0, 0, 0, 0…
## $ exp_name    &amp;lt;chr&amp;gt; &amp;quot;Appropriate&amp;quot;, &amp;quot;Appropriate&amp;quot;, &amp;quot;Appropriate&amp;quot;, &amp;quot;Appropriate&amp;quot;…
## $ trial_name  &amp;lt;chr&amp;gt; &amp;quot;penguins-429134_640&amp;quot;, &amp;quot;bird-349035_640&amp;quot;, &amp;quot;hedgehog-468228…
## $ dv          &amp;lt;dbl&amp;gt; 1, 4, 5, 1, 7, 1, 7, 6, 1, 7, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4…
## $ name        &amp;lt;chr&amp;gt; &amp;quot;penguin&amp;quot;, &amp;quot;chicken&amp;quot;, &amp;quot;hedgehog&amp;quot;, &amp;quot;squirrel&amp;quot;, &amp;quot;kitten2&amp;quot;, &amp;quot;…
## $ is_baby     &amp;lt;dbl&amp;gt; 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0…
## $ mammal      &amp;lt;dbl&amp;gt; 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1…&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;joining-questionnaire-and-experiment-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Joining Questionnaire and Experiment Data&lt;/h2&gt;
&lt;p&gt;Often, it makes most sense to process questionnaire data in wide format and experiment data in long format. If you need to add wide questionnaire data to a long experiment table, left join the questionnaire like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_data &amp;lt;- e_data_excl %&amp;gt;%
  left_join(q_data_excl, by = c(&amp;quot;session_id&amp;quot;, &amp;quot;user_id&amp;quot;))

names(all_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;session_id&amp;quot;     &amp;quot;user_id&amp;quot;        &amp;quot;user_sex.x&amp;quot;     &amp;quot;user_status.x&amp;quot; 
##  [5] &amp;quot;user_age.x&amp;quot;     &amp;quot;exp_name&amp;quot;       &amp;quot;trial_name&amp;quot;     &amp;quot;dv&amp;quot;            
##  [9] &amp;quot;name&amp;quot;           &amp;quot;is_baby&amp;quot;        &amp;quot;mammal&amp;quot;         &amp;quot;user_sex.y&amp;quot;    
## [13] &amp;quot;user_status.y&amp;quot;  &amp;quot;user_age.y&amp;quot;     &amp;quot;bmis&amp;quot;           &amp;quot;colour&amp;quot;        
## [17] &amp;quot;exercise&amp;quot;       &amp;quot;fiber_arts&amp;quot;     &amp;quot;hats&amp;quot;           &amp;quot;native_english&amp;quot;
## [21] &amp;quot;pets&amp;quot;           &amp;quot;spiders&amp;quot;        &amp;quot;pets.b&amp;quot;         &amp;quot;fiber_arts.b&amp;quot;  
## [25] &amp;quot;colours_fixed&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now go explore your data!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/demodata_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>WebMorph</title>
      <link>https://debruine.github.io/project/webmorph/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/project/webmorph/</guid>
      <description>&lt;p&gt;The goal of WebMorph and the associated R package, webmorphR, is to make the construction of image stimuli more reproducible, with a focus on face stimuli.&lt;/p&gt;
&lt;p class=&#34;info&#34;&gt;
webmorphR is now on CRAN!
&lt;/p&gt;
&lt;p&gt;This development of these tools was funded by ERC grant #647910 (KINSHIP).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Function Tips</title>
      <link>https://debruine.github.io/post/function-tips/</link>
      <pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/function-tips/</guid>
      <description>
&lt;script src=&#34;https://debruine.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I see a lot of functions from people new to coding that look like this and I want to point out a few common conceptual mistakes with writing functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# my data-generating parameters
my_n &amp;lt;- 50
my_mu &amp;lt;- c(0, 0.2)
my_sd &amp;lt;- c(1, 1)
my_r &amp;lt;- 0.5

my_func &amp;lt;- function(n = my_n) {
  # simulate data
  dat &amp;lt;- faux::rnorm_multi(
    n = my_n,
    vars = 2,
    mu = my_mu,
    sd = my_sd,
    r = my_r,
    varnames = c(&amp;quot;low&amp;quot;, &amp;quot;high&amp;quot;)
  )
  
  # test high-low difference
  t.test(dat$high, dat$low, paired = TRUE)
}

my_func(n = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Paired t-test
## 
## data:  dat$high and dat$low
## t = 1.9282, df = 49, p-value = 0.05964
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.01142952  0.55308436
## sample estimates:
## mean of the differences 
##               0.2708274&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;definitions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Definitions&lt;/h2&gt;
&lt;p&gt;First, it’s helpful to define a few terms I’ll use below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Variables&lt;/strong&gt; are words that identify and store the value of some data for later use. For example, in the code above, &lt;code&gt;my_n&lt;/code&gt; is a variable and its &lt;strong&gt;value&lt;/strong&gt; is set to &lt;code&gt;50&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;function&lt;/strong&gt; is &lt;code&gt;my_func()&lt;/code&gt; and is defined by the code inside the curly brackets &lt;code&gt;{...}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;arguments&lt;/strong&gt; are the variables that are set by the function. In the function above, &lt;code&gt;function(n = my_n)&lt;/code&gt; has one argument: &lt;code&gt;n&lt;/code&gt;.
Arguments can have a &lt;strong&gt;default value&lt;/strong&gt;. In the code above, the argument &lt;code&gt;n&lt;/code&gt; has a default value of &lt;code&gt;my_n&lt;/code&gt;, which is the value that argument takes if it’s not explicitly defined (e.g., if you just run &lt;code&gt;my_func()&lt;/code&gt; instead of &lt;code&gt;my_func(n = 25)&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;global environment&lt;/strong&gt; is the set of variables and functions that you create during your R session. It is what you can see in the Environment tab in RStudio.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;not-using-an-argument&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Not using an argument&lt;/h2&gt;
&lt;p&gt;The biggest thing wrong with this code is that it defines an argument called &lt;code&gt;n&lt;/code&gt;, but doesn’t use it in the code. It uses the variable &lt;code&gt;my_n&lt;/code&gt; instead. So when I ran &lt;code&gt;my_func(n = 100)&lt;/code&gt; above, I still got the result for &lt;code&gt;n = 50&lt;/code&gt; (check the df in the output), because &lt;code&gt;my_n&lt;/code&gt; still equals 50 and that’s what the function is using, not the value of &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It’s easy for this to happen when you’re first developing a function because you probably are modifying non-function code, where it does make sense to use &lt;code&gt;my_n&lt;/code&gt; in the &lt;code&gt;rnorm_multi()&lt;/code&gt; function. You can either make the name of the argument match (e.g., &lt;code&gt;function(my_n) {...}&lt;/code&gt;) or change the name of the variable in the function code (e.g., &lt;code&gt;dat &amp;lt;- faux::rnorm_multi(n = n, ...)&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;externally-defined-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Externally defined variables&lt;/h2&gt;
&lt;p&gt;The code in a function should usually be able to run without depending on having variables with correct names in the environment. This code doesn’t run if you don’t define &lt;code&gt;my_mu&lt;/code&gt;, &lt;code&gt;my_sd&lt;/code&gt; and &lt;code&gt;my_r&lt;/code&gt; before running it. It’s tempting to just use those variables inside the function, because it saves you typing the values as arguments to the function when you use it, especially in a script where you know you won’t need to change those values, but this makes the function less useful in other contexts (including reuse by future you).&lt;/p&gt;
&lt;p&gt;There can definitely be exceptions to this, but first master the rule that any variables used inside a function have to be defined as arguments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variables-as-default-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variables as default values&lt;/h2&gt;
&lt;p&gt;Normally, don’t set function argument defaults to a variable (again, there can be exceptions, but you need to master that rule before you understand when you can use exceptions). It will work fine when you’re testing it in context and all the variables you expect to be in the environment are there, but the point of a function is to be reusable in different contexts, so it’s best not to depend on external things.&lt;/p&gt;
&lt;p&gt;You don’t always have to set a default value for an argument, but it’s often useful to set the default value to a “neutral” thing that makes the code run even if the user doesn’t set all the arguments. So &lt;code&gt;n&lt;/code&gt; should be a sensible number like 100 (e.g., if you set it to 0, then the code won’t run correctly), and &lt;code&gt;sd&lt;/code&gt; should be 1 (not 0, since that’s not a valid value for SDs).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;better-versions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Better versions&lt;/h2&gt;
&lt;p&gt;In this version, I used the same argument names as the &lt;code&gt;rnorm_multi()&lt;/code&gt; function from &lt;a href=&#34;https://debruine.github.io/faux/&#34;&gt;faux&lt;/a&gt;, and also set their default values to the same defaults that function uses (simulating a dataset with 100 pairs of observations with means of 0, SDs of 1, and a correlation of 0).&lt;/p&gt;
&lt;p&gt;You could add more arguments to the function, like &lt;code&gt;vars&lt;/code&gt; or &lt;code&gt;varnames&lt;/code&gt;, but in this context I know I would never want to vary them, so I can “hard-code” them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_func2 &amp;lt;- function(n = 100, mu = 0, sd = 1, r = 0) {
  # simulate data
  dat &amp;lt;- faux::rnorm_multi(
    n = n,
    vars = 2,
    mu = mu,
    sd = sd,
    r = r,
    varnames = c(&amp;quot;low&amp;quot;, &amp;quot;high&amp;quot;)
  )
  
  # test high-low difference
  t.test(dat$high, dat$low, paired = TRUE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This lets you run the function without any arguments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_func2()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Paired t-test
## 
## data:  dat$high and dat$low
## t = 0.94057, df = 99, p-value = 0.3492
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.1338159  0.3750168
## sample estimates:
## mean of the differences 
##               0.1206004&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then I can add in new values or my data-generating parameters from the code above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_func2(n = 200, mu = my_mu, sd = my_sd, r = my_r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Paired t-test
## 
## data:  dat$high and dat$low
## t = 2.2674, df = 199, p-value = 0.02444
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.02327751 0.33404535
## sample estimates:
## mean of the differences 
##               0.1786614&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Your function doesn’t need to use the same variable names as the functions you might be using them in. Some people find using the same variable names to be easier because you can see the connection between the variable in your function and where you’re using it. But this can be confusing for new coders. You can give your function argument names that are different to clarify where you’re using them.&lt;/p&gt;
&lt;p&gt;If this pattern makes sense to you, I recommend using a consistent prefix to the name, like &lt;code&gt;the_&lt;/code&gt;, so you can always know if a variable is being defined as an argument to the function or externally.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_func3 &amp;lt;- function(the_n = 100, the_mu = 0, the_sd = 1, the_r = 0) {
  # simulate data
  dat &amp;lt;- faux::rnorm_multi(
    n = the_n,
    vars = 2,
    mu = the_mu,
    sd = the_sd,
    r = the_r,
    varnames = c(&amp;quot;low&amp;quot;, &amp;quot;high&amp;quot;)
  )
  
  # test high-low difference
  t.test(dat$high, dat$low, paired = TRUE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’re using argument names in your function call, you will need to make sure they’re consistent with the function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_func3(the_n = my_n, the_mu = my_mu, the_sd = my_sd, the_r = my_r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Paired t-test
## 
## data:  dat$high and dat$low
## t = 0.80255, df = 49, p-value = 0.4261
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.1585849  0.3694730
## sample estimates:
## mean of the differences 
##                0.105444&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or you can set the arguments by order and omit the names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_func3(my_n, my_mu, my_sd, my_r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Paired t-test
## 
## data:  dat$high and dat$low
## t = 0.29191, df = 49, p-value = 0.7716
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.2255497  0.3022130
## sample estimates:
## mean of the differences 
##              0.03833165&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;scope&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scope&lt;/h2&gt;
&lt;p&gt;The concept of &lt;strong&gt;scope&lt;/strong&gt; can also be confusing to new coders. In this context it’s just important to know that if you have argument names that are the same as variables in your environment, the function will use the values set by its arguments, not the ones set in your &lt;strong&gt;global environment&lt;/strong&gt; (what you can see in the Environment tab in RStudio).&lt;/p&gt;
&lt;p&gt;In other words, a function has access to variables in the global environment, but also has variables created by that function’s arguments, which can overwrite the values of variables with the same name in the global environment.&lt;/p&gt;
&lt;p&gt;For that reason, I advise new coders to avoid giving the parameter values in their global environment the same names as the arguments of the functions they are used in. this is fine, but can lead to confusion unless you have a very clear conceptual understanding of scope.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_func4 &amp;lt;- function(my_n = 100, my_mu = 0, my_sd = 1, my_r = 0) {
  # simulate data
  dat &amp;lt;- faux::rnorm_multi(
    n = my_n,
    vars = 2,
    mu = my_mu,
    sd = my_sd,
    r = my_r,
    varnames = c(&amp;quot;low&amp;quot;, &amp;quot;high&amp;quot;)
  )
  
  # test high-low difference
  t.test(dat$high, dat$low, paired = TRUE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, the function above has arguments called &lt;code&gt;my_n&lt;/code&gt;, &lt;code&gt;my_mu&lt;/code&gt;, &lt;code&gt;my_sd&lt;/code&gt; and &lt;code&gt;my_r&lt;/code&gt;. We can also create variables with those same names in the global environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# my data-generating parameters
my_n &amp;lt;- 50
my_mu &amp;lt;- c(0, 0.2)
my_sd &amp;lt;- c(1, 1)
my_r &amp;lt;- 0.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, when you run the function without setting the arguments in the function, it uses the default values of the arguments, not the values you set in the global environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_func4()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Paired t-test
## 
## data:  dat$high and dat$low
## t = -0.25649, df = 99, p-value = 0.7981
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.3074508  0.2370637
## sample estimates:
## mean of the differences 
##             -0.03519355&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you set the values in the function, then it will work as expected.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_func4(my_n, my_mu, my_sd, my_r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Paired t-test
## 
## data:  dat$high and dat$low
## t = 4.0469, df = 49, p-value = 0.0001838
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.2497333 0.7423856
## sample estimates:
## mean of the differences 
##               0.4960594&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Mann-Whitney False Positives</title>
      <link>https://debruine.github.io/post/mann-whitney/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/mann-whitney/</guid>
      <description>


&lt;p&gt;One of my favourite colleagues, &lt;a href=&#34;https://twitter.com/McAleerP&#34;&gt;Phil McAleer&lt;/a&gt;, asked about unequal sample sizes for Mann-Whitney tests on our group chat today. I had no idea, so, as always, I thought “This is a job for simulations!”&lt;/p&gt;
&lt;p&gt;I started by loading tidyverse, since I know I’ll need to wrangle data and plot things. I’m starting to get more comfortable with base R for package development, and it can make things faster, but tidyverse is my favourite for a quick analysis or a pipeline where understandability is more important than speed.&lt;/p&gt;
&lt;p&gt;And I set my favourite seed so my simulations will give me a reproducible answer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
set.seed(8675309)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I wrote a wee function to simulate data with the parameters I’m interested in varying, run a Mann-Whitney test, and return the p-value (all I need to look at power and false positives).&lt;/p&gt;
&lt;p&gt;First, I just wanted to look at false positives for different sample size, so I set &lt;code&gt;n1&lt;/code&gt; and &lt;code&gt;n2&lt;/code&gt; as arguments and set &lt;code&gt;alternative&lt;/code&gt; with a default of “two.sided”. The function &lt;code&gt;wilcox.test&lt;/code&gt; runs a Mann-Whitney test for independent samples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mw &amp;lt;- function(n1, n2, alternative = &amp;quot;two.sided&amp;quot;) {
  x1 &amp;lt;- rnorm(n1)
  x2 &amp;lt;- rnorm(n2)
  w &amp;lt;- wilcox.test(x1, x2, alternative = alternative)
  w$p.value
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I need to set up a table with all of the values I want to run simulations for. I set &lt;code&gt;n1&lt;/code&gt; and &lt;code&gt;n2&lt;/code&gt; to the numbers 10 to 100 in steps of ten. This was crossed with the number of replications I wanted to run (1000). I then removed the values where &lt;code&gt;n2&lt;/code&gt; &amp;gt; &lt;code&gt;n1&lt;/code&gt;, since they’re redundant with the opposite version (e.g., &lt;code&gt;n1 = 10, n2 = 20&lt;/code&gt; is the same as &lt;code&gt;n1 = 20, n2 = 10&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;params &amp;lt;- expand.grid(
  n1 = seq(10, 100, 10), 
  n2 = seq(10, 100, 10),
  reps = 1:1000
) %&amp;gt;%
  filter(n1 &amp;lt;= n2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;n1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;reps&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;…&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;…&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;90&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I then used the &lt;code&gt;pmap_dbl&lt;/code&gt; function from &lt;code&gt;purrr&lt;/code&gt; to map the values from &lt;code&gt;n1&lt;/code&gt; and &lt;code&gt;n2&lt;/code&gt; onto &lt;code&gt;mw&lt;/code&gt;, then grouped the results by &lt;code&gt;n1&lt;/code&gt; and &lt;code&gt;n2&lt;/code&gt; and calculated &lt;code&gt;false_pos&lt;/code&gt; as the proportion of &lt;code&gt;p&lt;/code&gt; less than &lt;code&gt;alpha&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha &amp;lt;- 0.05

mw1 &amp;lt;- params %&amp;gt;%
  mutate(p = pmap_dbl(list(n1, n2), mw)) %&amp;gt;%
  group_by(n1, n2) %&amp;gt;%
  summarise(false_pos = mean(p &amp;lt; alpha), .groups = &amp;quot;drop&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I plotted the false positive rate for each combination against the difference between &lt;code&gt;n1&lt;/code&gt; and &lt;code&gt;n2&lt;/code&gt;. You can see that the false positive rate is approximately nominal, or equal to the specified &lt;code&gt;alpha&lt;/code&gt; of 0.05.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mw1, aes(n2 - n1, false_pos)) +
  geom_point(aes(color = as.factor(n1))) +
  geom_smooth(formula = y ~ x, method = lm, color = &amp;quot;black&amp;quot;) +
  labs(x = &amp;quot;N2 - N1&amp;quot;, 
         y = &amp;quot;False Positive Rate&amp;quot;,
         color = &amp;quot;N1&amp;quot;) +
  ylim(0, .1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/mann-whitney_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But what if data aren’t drawn from a normal distribution? We can change the &lt;code&gt;mw()&lt;/code&gt; function to simulate data from a different distribtion, such as uniform, and run the whole process again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mw &amp;lt;- function(n1, n2, alternative = &amp;quot;two.sided&amp;quot;) {
  x1 &amp;lt;- runif(n1)
  x2 &amp;lt;- runif(n2)
  w &amp;lt;- wilcox.test(x1, x2, alternative = alternative)
  w$p.value
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The rest of our code is identical to above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mw2 &amp;lt;- params %&amp;gt;%
  mutate(p = pmap_dbl(list(n1, n2), mw)) %&amp;gt;%
  group_by(n1, n2) %&amp;gt;%
  summarise(false_pos = mean(p &amp;lt; alpha), .groups = &amp;quot;drop&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This doesn’t seem to make much difference.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/mann-whitney_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What if the variance between the two samples is different? First, let’s adjust the &lt;code&gt;mw()&lt;/code&gt; function to vary the SD of the two samples. We’ll give &lt;code&gt;sd1&lt;/code&gt; a default value of 1 and &lt;code&gt;sd2&lt;/code&gt; will default to the same as &lt;code&gt;sd1&lt;/code&gt;. We might as well add the option to change the means, so default &lt;code&gt;m1&lt;/code&gt; to 0 and &lt;code&gt;m2&lt;/code&gt; to the same as &lt;code&gt;m1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mw &amp;lt;- function(n1, m1 = 0, sd1 = 1, 
               n2 = n1, m2 = m1, sd2 = sd1,
               alternative = &amp;quot;two.sided&amp;quot;) {
  x1 &amp;lt;- rnorm(n1, m1, sd1)
  x2 &amp;lt;- rnorm(n2, m2, sd2)
  w &amp;lt;- wilcox.test(x1, x2, alternative = alternative)
  w$p.value
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to set up a new list of parameters to change. The Ns didn’t make much difference last time, so let’s vary them in steps of 20 this time. We’ll vary &lt;code&gt;sd1&lt;/code&gt; and &lt;code&gt;sd2&lt;/code&gt; from 0.5 to 2 in steps of 0.5, and also only keep combinations where &lt;code&gt;sd1&lt;/code&gt; is less than or equal to &lt;code&gt;sd2&lt;/code&gt; to avoid redundancy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;params &amp;lt;- expand.grid(
  reps = 1:1000,
  n1 = seq(10, 100, 20), 
  n2 = seq(10, 100, 20),
  sd1 = seq(0.5, 2, 0.5),
  sd2 = seq(0.5, 2, 0.5)
) %&amp;gt;%
  filter(n1 &amp;lt;= n2, sd1 &amp;lt;= sd2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mw3 &amp;lt;- params %&amp;gt;%
  mutate(p = pmap_dbl(list(n1, 0, sd1, n2, 0, sd2), mw)) %&amp;gt;%
  group_by(n1, n2, sd1, sd2) %&amp;gt;%
  summarise(false_pos = mean(p &amp;lt; alpha), .groups = &amp;quot;drop&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like differences in SD make a big difference in the false positive rate, and the effect is bigger as Ns and SDs get more unequal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mw3, aes(sd2 - sd1, false_pos, color = as.factor(n2-n1))) +
  geom_point() +
  geom_smooth(formula = y ~ x, method = lm) +
  labs(x = &amp;quot;SD2 - SD1&amp;quot;, 
         y = &amp;quot;False Positive Rate&amp;quot;,
         color = &amp;quot;N2-N1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/mann-whitney_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’ll leave it to the enterprising reader to simulate power for different effect sizes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How many simulations in my power analysis?</title>
      <link>https://debruine.github.io/post/how-many-sims/</link>
      <pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/how-many-sims/</guid>
      <description>


&lt;p&gt;Today I was trying to figure out how to advise on the number of simulations to run when calculating power by simulation.&lt;/p&gt;
&lt;p&gt;I tackled this question by running a simulation (of course).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(dplyr) # I love pipes
set.seed(8675309)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I wanted to figure out how close to the true power was the calculated power from a simulation where the number of replications ranges from 100 to 10K (in steps of 100) and power ranges from 0.5 to 1 in steps of .05 (the result is symmetric around 50%, so the figures below for 80% power also apply to 20% power)..&lt;/p&gt;
&lt;p&gt;First, I made all possible combinations of replications and power.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- expand.grid(
  reps = seq(100, 1e4, 100),
  power = seq(0.5, 1, .05)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, for each combination, I calculated the proportion of significant analyses in 10K simulations. I assumed this would have a binomial distribution where size is the number of replications in each simulation and probability is the true power. I then calculated the absolute difference from the true value of power and reported the mean (I find it more intuitive than SD or variance).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x$diff &amp;lt;- mapply(function(size, prob) {
  sig &amp;lt;- rbinom(1e4, size, prob) / size
  diff &amp;lt;- abs(sig - prob)
  mean(diff)
}, size = x$reps, prob = x$power)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I plotted the results to see if they make sense. As the number of replications per simulation increases, the mean difference from the true power decreases. Accuracy is higher for larger values of power.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/how-many-sims_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I also calculated the minimum number of replications to get a result that is, on average, less than 1% off from a power of 80%&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(x, power == .8, diff &amp;lt; .01) %&amp;gt;% 
  pull(reps) %&amp;gt;%  min()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also calculated the .95 quantile to see how many replications you need to run to get within 1% of the true value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x$q95 &amp;lt;- mapply(function(size, prob) {
  sig &amp;lt;- rbinom(1e4, size, prob) / size
  diff &amp;lt;- abs(sig - prob)
  quantile(diff, .95)
}, size = x$reps, prob = x$power)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/how-many-sims_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Turns out you need a lot more.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(x, power == .8, q95 &amp;lt; .01) %&amp;gt;% 
  pull(reps) %&amp;gt;%  min()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6300&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Testing interactive functions</title>
      <link>https://debruine.github.io/post/interactive-test/</link>
      <pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/interactive-test/</guid>
      <description>


&lt;p&gt;I’m a huge fan of unit tests, but it’s tricky to test interactive functions where the user needs to enter input before the function can progress. I used to test them manually, which is incredibly tedious and time-consuming. So I ended up not testing interative functions very thoroughly. I found a &lt;a href=&#34;https://stackoverflow.com/questions/41372146/test-interaction-with-users-in-r-package&#34;&gt;post on Stack Overflow&lt;/a&gt; with a useful answer by &lt;a href=&#34;https://stackoverflow.com/users/2752888/znk&#34;&gt;znk&lt;/a&gt;. I’ve expanded their answer into a full example of a unit test for an interactive function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(testthat)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;set-up-the-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Set up the function&lt;/h1&gt;
&lt;p&gt;Your function needs to use &lt;code&gt;readLines&lt;/code&gt; to get interactive input and take an argument for the connection (&lt;code&gt;con&lt;/code&gt;). The default value for the connection should be the same as its default value for &lt;code&gt;readLines&lt;/code&gt;, which is &lt;code&gt;stdin()&lt;/code&gt; (the terminal). You can’t use &lt;code&gt;readline&lt;/code&gt;, which only supports connection to the terminal.&lt;/p&gt;
&lt;p&gt;This function displays a prompt and a list of valid options. If your response isn’t in the list, it will repeat the prompt until it is.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ask_opts &amp;lt;- function(prompt, opts = NULL, con = stdin()) {
  # display prompt and options
  optlist &amp;lt;- paste(opts, collapse = &amp;quot;|&amp;quot;)
  prompt_opt &amp;lt;- paste0(prompt, &amp;quot; [&amp;quot;, optlist, &amp;quot;]\n&amp;quot;)
  cat(prompt_opt)
  input &amp;lt;- readLines(con = con, n = 1)
  
  # repeat if input is not in opts
  if (!is.null(opts) &amp;amp; !input %in% opts) {
    input &amp;lt;- ask_opts(prompt, opts, con)
  }

  input
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;set-up-the-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set up the Test&lt;/h2&gt;
&lt;p&gt;You need to create a file containing the input you want to send to the function, one input per line. I want to answer the first time with something not in the option list, then the second time with something that is in the option list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set up interactive answers
f &amp;lt;- file()
lines &amp;lt;- c(&amp;quot;echidna&amp;quot;, &amp;quot;ferret&amp;quot;)
ans &amp;lt;- paste(lines, collapse = &amp;quot;\n&amp;quot;)
write(ans, f)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then run your interactive function, setting the connection to your file. Run it inside &lt;code&gt;capture_output_lines&lt;/code&gt; if you want to test the prompts and not just the output. Close the file when you are done with it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prompt &amp;lt;- &amp;quot;What is your favourite animal?&amp;quot;
opts &amp;lt;- c(&amp;quot;cat&amp;quot;, &amp;quot;dog&amp;quot;, &amp;quot;ferret&amp;quot;)

output_prompts &amp;lt;- capture_output_lines({
  result &amp;lt;- ask_opts(prompt, opts, f)
})

close(f) # close the file&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can run your tests&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;txt &amp;lt;- &amp;quot;What is your favourite animal? [cat|dog|ferret]&amp;quot;
expect_equal(result, &amp;quot;ferret&amp;quot;)
expect_equal(output_prompts, rep(txt, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;without-a-new-argument&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Without a new argument&lt;/h2&gt;
&lt;p&gt;What if you don’t want to change the arguments to your function to add a connection? You can set the connection in the options and test for it in the function, defaulting to &lt;code&gt;stdin()&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ask_opts &amp;lt;- function(prompt, opts = NULL) {
  # set up connection, default to stdin() if not set
  con &amp;lt;- getOption(&amp;quot;ask_opts.con&amp;quot;, stdin())
  
  # display prompt and options
  optlist &amp;lt;- paste(opts, collapse = &amp;quot;|&amp;quot;)
  prompt_opt &amp;lt;- paste0(prompt, &amp;quot; [&amp;quot;, optlist, &amp;quot;]\n&amp;quot;)
  cat(prompt_opt)
  input &amp;lt;- readLines(con = con, n = 1)
  
  # repeat if input is not in opts
  if (!is.null(opts) &amp;amp; !input %in% opts) {
    input &amp;lt;- ask_opts(prompt, opts)
  }

  input
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you just need to set this option before you run the interactive function in your testing environment. Make sure to reset it to &lt;code&gt;stdin()&lt;/code&gt; when you’re done.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_that(&amp;quot;interactive&amp;quot;, {
  # set up interactive answers
  f &amp;lt;- file()
  lines &amp;lt;- c(&amp;quot;maybe&amp;quot;, &amp;quot;y&amp;quot;)
  ans &amp;lt;- paste(lines, collapse = &amp;quot;\n&amp;quot;)
  write(ans, f)
  
  options(&amp;quot;ask_opts.con&amp;quot; = f) # set connection option 
  
  # run interactive function
  prompt &amp;lt;- &amp;quot;Was this helpful?&amp;quot;
  opts &amp;lt;- c(&amp;quot;y&amp;quot;, &amp;quot;n&amp;quot;)
  
  output_prompts &amp;lt;- capture_output_lines({
    result &amp;lt;- ask_opts(prompt, opts)
  })
  
  close(f) # close the file
  options(&amp;quot;ask_opts.con&amp;quot; = stdin()) # reset connection option
  
  # tests
  txt &amp;lt;- &amp;quot;Was this helpful? [y|n]&amp;quot;
  expect_equal(result, &amp;quot;y&amp;quot;)
  expect_equal(output_prompts, rep(txt, 2))
})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Inputting data table rows as function arguments</title>
      <link>https://debruine.github.io/post/pmap_df/</link>
      <pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/pmap_df/</guid>
      <description>


&lt;p&gt;I was working on a simulation project with an undergraduate dissertation student today (I’m so amazed at what our students can do now!) and wanted to show her how to efficiently run simulations for all combinations of a range of parameters. It took 20 minutes of googling map functions in purrr to figure it out. I find I have to do this every time I want to use this pattern, so I decided to write a quick tutorial on it.&lt;/p&gt;
&lt;p&gt;You’ll need functions from the purrr library, as well as some dplyr and tidyr functions, so I just load the whole tidyverse.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fisrt, I need to define the function I want to run for the imulation. I’ll make a relatively simple one, that takes the samples sizes, means and standard deviations for two samples, simulates data, and returns the sample effect size, t-value, p-value, and degrees of freedom from &lt;code&gt;t.test&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_t_sim &amp;lt;- function(n1 = 100, m1 = 0, sd1 = 1,
                     n2 = 100, m2 = 0, sd2 = 1) {
  # simulate data
  grp1 &amp;lt;- rnorm(n1, m1, sd1)
  grp2 &amp;lt;- rnorm(n2, m2, sd2)
  
  # analyse
  tt &amp;lt;- t.test(grp1, grp2)
  
  # calculate cohens d for independent samples
  s_pooled &amp;lt;- sqrt(((n1-1) * sd(grp1)^2 + (n2-1) *
                      sd(grp2)^2)/(n1+n2))
  d &amp;lt;- (tt$estimate[[1]] - tt$estimate[[2]]) / s_pooled
  
  # return named list of values
  list(d = d,
       t  = tt$statistic[[1]],
       df = tt$parameter[[1]],
       p  = tt$p.value)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can simulate a study with 20 observations in each group and an effect size of 0.5.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_t_sim(n1 = 20, m1 = 100, sd1 = 10, 
         n2 = 20, m2 = 105, sd2 = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $d
## [1] -0.7037
## 
## $t
## [1] -2.169
## 
## $df
## [1] 33.89
## 
## $p
## [1] 0.0372&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to run it 100 times, you can use the &lt;code&gt;map_df()&lt;/code&gt; function to create a data frame of the results for each repeat.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- map_df(1:100, ~my_t_sim(n1 = 20, m1 = 100, sd1 = 10, 
                                   n2 = 20, m2 = 105, sd2 = 10))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;d&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.6277&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.9346&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.22&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0613&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.2027&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.6247&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;38.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5359&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.1913&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.5898&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.22&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5591&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.7094&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.1865&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37.58&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0351&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.4612&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1.4215&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;38.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1633&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.9266&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-2.8560&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37.52&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0070&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But this only lets you run one set of arguments for n1, n2, m1, m2, sd1, and sd2. What if you want to run the function 100 times for each of a range of parameters?&lt;/p&gt;
&lt;p&gt;First, set up a data frame that contains every combination of parameters you want to explore using the &lt;code&gt;crossing()&lt;/code&gt; function. The function &lt;code&gt;seq()&lt;/code&gt; makes a vector ranging from the first argument to the second, in steps of the third (e.g., &lt;code&gt;seq(30, 60, 5)&lt;/code&gt; makes the vector &lt;code&gt;c(30, 35, 40, 45, 50, 55, 60)&lt;/code&gt;). If you don’t want to vary a parameter, set it to a single value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;params &amp;lt;- crossing(
  n1 = seq(30, 120, 5),
  m1 = seq(0, 0.5, 0.1),
  sd1 = 1,
  m2 = 0,
  sd2 = 1
) %&amp;gt;%
  mutate(n2 = n1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can now use the function &lt;code&gt;pmap_dfr&lt;/code&gt; to iterate over the rows of the &lt;code&gt;params&lt;/code&gt; data table, using the values as arguments to the function &lt;code&gt;my_t_sim&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- pmap_dfr(params, my_t_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;d&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-0.2321&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.8836&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;55.35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3807&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.3326&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2663&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;53.97&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2108&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.2874&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0944&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;56.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2784&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.2734&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0410&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50.49&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3028&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.5160&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9647&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;57.99&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0542&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.1670&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6361&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;57.81&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5273&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can also wrap this in an anonymous function and do some more processing on the results, like running each combination 100 times and adding the parameters to the data table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- pmap_dfr(params, function(...) {
  args &amp;lt;- list(...) # get list of named arguments
  # run 500 replications per set of parameters
  map_df(1:500, ~my_t_sim(n1 = args$n1, m1 = args$m1, sd1 = args$sd1,
                        n2 = args$n2, m2 = args$m2, sd2 = args$sd2)) %&amp;gt;%
    mutate(!!!args) # add columns to specify arguments
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The three dots in &lt;code&gt;function(...)&lt;/code&gt; lets this function takes any named arguments. You need to assign that list of arguments using &lt;code&gt;args &amp;lt;- list(...)&lt;/code&gt; and then you can use the arguments in your code (e.g., &lt;code&gt;args$n1&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The triple bang (&lt;code&gt;!!!&lt;/code&gt;) expands a list in tidyverse functions. For example, &lt;code&gt;mutate(!!!args)&lt;/code&gt; is equivalent to &lt;code&gt;mutate(n1 = args$n1, m1 = args$m1, sd1 = args$sd1, n2 = args$n2, m2 = args$m2, sd2 = args$sd2)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now you have a data table with 57000 results. You can summarise or graph these results to look at how varying parameters systematically affects things like effect size or power.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results %&amp;gt;%
  group_by(n1, n2, m1, m2) %&amp;gt;%
  summarise(power = mean(p &amp;lt; .05)) %&amp;gt;%
  ggplot(aes(n1, power, color = as.factor(m1))) +
  geom_hline(yintercept = 0.05) +
  geom_hline(yintercept = 0.80) +
  geom_point() +
  geom_line() +
  scale_color_discrete(name = &amp;quot;Effect Size&amp;quot;) +
  xlab(&amp;quot;Number of observations per group&amp;quot;) +
  scale_y_continuous(breaks = seq(0,1,.2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/pmap_dfr_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Composite Images</title>
      <link>https://debruine.github.io/post/composite-images/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/composite-images/</guid>
      <description>
&lt;script src=&#34;https://debruine.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I recently responded to a &lt;a href=&#34;https://twitter.com/PsychoSchmitt/status/1221883383778811906?s=20&#34;&gt;tweet&lt;/a&gt; about a preprint about whether people can see Dark Triad traits (narcissism, Machiavellianism, and psychopathy) in facial appearance.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Can you tell a Dark Triad person from the face? Apparently so (data from USA and Turkey). &lt;a href=&#34;https://t.co/BxZUcJ9cTY&#34;&gt;https://t.co/BxZUcJ9cTY&lt;/a&gt; &lt;a href=&#34;https://t.co/xh1pcmyB5E&#34;&gt;pic.twitter.com/xh1pcmyB5E&lt;/a&gt;
&lt;/p&gt;
— David Schmitt (&lt;span class=&#34;citation&#34;&gt;@PsychoSchmitt&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/PsychoSchmitt/status/1221883383778811906?ref_src=twsrc%5Etfw&#34;&gt;January 27, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;The &lt;a href=&#34;https://psyarxiv.com/c3ngz/&#34;&gt;preprint&lt;/a&gt; by Alper, Bayrak, and Yilmaz used faces from the &lt;a href=&#34;http://www.nickholtzman.com/faceaurus.htm&#34;&gt;Faceaurus database&lt;/a&gt; (Holtzman, 2011). “Holtzman (2011) standardized the assessment scores, computed average scores of self- and peer-reports, and ranked the face images based on the resulting scores. Then, prototypes for each of the personality dimensions were created by digitally combining 10 faces with the highest and 10 faces with the lowest scores on the personality trait in question (Holtzman, 2011).” This was done separately for male and female faces.&lt;/p&gt;
&lt;p&gt;Since scores on the three dark triad traits are positively correlated, the three pairs of composite faces are not independent. Indeed, Holtzman states that 5 individuals were in all three low composites for the male faces, while the overlap was less extreme in other cases. With 105 observers, Holtzman found that the ability to detect the composite higher in a dark triad trait was greater than chance.&lt;/p&gt;
&lt;p&gt;While I commend both Holtzman and Alper, Bayrak, and Yilmaz for their transparency, data sharing, and material sharing, &lt;strong&gt;I am arguing that this test has an effective N of 2, and that further replications using these images, such as those done by Alper, Bayrak, and Yilmaz, regardless of number of observers or preregistered status, lend no further weight of evidence to the assertion that dark triad traits are visible in physical appearance.&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;womens-height&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Women’s height&lt;/h3&gt;
&lt;p&gt;Let’s go back to my favourite example for demonstrating the problems with aggregating ratings before analysis, &lt;a href=&#34;https://debruine.github.io/tutorials/aggregate.html&#34;&gt;Armenian women’s height&lt;/a&gt;. The problem is the same here, but we’ve just averaged the stimuli before rating, rather than averaging the ratings of individual stimuli.&lt;/p&gt;
&lt;p&gt;First, we’re going to simulate a sample of 20 women from a population with a mean height of 158.1 cm and an SD of 5.7. Half are born on odd days and half on even days.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(8675309)

stim_n &amp;lt;- 10
height_m &amp;lt;- 158.1
height_sd &amp;lt;- 5.7

odd &amp;lt;- rnorm(stim_n, height_m, height_sd)
even &amp;lt;- rnorm(stim_n, height_m, height_sd)

t.test(odd, even)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  odd and even
## t = 1.7942, df = 17.409, p-value = 0.09016
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.7673069  9.5977215
## sample estimates:
## mean of x mean of y 
##  161.1587  156.7435&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A t-test shows no significant difference, which is unsurprising. We simulated the data from the same distributions, so we know for sure there is no real difference here. Now we’re going to average the height of the women with odd and even birthdays.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;odd_mean &amp;lt;- mean(odd)
even_mean &amp;lt;- mean(even)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So if we create a composite of women born on odd days, she would be 161.2 cm tall, and a composite of women born on even days would be 156.7 cm tall.&lt;/p&gt;
&lt;p&gt;If we ask 100 observers to look at these two composites and judge which one looks taller, what do you imagine would happen? Let’s say that observers are pretty bad with height estimation, and their estimates for each composite have error with a standard deviation of 10 cm. They then judge whether, by their estimation, the odd-birthday composite looks taller than the even-birthday composite.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obs_n &amp;lt;-100
error_sd &amp;lt;- 10

odd_estimate &amp;lt;- odd_mean + rnorm(obs_n, 0, error_sd)
even_estimate &amp;lt;- even_mean + rnorm(obs_n, 0, error_sd)

judgment &amp;lt;- odd_estimate &amp;gt; even_estimate

bt &amp;lt;- binom.test(sum(judgment), obs_n, p = 0.5) %&amp;gt;% print()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Exact binomial test
## 
## data:  sum(judgment) and obs_n
## number of successes = 65, number of trials = 100, p-value = 0.003518
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.5481506 0.7427062
## sample estimates:
## probability of success 
##                   0.65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A binomial test shows that they are significantly better than chance at this (p = 0.004). What’s going on?&lt;/p&gt;
&lt;p&gt;We can be sure that by chance alone, our two composites will be at least slightly different on any measure, even if they are drawn from identical populations. The mean (unsigned) size of this difference is larger, the smaller the number of stimuli that go into each composite. The graph below shows simulations of the unsigned difference between composites for 1000 samples per number of stimuli per composite.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/composite_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- replicate(10000, mean(rnorm(10))-mean(rnorm(10)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With only 10 stimuli per composite, the mean unsigned effect size of the difference between composites is 0.36 (in units of SD of the original trait distribution). 65% of random pairs have a difference of greater than 0.2 SD. If our observers are accurate enough at perceiving this difference or we run a very large number of observers, we are virtually guarateed to find significant results every time, and we have a 50% chance that all of these results will be in the predicted direction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;personality-traits-and-faces&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Personality Traits and Faces&lt;/h3&gt;
&lt;p&gt;So what does this mean for studies of the link between personality traits and facial appearance? The analogy with birth date and height holds. As long as there are facial morphologies that are even slightly consistently associated with the perception of a trait, then composites will not be identical in that morphology, even if it is totally unassociated with the trait as measured by, e.g., personality scales or peer report.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The smaller the number of stimuli that go into each composite, the greater the chance that they will be visibly different in morphology related to the judgment of interest, just by chance alone.&lt;/li&gt;
&lt;li&gt;The larger the number of observers or the better observers are at detecting small differences in this morphology, the more likley that “detection” will be significantly above chance.&lt;/li&gt;
&lt;li&gt;Repeating this with a new set of observers does not increase the amount of evidence you have for the association between the face morphology and the measured trait. You’ve only measured it once in one population of faces.&lt;/li&gt;
&lt;li&gt;If observers are your unit of analyses, you are making conclusions about whether the population of observers can detect the difference between your stimuli, you cannot generalise this to new stimulus sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Alper, S., Bayrak, F., &amp;amp; Yilmaz, O. (2020, January 27). All the Dark Triad and Some of the Big Five Traits are Visible in the Face. &lt;a href=&#34;https://doi.org/10.31234/osf.io/c3ngz&#34; class=&#34;uri&#34;&gt;https://doi.org/10.31234/osf.io/c3ngz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Holtzman, N. S. (2011). Facing a psychopath: Detecting the dark triad from emotionally-neutral faces, using prototypes from the Personality Faceaurus. Journal of Research in Personality, 45, 648-654. &lt;a href=&#34;https://doi.org/10.1016/j.jrp.2011.09.002&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jrp.2011.09.002&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What&#39;s wrong with aggregating data?</title>
      <link>https://debruine.github.io/post/aggregating/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/aggregating/</guid>
      <description>
&lt;script src=&#34;https://debruine.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p class=&#34;info&#34;&gt;
&lt;a href=&#34;http://shiny.psy.gla.ac.uk/debruine/anova_vs_lmer/&#34;&gt;Shiny app&lt;/a&gt; for a face-rating example.
&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lmerTest)
set.seed(90210)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Imagine you want to find out if Armenian women born on an even-numbered day are taller than women born on an odd-numbered day. (I’ve chosen Armenian women because they’re the first row in &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0018962&#34;&gt;this paper&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;First, let’s simulate a group of 20 women born on even-numbered days and 20 women born on odd-numbered days.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stim_n &amp;lt;- 20
# height stats from https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0018962
height_m &amp;lt;- 158.1
height_sd &amp;lt;- 5.7

stim &amp;lt;- tibble(
  stim_id = 1:(stim_n*2),
  birthday = rep(c(&amp;quot;odd&amp;quot;, &amp;quot;even&amp;quot;), stim_n),
  height = rnorm(stim_n*2, height_m, height_sd)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/aggregation_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Obviously, the oddness of date of birth is not going to have any effect on women’s actual height and a two-sample t-test confirms this. However, there is a small difference between the means of the groups just due to chance (2.81 cm).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(stim$height ~ stim$birthday)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  stim$height by stim$birthday
## t = -1.5, df = 38, p-value = 0.1
## alternative hypothesis: true difference in means between group even and group odd is not equal to 0
## 95 percent confidence interval:
##  -6.4997  0.8767
## sample estimates:
## mean in group even  mean in group odd 
##              154.9              157.7&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;measurement-with-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Measurement with Error&lt;/h3&gt;
&lt;p&gt;But what if we don’t measure height from each women once, but have a few different raters estimate it? The raters will each have their own bias, systematically overestimating or underestimating height on average. Let’s simulate 20 raters who have biases with an SD of 2 cm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rater_n &amp;lt;- 20
rater_bias_sd &amp;lt;- 2

rater &amp;lt;- tibble(
  rater_id = 1:rater_n,
  rater_bias = rnorm(rater_n, 0, rater_bias_sd)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/aggregation_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;New we can get each rater to estimate the height of each woman. Their estimate is the woman’s actual height, plus the rater’s bias, plus some error (sampled from a normal distribution with a mean of 0 and an SD of 4 cm, since estimating height is hard).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- expand.grid(
  rater_id = rater$rater_id,
  stim_id = stim$stim_id
) %&amp;gt;%
  left_join(rater, by = &amp;quot;rater_id&amp;quot;) %&amp;gt;%
  left_join(stim, by = &amp;quot;stim_id&amp;quot;) %&amp;gt;%
  mutate(
    error = rnorm(nrow(.), 0, 4),
    estimate = height + rater_bias + error
  )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregating-by-stimuli&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Aggregating by Stimuli&lt;/h3&gt;
&lt;p&gt;You can aggregate by stimuli, that is, average the 20 raters’ estimate for each stimulus. You now have 40 mean estimates that are fairly well-correlated with the women’s actual heights.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_agg_by_stim &amp;lt;- dat %&amp;gt;%
  group_by(stim_id, birthday, height) %&amp;gt;%
  summarise(mean_estimate = mean(estimate))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;stim_id&amp;#39;, &amp;#39;birthday&amp;#39;. You can override using the `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/aggregation_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You get pretty much the same result when you compare these mean estimates between the groups of women with odd and even birthdays.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(dat_agg_by_stim$mean_estimate ~ dat_agg_by_stim$birthday)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  dat_agg_by_stim$mean_estimate by dat_agg_by_stim$birthday
## t = -1.4, df = 38, p-value = 0.2
## alternative hypothesis: true difference in means between group even and group odd is not equal to 0
## 95 percent confidence interval:
##  -6.473  1.130
## sample estimates:
## mean in group even  mean in group odd 
##              155.2              157.9&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;aggregating-by-raters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Aggregating by Raters&lt;/h3&gt;
&lt;p&gt;Alternatively, you can aggregate by raters, that is, average the 20 odd-group estimates and 20 even-group estimates for each rater. Now raters are your unit of analysis, so you’ve increased your power by having 20 raters and a within-subject design (each rater estimates heights for both odd- and even-birthday groups).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_agg_by_rater &amp;lt;- dat %&amp;gt;%
  group_by(rater_id, birthday) %&amp;gt;%
  summarise(mean_estimate = mean(estimate)) %&amp;gt;%
  spread(birthday, mean_estimate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;rater_id&amp;#39;. You can override using the `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(dat_agg_by_rater$odd, dat_agg_by_rater$even, paired = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Paired t-test
## 
## data:  dat_agg_by_rater$odd and dat_agg_by_rater$even
## t = 11, df = 19, p-value = 0.000000002
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  2.145 3.198
## sample estimates:
## mean of the differences 
##                   2.672&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the difference between the odd- and even-birthday groups is highly significant! What’s going is that you now have a relatively accurate estimate of the chance mean difference between the 20 women in the odd-birthday group and the 20 women in the even-birthday group. Since raters are the unit of analysis, this effect is likely to generalise to the larger population of potential raters, but only when they are rating &lt;strong&gt;these exact stimuli&lt;/strong&gt;. Your conclusions cannot generalise beyond the stimulus set used here.&lt;/p&gt;
&lt;p&gt;While this seems like an obvious problem when the question is whether Armenian women with odd birthdays are taller or shorter than Armenian women with even birthdays, the problem is not so obvious for other questions, like whether boxers who won their last match have more masculine faces than boxers who lost their last match. The point of this tutorial isn’t to call out any particular studies (I’ve certainly done this wrong myself plenty of times in the past), but to illustrate the enormous problem with this method and to explain the solution.&lt;/p&gt;
&lt;p&gt;The larger the number of raters, the larger this false positive problem becomes because you’re increasing power to detect the small chance diffference between the two groups. You can play around with how changing parameters changes the power and false positive rates for by-stimulus, by-rater, and mixed effect designs at &lt;a href=&#34;http://shiny.psy.gla.ac.uk/debruine/anova_vs_lmer/&#34;&gt;this shiny app&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mixed-effect-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mixed Effect Model&lt;/h3&gt;
&lt;p&gt;In the particular case above, we’re only interested in the between-stimulus (and within-rater) main effect of birthday oddness. Therefore, aggregating by stimuli doesn’t inflate your false positive rate, while aggregating by rater does. However, other designs might have increased false positives for aggregating by stimuli but not by rater, or when aggregating by either.&lt;/p&gt;
&lt;p&gt;A mixed effects model avoids the problems of aggregation completely by modelling random variation for both the stimuli and raters, as well as random variation in the size of within-group effects.&lt;/p&gt;
&lt;p class=&#34;info&#34;&gt;
I &lt;a href=&#34;https://debruine.github.io/posts/coding-schemes/&#34;&gt;effect code&lt;/a&gt; the &lt;code&gt;birthday&lt;/code&gt; variable to make interpretation of the effects easier).
&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# effect-code birthday
dat$birthday.e &amp;lt;- recode(dat$birthday, &amp;quot;odd&amp;quot; = 0.5, &amp;quot;even&amp;quot; = -0.5)

mod &amp;lt;- lmer(estimate ~ birthday.e +
              # random slope for effect of birthday, random intercept for rater bias
              (1 + birthday.e || rater_id) + 
              # random intercept for variation in stim height
              (1 | stim_id), 
            data = dat)

summary(mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML. t-tests use Satterthwaite&amp;#39;s method [
## lmerModLmerTest]
## Formula: estimate ~ birthday.e + (1 + birthday.e || rater_id) + (1 | stim_id)
##    Data: dat
## 
## REML criterion at convergence: 4687
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.8414 -0.6590  0.0102  0.6776  2.6231 
## 
## Random effects:
##  Groups     Name        Variance      Std.Dev.
##  stim_id    (Intercept) 34.4640965341 5.870613
##  rater_id   birthday.e   0.0000000214 0.000146
##  rater_id.1 (Intercept) 10.0890113704 3.176320
##  Residual               15.8201186615 3.977451
## Number of obs: 800, groups:  stim_id, 40; rater_id, 20
## 
## Fixed effects:
##             Estimate Std. Error     df t value Pr(&amp;gt;|t|)    
## (Intercept)   156.55       1.18  55.02  132.98   &amp;lt;2e-16 ***
## birthday.e      2.67       1.88  38.00    1.42     0.16    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Correlation of Fixed Effects:
##            (Intr)
## birthday.e 0.000 
## optimizer (nloptwrap) convergence code: 0 (OK)
## boundary (singular) fit: see ?isSingular&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The estimate for &lt;code&gt;(Intercept)&lt;/code&gt; is just the mean height estimate (156.55 cm) and the estimate for &lt;code&gt;birthday.e&lt;/code&gt; is the mean difference between the odd and even birthday groups (2.67 cm). You can now generalise the conclusions of this mixed effects model to both the population of raters and the population of stimuli.&lt;/p&gt;
&lt;p class=&#34;info&#34;&gt;
Thanks to &lt;a href=&#34;https://twitter.com/lpsatchell&#34;&gt;Liam Satchell&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/AlexJonesPhD&#34;&gt;Alex Jones&lt;/a&gt;, and &lt;a href=&#34;https://twitter.com/Ben_C_J&#34;&gt;Ben Jones&lt;/a&gt; for the stimulating late-night Twitter discussion that prompted this blog post!
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Plenty of papers have made this point much more thoroughly &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-Wolsiefer2017&#34; role=&#34;doc-biblioref&#34;&gt;Wolsiefer, Westfall, and Judd&lt;/a&gt; (&lt;a href=&#34;#ref-Wolsiefer2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-KeepItMaximal&#34; class=&#34;csl-entry&#34;&gt;
Barr, Dale J, Roger Levy, Christoph Scheepers, and Harry J Tily. 2013. &lt;span&gt;“Random Effects Structure for Confirmatory Hypothesis Testing: Keep It Maximal.”&lt;/span&gt; &lt;em&gt;Journal of Memory and Language&lt;/em&gt; 68 (3): 10.1016/j.jml.2012.11.001.
&lt;/div&gt;
&lt;div id=&#34;ref-Coleman1964&#34; class=&#34;csl-entry&#34;&gt;
Coleman, E. B. 1964. &lt;span&gt;“Generalizing to a Language Population.”&lt;/span&gt; &lt;em&gt;Psychological Reports&lt;/em&gt; 14 (1): 219–26. &lt;a href=&#34;https://doi.org/10.2466/pr0.1964.14.1.219&#34;&gt;https://doi.org/10.2466/pr0.1964.14.1.219&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Judd2012&#34; class=&#34;csl-entry&#34;&gt;
Judd, Charles M., Jacob Westfall, and David A. Kenny. 2012. &lt;span&gt;“Treating Stimuli as a Random Factor in Social Psychology: A New and Comprehensive Solution to a Pervasive but Largely Ignored Problem.”&lt;/span&gt; &lt;em&gt;Journal of Personality and Social Psychology&lt;/em&gt; 103 (1): 54–69. &lt;a href=&#34;https://doi.org/doi:10.1037/a0028347&#34;&gt;https://doi.org/doi:10.1037/a0028347&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Wolsiefer2017&#34; class=&#34;csl-entry&#34;&gt;
Wolsiefer, Katie, Jacob Westfall, and Charles M. Judd. 2017. &lt;span&gt;“Modeling Stimulus Variation in Three Common Implicit Attitude Tasks.”&lt;/span&gt; &lt;em&gt;Behavior Research Methods&lt;/em&gt; 49 (4): 1193–1209. &lt;a href=&#34;https://doi.org/10.3758/s13428-016-0779-0&#34;&gt;https://doi.org/10.3758/s13428-016-0779-0&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Random Slopes</title>
      <link>https://debruine.github.io/post/simulating-random-slopes/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/simulating-random-slopes/</guid>
      <description>


&lt;p&gt;This tutorial has been moved to the &lt;a href=&#34;https://debruine.github.io/tutorials/&#34;&gt;tutorials&lt;/a&gt; section.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulate from Existing Data</title>
      <link>https://debruine.github.io/post/simdf/</link>
      <pubDate>Sat, 29 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/simdf/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(faux)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I added a new function to the package &lt;a href=&#34;https://github.com/debruine/faux&#34;&gt;&lt;code&gt;faux&lt;/code&gt;&lt;/a&gt; to generate a new dataframe from an existing dataframe, simulating all numeric columns from normal distributions with the same mean and SD as the existing data and the same correlation structure as the existing data. (Update: faux is now on CRAN!)&lt;/p&gt;
&lt;p&gt;For example, here is the relationship between speed and distance in the built-in dataset &lt;code&gt;cars&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cars %&amp;gt;%
  ggplot(aes(speed, dist)) + 
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:plot-cars-orig&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://debruine.github.io/posts/simdf_files/figure-html/plot-cars-orig-1.png&#34; alt=&#34;Original cars dataset&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Original cars dataset
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;You can create a new sample with the same parameters and 500 rows with the code &lt;code&gt;sim_df(cars, 500)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_df(cars, 500) %&amp;gt;%
  ggplot(aes(speed, dist)) + 
    geom_point() +
    geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:plot-cars-sim&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://debruine.github.io/posts/simdf_files/figure-html/plot-cars-sim-1.png&#34; alt=&#34;Simulated cars dataset&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Simulated cars dataset
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;You can also optionally add grouping variables. For example, here is the relationship between sepal length and width in the built-in dataset &lt;code&gt;iris&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris %&amp;gt;%
  ggplot(aes(Sepal.Width, Sepal.Length, color = Species)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:plot-iris-orig&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://debruine.github.io/posts/simdf_files/figure-html/plot-iris-orig-1.png&#34; alt=&#34;Original iris dataset&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Original iris dataset
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;And here is a new sample with 50 observations of each species, made with the code &lt;code&gt;sim_df(iris, 100, &#34;Species&#34;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_df(iris, 50, between = &amp;quot;Species&amp;quot;) %&amp;gt;%
  ggplot(aes(Sepal.Width, Sepal.Length, color = Species)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:plot-iris-sim&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://debruine.github.io/posts/simdf_files/figure-html/plot-iris-sim-1.png&#34; alt=&#34;Simulated iris dataset&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Simulated iris dataset
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For now, the function only creates new variables sampled from a continuous normal distribution. I hope to add in other sampling distributions in the future. So you’d need to do any rounding or truncating yourself.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_df(iris, 50, between = &amp;quot;Species&amp;quot;) %&amp;gt;%
  mutate_if(is.numeric, round, 1) %&amp;gt;%
  ggplot(aes(Sepal.Width, Sepal.Length, color = Species)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:plot-iris-sim-round&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://debruine.github.io/posts/simdf_files/figure-html/plot-iris-sim-round-1.png&#34; alt=&#34;Simulated iris dataset (rounded)&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Simulated iris dataset (rounded)
&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Mixed Effects</title>
      <link>https://debruine.github.io/post/simulating-mixed-effects/</link>
      <pubDate>Wed, 26 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/simulating-mixed-effects/</guid>
      <description>


&lt;p&gt;This tutorial has been moved to the &lt;a href=&#34;https://debruine.github.io/tutorials/&#34;&gt;tutorials&lt;/a&gt; section.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pipes</title>
      <link>https://debruine.github.io/post/pipes/</link>
      <pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/pipes/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/images/pipe_sticker.png&#34; style=&#34;width: 200px; float: right;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Pipes are a way to order your code in a more readable format.&lt;/p&gt;
&lt;p&gt;Let’s say you have a small data table with 10 participant IDs, two columns with variable type A, and 2 columns with variable type B. You want to calculate the mean of the A variables and the mean of the B variables and return a table with 10 rows (1 for each participant) and 3 columns (&lt;code&gt;id&lt;/code&gt;, &lt;code&gt;A_mean&lt;/code&gt; and &lt;code&gt;B_mean&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;One way you could do this is by creating a new object at every step and using that object in the next step. This is pretty clear, but you’ve created 6 unnecessary data objects in your environment. This can get confusing in very long scripts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make a data table with 10 subjects
data_original &amp;lt;- tibble(
  id = 1:10,
  A1 = rnorm(10, 0),
  A2 = rnorm(10, 1),
  B1 = rnorm(10, 2),
  B2 = rnorm(10, 3)
)

# gather columns A1 to B2 into &amp;quot;variable&amp;quot; and &amp;quot;value&amp;quot; columns
data_gathered &amp;lt;- gather(data_original, variable, value, A1:B2)

# separate the variable column at the _ into &amp;quot;var&amp;quot; and &amp;quot;var_n&amp;quot; columns
data_separated &amp;lt;- separate(data_gathered, variable, c(&amp;quot;var&amp;quot;, &amp;quot;var_n&amp;quot;), sep = 1)

# group the data by id and var
data_grouped &amp;lt;- group_by(data_separated, id, var)

# calculate the mean value for each id/var 
data_summarised &amp;lt;- summarise(data_grouped, mean = mean(value))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;id&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# spread the mean column into A and B columns
data_spread &amp;lt;- spread(data_summarised, var, mean)

# rename A and B to A_mean and B_mean
data &amp;lt;- rename(data_spread, A_mean = A, B_mean = B)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;id&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;A_mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;B_mean&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2304382&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.376790&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.2460394&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.150984&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4908918&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.020612&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.3616511&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.706038&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.5155883&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.599207&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0474370&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.913186&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1864482&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.065799&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5501416&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.405596&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2093015&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.425043&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.7423514&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.399520&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p class=&#34;warning&#34;&gt;
You &lt;em&gt;can&lt;/em&gt; name each object &lt;code&gt;data&lt;/code&gt; and keep replacing the old data object with the new one at each step. This will keep you environment clean, but I don’t recommend it because it makes it too easy to accidentally run your code out of order when you are running line-by-line for development or debugging.
&lt;/p&gt;
&lt;p&gt;One way to avoid extra objects is to nest your functions, literally replacing each data object with the code that generated it in the previous step. This can be fine for very short chains.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_petal_width &amp;lt;- round(mean(iris$Petal.Width), 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But it gets extremely confusing for long chains:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# do not ever do this!!
data &amp;lt;- rename(
  spread(
    summarise(
      group_by(
        separate(
          gather(
            tibble(
              id = 1:10,
              A1 = rnorm(10, 0),
              A2 = rnorm(10, 1),
              B1 = rnorm(10, 2),
              B2 = rnorm(10, 3)), 
            variable, value, A1:B2), 
          variable, c(&amp;quot;var&amp;quot;, &amp;quot;var_n&amp;quot;), sep = 1), 
        id, var), 
      mean = mean(value)), 
    var, mean), 
  A_mean = A, B_mean = B)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;id&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pipe lets you “pipe” the result of each function into the next function, allowing you to put your code in a logical order without creating too many extra objects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate mean of A and B variables for each participant
data &amp;lt;- tibble(
  id = 1:10,
  A1 = rnorm(10, 0),
  A2 = rnorm(10, 1),
  B1 = rnorm(10, 2),
  B2 = rnorm(10, 3)
) %&amp;gt;%
  gather(variable, value, A1:B2) %&amp;gt;%
  separate(variable, c(&amp;quot;var&amp;quot;, &amp;quot;var_n&amp;quot;), sep=1) %&amp;gt;%
  group_by(id, var) %&amp;gt;%
  summarise(mean = mean(value)) %&amp;gt;%
  spread(var, mean) %&amp;gt;%
  rename(A_mean = A, B_mean = B)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;id&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can read this code from top to bottom as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Make a tibble called &lt;code&gt;data&lt;/code&gt; with
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;id&lt;/code&gt; of 1 to 10,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A1&lt;/code&gt; of 10 random numbers from a normal distribution with a mean of 0,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A2&lt;/code&gt; of 10 random numbers from a normal distribution with a mean of 1,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;B1&lt;/code&gt; of 10 random numbers from a normal distribution with a mean of 2,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;B2&lt;/code&gt; of 10 random numbers from a normal distribution with a mean of 3; and then&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Gather to create &lt;code&gt;variable&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; column from columns &lt;code&gt;A_1&lt;/code&gt; to &lt;code&gt;B_2&lt;/code&gt;; and then&lt;/li&gt;
&lt;li&gt;Separate the column &lt;code&gt;variable&lt;/code&gt; into 2 new columns called &lt;code&gt;var&lt;/code&gt;and &lt;code&gt;var_n&lt;/code&gt;, separate at character 1; and then&lt;/li&gt;
&lt;li&gt;Group by columns &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;var&lt;/code&gt;; and then&lt;/li&gt;
&lt;li&gt;Summarise and new column called &lt;code&gt;mean&lt;/code&gt; as the mean of the &lt;code&gt;value&lt;/code&gt; column for each group; and then&lt;/li&gt;
&lt;li&gt;Spread to make new columns with the key names in &lt;code&gt;var&lt;/code&gt; and values in &lt;code&gt;mean&lt;/code&gt;; and then&lt;/li&gt;
&lt;li&gt;Rename to make columns called &lt;code&gt;A_mean&lt;/code&gt; (old &lt;code&gt;A&lt;/code&gt;) and &lt;code&gt;B_mean&lt;/code&gt; (old &lt;code&gt;B&lt;/code&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can make intermediate objects whenever you need to break up your code because it’s getting too complicated or you need to debug something.&lt;/p&gt;
&lt;p class=&#34;info&#34;&gt;
You can debug a pipe by running just the first few functions by highlighting from the beginning to just before the pipe you want to stop at. Try this by highlighting from &lt;code&gt;data &amp;lt;-&lt;/code&gt; to the end of the &lt;code&gt;separate&lt;/code&gt; function and typing cmd-return. What does &lt;code&gt;data&lt;/code&gt; look like now?
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Multiple Vectors</title>
      <link>https://debruine.github.io/post/rnorm_multi/</link>
      <pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/rnorm_multi/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(faux)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m working on a package for simulations called &lt;a href=&#34;https://github.com/debruine/faux&#34;&gt;faux&lt;/a&gt;. (Update: faux is now on CRAN!)&lt;/p&gt;
&lt;p&gt;The first function, &lt;code&gt;rnorm_multi&lt;/code&gt;, makes multiple normally distributed vectors with specified relationships and takes the following arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;n&lt;/code&gt; = the number of samples required (required)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vars&lt;/code&gt; = the number of variables to return (default = &lt;code&gt;3&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cors&lt;/code&gt; = the correlations among the variables (can be a single number, vars*vars matrix, vars*vars vector, or a vars*(vars-1)/2 vector; default = &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mu&lt;/code&gt; = a vector giving the means of the variables (numeric vector of length 1 or vars; default = &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sd&lt;/code&gt; = the standard deviations of the variables (numeric vector of length 1 or vars; default = &lt;code&gt;1&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;varnames&lt;/code&gt; = optional names for the variables (string vector of length vars; default = &lt;code&gt;NULL&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;empirical&lt;/code&gt; = logical. If true, mu, sd and cors specify the empirical not population mean, sd and covariance (default = &lt;code&gt;FALSE&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;p&gt;The following example creates a 100-row dataframe of 3 columns names &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;C&lt;/code&gt;, with means = 0, SDs = 1, and where r&lt;sub&gt;AB&lt;/sub&gt; = 0.2, r&lt;sub&gt;AC&lt;/sub&gt; = -0.5, and r&lt;sub&gt;BC&lt;/sub&gt; = 0.5.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex1 &amp;lt;- rnorm_multi(100, 3, c(0.2, -0.5, 0.5), varnames=c(&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;correlation-matrix-of-sample-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation Matrix of Sample Data&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;A&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;B&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;C&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;A&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.087499&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1202283&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;B&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0874990&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0157210&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1202283&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.015721&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2&lt;/h2&gt;
&lt;p&gt;The following example calculates the correlation matrix, means, and SDs from the &lt;code&gt;iris&lt;/code&gt; dataset and uses them to simulate a dataset of 100 rows with the same parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- select_if(iris, is.numeric)

iris_sim &amp;lt;- rnorm_multi(
  n = 100, 
  vars = ncol(dat), 
  r = cor(dat),
  mu = summarise_all(dat, mean) %&amp;gt;% t(), 
  sd = summarise_all(dat, sd) %&amp;gt;% t(), 
  varnames = names(dat)
)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;correlation-matrix-of-original-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation Matrix of Original Data&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sepal.Length&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sepal.Width&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Petal.Length&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Petal.Width&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sepal.Length&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1175698&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8717538&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8179411&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sepal.Width&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1175698&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4284401&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.3661259&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Petal.Length&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8717538&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4284401&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9628654&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Petal.Width&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8179411&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.3661259&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9628654&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-matrix-of-sample-data-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation Matrix of Sample Data&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sepal.Length&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sepal.Width&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Petal.Length&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Petal.Width&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sepal.Length&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1591051&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8491459&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7544625&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sepal.Width&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1591051&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4527400&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.3513351&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Petal.Length&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8491459&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4527400&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9485627&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Petal.Width&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7544625&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.3513351&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9485627&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0000000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Plot Comparison</title>
      <link>https://debruine.github.io/post/plot-comparison/</link>
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/plot-comparison/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I compared bar plots to violin plots in a recent talk to make the point that real data plotted with the full distribution make your effects look less impressive than minimalist bar charts that just show the means and standard errors, but give you a much better idea of what’s going on with your data.&lt;/p&gt;
&lt;p&gt;I also made a &lt;a href=&#34;http://shiny.psy.gla.ac.uk/debruine/plotdemo/&#34;&gt;shiny app&lt;/a&gt; where you can set the sample size, main effects, and interaction effect, then view six different visualisations of the simulated data.&lt;/p&gt;
&lt;p&gt;I thought I’d post a quick tutorial for anyone who wants to see some code for creating violin-box plots and split-violin plots.&lt;/p&gt;
&lt;p&gt;First, let’s simulate some data from a 2x2 design with a cross-over interaction with a 0.5 SD effect size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 100
data &amp;lt;- tibble(
  sex = rep(c(&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;), n),
  face_sex = rep(c(&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;), each = n)
) %&amp;gt;%
  mutate(
    dv = rnorm(n*2, 0, 1),
    effect = ifelse(sex==face_sex, .5, 0),
    dv = dv + effect
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I like to create a theme for all the plots in a talk or paper. This one is my standard white-on-black talk theme.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bgcolor &amp;lt;- &amp;quot;black&amp;quot;
textcolor &amp;lt;- &amp;quot;white&amp;quot;
my_theme &amp;lt;- theme(
    plot.background = element_rect(fill = bgcolor, colour = bgcolor),
    panel.background = element_rect(fill = NA),
    legend.background = element_rect(fill = NA),
    legend.position=&amp;quot;none&amp;quot;,
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    text = element_text(family=&amp;#39;Fira Code&amp;#39;, colour = textcolor, size=20),
    axis.text = element_text(family=&amp;#39;Fira Code&amp;#39;, colour = textcolor, size=15)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;bar-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bar Plot&lt;/h2&gt;
&lt;p&gt;I’ve commented it out below, but I recommend alsways using &lt;code&gt;ggsave&lt;/code&gt; to save your plots for papers or talks. They are much better resolution than the plots you copy out of an Rmarkdown notebook.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  group_by(sex, face_sex) %&amp;gt;%
  summarise(
    mean = mean(dv),
    se = sd(dv)/sqrt(n())
  ) %&amp;gt;%
  ggplot(aes(sex, mean, fill=face_sex)) +
  geom_hline(yintercept=0, color=textcolor, size=1) +
  geom_col(color = &amp;quot;white&amp;quot;, position=&amp;quot;dodge&amp;quot;, alpha = 0.5) +
  geom_errorbar(aes(ymin = mean-se, ymax=mean+se), 
                width=0.1, 
                position=position_dodge(0.9), 
                color=textcolor) +
  ylab(&amp;quot;DV&amp;quot;) +
  xlab(&amp;quot;Participant Sex&amp;quot;) +
  scale_fill_manual(values = c(&amp;quot;#3D66CC&amp;quot;, &amp;quot;#892110&amp;quot;)) +
  my_theme&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;sex&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/plot_comparison_files/figure-html/plotcomp-bar-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ggsave(&amp;quot;bar.png&amp;quot;, width=10, height = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how the bar plot hides the real range of the data. This is what it would look like plotted with the y-axis ranges shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  group_by(sex, face_sex) %&amp;gt;%
  summarise(
    mean = mean(dv),
    se = sd(dv)/sqrt(n())
  ) %&amp;gt;%
  ggplot(aes(sex, mean, fill=face_sex)) +
  geom_hline(yintercept=0, color=textcolor, size=1) +
  geom_col(color = &amp;quot;white&amp;quot;, position=&amp;quot;dodge&amp;quot;, alpha = 0.5) +
  geom_errorbar(aes(ymin = mean-se, ymax=mean+se), 
                width=0.1, 
                position=position_dodge(0.9), 
                color=textcolor) +
  ylab(&amp;quot;DV&amp;quot;) +
  xlab(&amp;quot;Participant Sex&amp;quot;) +
  ylim(-5, 5) +
  scale_fill_manual(values = c(&amp;quot;#3D66CC&amp;quot;, &amp;quot;#892110&amp;quot;)) +
  my_theme&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;sex&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/plot_comparison_files/figure-html/plotcomp-bar-range-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;violinbox-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ViolinBox Plot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  ggplot(aes(sex, dv, fill = face_sex)) +
  geom_hline(yintercept=0, color=textcolor, size=1) +
  geom_violin(color=textcolor, trim=FALSE, alpha = 0.5) +
  geom_boxplot(color = textcolor, width = 0.25, position = position_dodge(width=0.9)) +
  ylab(&amp;quot;DV&amp;quot;) +
  xlab(&amp;quot;Participant Sex&amp;quot;) +
  ylim(-5, 5) + 
  scale_fill_manual(values = c(&amp;quot;#3D66CC&amp;quot;, &amp;quot;#892110&amp;quot;)) +
  my_theme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/plot_comparison_files/figure-html/plotcomp-violin-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ggsave(&amp;quot;violinbox.png&amp;quot;, width=10, height = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;violin-point-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Violin-Point Plot&lt;/h2&gt;
&lt;p&gt;The boxplot above showss the median and quartiles, which sometimes isn’t the summary statistic you want to emphasise (HT &lt;a href=&#34;@PaulMinda1&#34;&gt;https://twitter.com/PaulMinda1&lt;/a&gt;). You can alternatively plot the mean and 95% CI using &lt;code&gt;geom_pointrange&lt;/code&gt;. This requires a bit of data wrangling first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary_data &amp;lt;- data %&amp;gt;%
  group_by(sex, face_sex) %&amp;gt;%
  summarise(
    mean = mean(dv),
    min = mean(dv) - qnorm(0.975)*sd(dv)/sqrt(n()),
    max = mean(dv) + qnorm(0.975)*sd(dv)/sqrt(n())
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;sex&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  ggplot(aes(sex, dv, fill = face_sex)) +
  geom_hline(yintercept=0, color=textcolor, size=1) +
  geom_violin(color=textcolor, trim=FALSE, alpha = 0.5) +
  geom_pointrange(
    data = summary_data,
    aes(sex, mean, ymin=min, ymax=max),
    shape = 20,
    color = textcolor, 
    position = position_dodge(width = 0.9)
  ) +
  ylab(&amp;quot;DV&amp;quot;) +
  xlab(&amp;quot;Participant Sex&amp;quot;) +
  ylim(-5, 5) + 
  scale_fill_manual(values = c(&amp;quot;#3D66CC&amp;quot;, &amp;quot;#892110&amp;quot;)) +
  scale_color_manual(values = c(&amp;quot;#3D66CC&amp;quot;, &amp;quot;#892110&amp;quot;)) +
  my_theme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/plot_comparison_files/figure-html/plotcomp-pointrange-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ggsave(&amp;quot;violin_pointrange.png&amp;quot;, width=10, height = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;split-violin-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split-Violin Plots&lt;/h2&gt;
&lt;p&gt;To make a split violin plot, first you have to define &lt;code&gt;geom_split_violin()&lt;/code&gt;. I derived the code from
&lt;a href=&#34;&#34;&gt;https://stackoverflow.com/questions/35717353/split-violin-plot-with-ggplot2&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GeomSplitViolin &amp;lt;- ggproto(
  &amp;quot;GeomSplitViolin&amp;quot;, 
  GeomViolin, 
  draw_group = function(self, data, ..., draw_quantiles = NULL) {
    data &amp;lt;- transform(data, 
                      xminv = x - violinwidth * (x - xmin), 
                      xmaxv = x + violinwidth * (xmax - x))
    grp &amp;lt;- data[1,&amp;#39;group&amp;#39;]
    newdata &amp;lt;- plyr::arrange(
      transform(data, x = if(grp%%2==1) xminv else xmaxv), 
      if(grp%%2==1) y else -y
    )
    newdata &amp;lt;- rbind(newdata[1, ], newdata, newdata[nrow(newdata), ], newdata[1, ])
    newdata[c(1,nrow(newdata)-1,nrow(newdata)), &amp;#39;x&amp;#39;] &amp;lt;- round(newdata[1, &amp;#39;x&amp;#39;]) 
    if (length(draw_quantiles) &amp;gt; 0 &amp;amp; !scales::zero_range(range(data$y))) {
      stopifnot(all(draw_quantiles &amp;gt;= 0), all(draw_quantiles &amp;lt;= 1))
      quantiles &amp;lt;- ggplot2:::create_quantile_segment_frame(data, draw_quantiles)
      aesthetics &amp;lt;- data[rep(1, nrow(quantiles)), setdiff(names(data), c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)), drop = FALSE]
      aesthetics$alpha &amp;lt;- rep(1, nrow(quantiles))
      both &amp;lt;- cbind(quantiles, aesthetics)
      quantile_grob &amp;lt;- GeomPath$draw_panel(both, ...)
      ggplot2:::ggname(&amp;quot;geom_split_violin&amp;quot;, 
                       grid::grobTree(GeomPolygon$draw_panel(newdata, ...), quantile_grob))
    } else {
      ggplot2:::ggname(&amp;quot;geom_split_violin&amp;quot;, GeomPolygon$draw_panel(newdata, ...))
    }
  }
)

geom_split_violin &amp;lt;- function (mapping = NULL, 
                               data = NULL, 
                               stat = &amp;quot;ydensity&amp;quot;, 
                               position = &amp;quot;identity&amp;quot;, ..., 
                               draw_quantiles = NULL, 
                               trim = TRUE, 
                               scale = &amp;quot;area&amp;quot;, 
                               na.rm = FALSE, 
                               show.legend = NA, 
                               inherit.aes = TRUE) {
  layer(data = data, 
        mapping = mapping, 
        stat = stat, 
        geom = GeomSplitViolin, 
        position = position, 
        show.legend = show.legend, 
        inherit.aes = inherit.aes, 
        params = list(trim = trim, 
                      scale = scale, 
                      draw_quantiles = draw_quantiles, 
                      na.rm = na.rm, ...)
        )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you’ve defined the new geom, you can use &lt;code&gt;geom_split_violin&lt;/code&gt; pretty much like &lt;code&gt;geom_violin&lt;/code&gt;. Note how the position of the &lt;code&gt;geom_boxplot&lt;/code&gt; changes to put the boxplots side-by-side.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  ggplot(aes(sex, dv, fill = face_sex)) +
  geom_hline(yintercept=0, color=textcolor, size=1) +
  geom_split_violin(color=textcolor, trim=FALSE, alpha = 0.5) +
  geom_boxplot(color = textcolor, 
               width = 0.25, 
               position = position_dodge(width=0.25)) +
  ylab(&amp;quot;DV&amp;quot;) +
  xlab(&amp;quot;Participant Sex&amp;quot;) +
  ylim(-5, 5) + 
  scale_fill_manual(values = c(&amp;quot;#3D66CC&amp;quot;, &amp;quot;#892110&amp;quot;)) +
  my_theme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/plot_comparison_files/figure-html/plotcomp-split-violin-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ggsave(&amp;quot;split_violin.png&amp;quot;, width=10, height = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a split violin with means and 95% CIs defined.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary_data &amp;lt;- data %&amp;gt;%
  group_by(sex, face_sex) %&amp;gt;%
  summarise(
    mean = mean(dv),
    min = mean(dv) - qnorm(0.975)*sd(dv)/sqrt(n()),
    max = mean(dv) + qnorm(0.975)*sd(dv)/sqrt(n())
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;sex&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;%
  ggplot(aes(sex, dv, fill = face_sex)) +
  geom_hline(yintercept=0, color=textcolor, size=1) +
  geom_split_violin(color=textcolor, trim=FALSE, alpha = 0.5) +
  geom_pointrange(
    data = summary_data,
    aes(sex, mean, ymin=min, ymax=max),
    color = textcolor, 
    shape = 20, # 95,
    position = position_dodge(width = 0.25)
  ) +
  ylab(&amp;quot;DV&amp;quot;) +
  xlab(&amp;quot;Participant Sex&amp;quot;) +
  ylim(-5, 5) + 
  scale_fill_manual(values = c(&amp;quot;#3D66CC&amp;quot;, &amp;quot;#892110&amp;quot;)) +
  scale_color_manual(values = c(&amp;quot;#3D66CC&amp;quot;, &amp;quot;#892110&amp;quot;)) +
  my_theme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/plot_comparison_files/figure-html/plotcomp-splitpointrange-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#ggsave(&amp;quot;split_violin_pointrange.png&amp;quot;, width=10, height = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;raincloud-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Raincloud Plots&lt;/h2&gt;
&lt;p&gt;The code for raincloud plots is from &lt;a href=&#34;https://micahallen.org/2018/03/15/introducing-raincloud-plots/&#34;&gt;Micah Allen&lt;/a&gt; and
&lt;a href=&#34;https://gist.githubusercontent.com/benmarwick/2a1bb0133ff568cbe28d/raw/fb53bd97121f7f9ce947837ef1a4c65a73bffb3f/geom_flat_violin.R&#34;&gt;Ben Marwick&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;quot;%||%&amp;quot; &amp;lt;- function(a, b) {
  if (!is.null(a)) a else b
}

geom_flat_violin &amp;lt;- function(mapping = NULL, data = NULL, stat = &amp;quot;ydensity&amp;quot;,
                             position = &amp;quot;dodge&amp;quot;, trim = TRUE, scale = &amp;quot;area&amp;quot;,
                             show.legend = NA, inherit.aes = TRUE, ...) {
  layer(
    data = data,
    mapping = mapping,
    stat = stat,
    geom = GeomFlatViolin,
    position = position,
    show.legend = show.legend,
    inherit.aes = inherit.aes,
    params = list(
      trim = trim,
      scale = scale,
      ...
    )
  )
}

GeomFlatViolin &amp;lt;-
  ggproto(&amp;quot;GeomFlatViolin&amp;quot;, Geom,
          setup_data = function(data, params) {
            data$width &amp;lt;- data$width %||%
              params$width %||% (resolution(data$x, FALSE) * 0.9)
            
            # ymin, ymax, xmin, and xmax define the bounding rectangle for each group
            data %&amp;gt;%
              group_by(group) %&amp;gt;%
              mutate(ymin = min(y),
                     ymax = max(y),
                     xmin = x,
                     xmax = x + width / 2)
            
          },
          
          draw_group = function(data, panel_scales, coord) {
            # Find the points for the line to go all the way around
            data &amp;lt;- transform(data, xminv = x,
                              xmaxv = x + violinwidth * (xmax - x))
            
            # Make sure it&amp;#39;s sorted properly to draw the outline
            newdata &amp;lt;- rbind(plyr::arrange(transform(data, x = xminv), y),
                             plyr::arrange(transform(data, x = xmaxv), -y))
            
            # Close the polygon: set first and last point the same
            # Needed for coord_polar and such
            newdata &amp;lt;- rbind(newdata, newdata[1,])
            
            ggplot2:::ggname(&amp;quot;geom_flat_violin&amp;quot;, GeomPolygon$draw_panel(newdata, panel_scales, coord))
          },
          
          draw_key = draw_key_polygon,
          
          default_aes = aes(weight = 1, colour = &amp;quot;grey20&amp;quot;, fill = &amp;quot;white&amp;quot;, size = 0.5,
                            alpha = NA, linetype = &amp;quot;solid&amp;quot;),
          
          required_aes = c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; data %&amp;gt;%
  ggplot(aes(sex, dv, fill = face_sex)) +
  geom_hline(yintercept=0, color=textcolor, size=1) +
  geom_flat_violin(position = position_nudge(x = .25, y = 0), 
                   color=textcolor, trim=FALSE, alpha = 0.75) +
  geom_point(aes(color = face_sex), 
             position = position_jitter(width = .2, height = 0), 
             size = .5, alpha = .75) +
  ylab(&amp;quot;DV&amp;quot;) +
  xlab(&amp;quot;Participant Sex&amp;quot;) +
  coord_flip() +
  scale_fill_manual(values = c(&amp;quot;#3D66CC&amp;quot;, &amp;quot;#892110&amp;quot;)) +
  scale_color_manual(values = c(&amp;quot;#3D66CC&amp;quot;, &amp;quot;#892110&amp;quot;)) +
  my_theme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/plot_comparison_files/figure-html/plot-raincloud-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Row Headers</title>
      <link>https://debruine.github.io/post/multi-row-headers/</link>
      <pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/multi-row-headers/</guid>
      <description>
&lt;script src=&#34;https://debruine.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A student on our help forum recently asked for help making a wide-format dataset long. When I tried to load the data, I realised the first three rows were all header rows. Here’s the code I wrote to deal with it.&lt;/p&gt;
&lt;p&gt;First, I’ll make a small CSV “file” below. In a typical case, you’d read the data in from a file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;demo_csv &amp;lt;- &amp;quot;SUB1, SUB1, SUB1, SUB1, SUB2, SUB2, SUB2, SUB2
COND1, COND1, COND2, COND2, COND1, COND1, COND2, COND2
X, Y, X, Y, X, Y, X, Y
10, 15, 6, 2, 42, 4, 32, 5
4, 43, 7, 34, 56, 43, 2, 33
77, 12, 14, 75, 36, 85, 3, 2&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you try to read in this data, you’ll get an error message about the duplicate column names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- read_csv(demo_csv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Duplicated column names deduplicated: &amp;#39;SUB1&amp;#39; =&amp;gt; &amp;#39;SUB1_1&amp;#39; [2], &amp;#39;SUB1&amp;#39;
## =&amp;gt; &amp;#39;SUB1_2&amp;#39; [3], &amp;#39;SUB1&amp;#39; =&amp;gt; &amp;#39;SUB1_3&amp;#39; [4], &amp;#39;SUB2&amp;#39; =&amp;gt; &amp;#39;SUB2_1&amp;#39; [6], &amp;#39;SUB2&amp;#39; =&amp;gt;
## &amp;#39;SUB2_2&amp;#39; [7], &amp;#39;SUB2&amp;#39; =&amp;gt; &amp;#39;SUB2_3&amp;#39; [8]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead, you should read in just the header rows by setting &lt;code&gt;n_max&lt;/code&gt; equal to the number of header rows and &lt;code&gt;col_names&lt;/code&gt; to &lt;code&gt;FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_head &amp;lt;- read_csv(demo_csv, n_max = 3, col_names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will get a table that looks like this:&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
X1
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
X2
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
X3
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
X4
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
X5
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
X6
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
X7
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
X8
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
X
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Y
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
X
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Y
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
X
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Y
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
X
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Y
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can then transpose the table (rotate it) and make new header names by pasting together the names of the three headers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_names &amp;lt;- data_head %&amp;gt;%
  t() %&amp;gt;% # transposes the data and turns it into a matrix
  as_tibble() %&amp;gt;% # turn it back to a tibble
  mutate(name = paste(V1, V2, V3, sep = &amp;quot;_&amp;quot;)) %&amp;gt;%
  pull(name)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.
## Using compatibility `.name_repair`.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can read in the data without the three header rows. Use &lt;code&gt;skip&lt;/code&gt; to skip the headers and set &lt;code&gt;col_names&lt;/code&gt; to the new names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- read_csv(demo_csv, skip = 3, col_names = new_names)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have an excel file that merges the duplicate headers across rows, it’s a little trickier, but still do-able.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/images/multirow-excel.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first steps is the same: read in the first three rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_head &amp;lt;- readxl::read_excel(&amp;quot;3headers_demo.xlsx&amp;quot;, n_max = 3, col_names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New names:
## * `` -&amp;gt; ...1
## * `` -&amp;gt; ...2
## * `` -&amp;gt; ...3
## * `` -&amp;gt; ...4
## * `` -&amp;gt; ...5
## * ...&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
…1
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
…2
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
…3
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
…4
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
…5
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
…6
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
…7
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
…8
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
X
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Y
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
X
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Y
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
X
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Y
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
X
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Y
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The function below starts at the top and fills in any missing data with the value in the previous row. You’ll have to define this function in your script before you run the next part.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fillHeaders &amp;lt;- function(header_table) {
  for (row in 2:nrow(header_table)) {
    this_row &amp;lt;- header_table[row, ]
    last_row &amp;lt;- header_table[row-1, ]
    new_row &amp;lt;- ifelse(is.na(this_row), last_row, this_row)
    header_table[row, ] &amp;lt;- new_row
  }
  header_table
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just run the &lt;code&gt;fillHeaders()&lt;/code&gt; function after you transpose as re-tibble the header data, then continue generating the pasted name the same as above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_names &amp;lt;- data_head %&amp;gt;%
  t() %&amp;gt;% # transposes the data and turns it into a matrix
  as_tibble() %&amp;gt;% # turn it back to a tibble
  fillHeaders() %&amp;gt;% # fill in missing headers 
  mutate(name = paste(V1, V2, V3, sep = &amp;quot;_&amp;quot;)) %&amp;gt;%
  pull(name)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your data are set up with multiple headers, you’ll probably want to change them from wide to long format. Here’s a quick example how to use &lt;code&gt;gather&lt;/code&gt;, &lt;code&gt;separate&lt;/code&gt;, and &lt;code&gt;spread&lt;/code&gt; to do this with variable names like above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- readxl::read_excel(&amp;quot;3headers_demo.xlsx&amp;quot;, skip = 3, col_names = new_names)

data_long &amp;lt;- data %&amp;gt;%
  # add row IDs if each row doesn&amp;#39;t already have uniquely identifying column(s)
  mutate(trial = row_number()) %&amp;gt;% 
  # gather creates a column of variable names and a column of values
  gather(&amp;quot;var&amp;quot;, &amp;quot;val&amp;quot;, new_names) %&amp;gt;% 
  # split the variable names into their three component parts
  separate(var, c(&amp;quot;subID&amp;quot;, &amp;quot;condition&amp;quot;, &amp;quot;coord&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  # put X and Y in separate columns 
  spread(coord, val)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Note: Using an external vector in selections is ambiguous.
## ℹ Use `all_of(new_names)` instead of `new_names` to silence this message.
## ℹ See &amp;lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&amp;gt;.
## This message is displayed once per session.&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
trial
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
subID
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
condition
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
X
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Y
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8316380
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7881552
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3941482
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2056488
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9332829
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1530898
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6189847
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9400281
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4147148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1366791
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9805130
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7493469
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1048907
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6573472
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9579583
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3430333
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5577673
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0956297
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3045316
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3540656
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3621907
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8460132
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0167339
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1886913
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4326746
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8276863
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2845026
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6236266
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0439374
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5379287
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0712748
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3511542
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6545546
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6501679
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9202481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2525272
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8117072
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3455603
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7073851
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4249118
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0679236
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6978207
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3979061
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6922928
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5282960
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1093352
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SUB2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COND2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6622162
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5567239
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>How many raters do I need?</title>
      <link>https://debruine.github.io/post/how-many-raters/</link>
      <pubDate>Sat, 17 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/how-many-raters/</guid>
      <description>
&lt;script src=&#34;https://debruine.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I’ve often wondered how many raters I need to sample to get reliable stimulus ratings.&lt;/p&gt;
&lt;p&gt;This will obviously depend on the stimuli and what they’re being rated for. If there is a lot of inter-rater variation or very little inter-stimulus variation, you will need more raters to generate mean ratings with any reliability.&lt;/p&gt;
&lt;p&gt;If you have a large set of ratings of a type of stimulus, population of rater, and type of rating you’re interested in, you can use the script below to figure out how many raters you need to sample to get mean stimulus ratings that are well-correlated with mean ratings from very large samples.&lt;/p&gt;
&lt;p&gt;The example below is for attractiveness ratings using an open-access image set from my lab.&lt;/p&gt;
&lt;p&gt;You can cite this method as:
&lt;a href=&#34;https://osf.io/x7fus/&#34;&gt;DeBruine, Lisa &amp;amp; Jones, Benedict C. (2018) Determining the number of raters for reliable mean ratings. OSF. doi: 10.17605/OSF.IO/X7FUS&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(psych)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Read data from DeBruine, Lisa; Jones, Benedict (2017): Face Research Lab London Set. figshare. &lt;a href=&#34;https://doi.org/10.6084/m9.figshare.5047666&#34;&gt;doi: 10.6084/m9.figshare.5047666&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/facelab_london.jpg&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- read_csv(&amp;quot;https://ndownloader.figshare.com/files/8542045&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Calculate canonical mean ratings (average of all available ratings)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;canon &amp;lt;- data %&amp;gt;%
  select(X001:X173) %&amp;gt;%
  group_by() %&amp;gt;%
  summarise_all(mean) %&amp;gt;%
  t()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is a function to sample n raters from the set and calculate Cronbach’s &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;r&lt;/code&gt; from the Pearson’s correlation with the canonical ratings.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_alpha &amp;lt;- function(data, n) {
  # sample your full dataset
  data_sample &amp;lt;- data %&amp;gt;%
    sample_n(n) %&amp;gt;%
    select(X001:X173) # select only columns with your stimuli
  
  # calculate cronbach&amp;#39;s alpha
  capture.output(suppressWarnings(a &amp;lt;- alpha(t(data_sample))))
  alpha &amp;lt;- a$total[&amp;quot;std.alpha&amp;quot;] %&amp;gt;% pluck(1)

  # calculate mean sample ratings
  sample_means &amp;lt;- data_sample %&amp;gt;%
    group_by() %&amp;gt;%
    summarise_all(mean) %&amp;gt;%
    t()
  
  # calculate correlation between sample mean ratings and canon
  r &amp;lt;- cor(sample_means, canon)[[1,1]]
  
  # return relevant data
  tibble(
    n = n,
    alpha = alpha,
    r = r
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Generate 100 samples for 5 to 50 raters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_samples &amp;lt;- 100
n_raters &amp;lt;- seq(5, 50, by = 5)

sim &amp;lt;- rep(n_raters, each = n_samples) %&amp;gt;% 
  purrr::map_df( function(n) { 
    get_alpha(data, n)
  })&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This graph of the distribution of Cronbach’s alphas shows that alphas tend to be fairly “high” (&amp;gt;.8) after about 15 raters for this stimulus set and rating.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/raters_files/figure-html/alphas-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is a graph of the distribution of correlations between sample means and canonical mean ratings. Again, the sample mean ratings are very highly correlated with the canonical ratings from the full set of 2513 raters after about 15 raters.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/raters_files/figure-html/cors-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This table gives the median and 10th percentiles for &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;r&lt;/code&gt;, as well as the proportion of &lt;code&gt;alpha&lt;/code&gt;s over 0.8 (typically considered high).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
n
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
median alpha
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
90% alpha &amp;gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
alpha &amp;gt;= 0.8
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
median r
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
90% r &amp;gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.73
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.79
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.78
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.82
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.89
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.89
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.99
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.95
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.93
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.89
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.94
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.93
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.91
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.97
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.96
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.94
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.93
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.97
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.96
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
35
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.95
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.94
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.97
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.95
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.97
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
45
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.95
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.97
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.98
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Solution Feedback</title>
      <link>https://debruine.github.io/post/solution-feedback/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/solution-feedback/</guid>
      <description>


&lt;p&gt;Download a simple demo &lt;a href=&#34;https://debruine.github.io/files/solveme_demo.Rmd&#34;&gt;.rmd&lt;/a&gt;/&lt;a href=&#34;https://debruine.github.io/files/solveme_demo.html&#34;&gt;.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’d noticed that students were tending to just look at the &lt;a href=&#34;hidden_solutions.html&#34;&gt;hidden solutions&lt;/a&gt; and not try very hard to understand the exercises, so I wanted a way for them to check if they’re right, but not give them the answer immediately. I made a quick way to set up an input box that turns blue when you type in the right answer.&lt;/p&gt;
&lt;p&gt;What is 2 + 2? &lt;input class=&#34;solveme&#34; size=&#34;3&#34; answer=&#34;4&#34;&gt;&lt;/p&gt;
&lt;p&gt;Setting this up requires a few lines at the beginning and end of each file, plus surrounding your solutions with a line of html.&lt;/p&gt;
&lt;div id=&#34;put-this-at-the-beginning-of-your-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Put this at the beginning of your file&lt;/h2&gt;
&lt;p&gt;If you’re making your website in RMarkdown, put it after the second &lt;code&gt;---&lt;/code&gt; in the yml header. If you’re writing html directly, put it inside the &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; element.&lt;/p&gt;
&lt;p&gt;You can use different colours or line types by changing the border styles.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;style&amp;gt;
  /* styles for solveme */
  .solveme { border: 2px dotted red; }
  .solveme.correct { border: 2px solid blue; }
&amp;lt;/style&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’re using &lt;a href=&#34;http://rmarkdown.rstudio.com/rmarkdown_websites.html&#34;&gt;RMarkdown Websites&lt;/a&gt;, you can just put these lines of css into an external stylesheet linked in your &lt;code&gt;_site.yml&lt;/code&gt; file (e.g., &lt;a href=&#34;rguppies.css&#34;&gt;rguppies.css&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;put-this-at-the-end-of-your-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Put this at the end of your file&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;script&amp;gt;
  tc = function() {
    if (t = document.getElementById(&amp;quot;total_correct&amp;quot;)) {
      t.innerHTML =       
        document.getElementsByClassName(&amp;quot;correct&amp;quot;).length + &amp;quot; of &amp;quot; +
        document.getElementsByClassName(&amp;quot;solveme&amp;quot;).length + &amp;quot; correct&amp;quot;;
    }
  }
  window.onload = function() {
    tc();
    var solveme = document.getElementsByClassName(&amp;quot;solveme&amp;quot;);
    for (var i = 0; i &amp;lt; solveme.length; i++) {
      solveme[i].setAttribute(&amp;quot;autocomplete&amp;quot;,&amp;quot;off&amp;quot;);
      solveme[i].value = &amp;quot;&amp;quot;;
      solveme[i].onkeyup = function(e) {
        var real_answer = this.getAttribute(&amp;quot;answer&amp;quot;).trim();
        var my_answer = this.value;
        var cl = this.classList;
        if (cl.contains(&amp;quot;nospaces&amp;quot;)) {
          real_answer = real_answer.replace(/ /g, &amp;quot;&amp;quot;);
          my_answer = my_answer.replace(/ /g, &amp;quot;&amp;quot;);
        }
        if (cl.contains(&amp;quot;ignorecase&amp;quot;)) {
          real_answer = real_answer.toLowerCase();
          my_answer = my_answer.toLowerCase();
        }
        var linend = new RegExp(/\s*(:or:)\s*/, &amp;#39;g&amp;#39;)
        real_answer = real_answer.split(linend);
        if (my_answer !== &amp;quot;&amp;quot; &amp;amp; real_answer.includes(my_answer)) {
          cl.add(&amp;quot;correct&amp;quot;);
        } else {
          cl.remove(&amp;quot;correct&amp;quot;);
        }
        tc();
      }
      solveme[i].onchange = function() {
        this.onkeyup();
      }
    }
  }
&amp;lt;/script&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’re using &lt;a href=&#34;http://rmarkdown.rstudio.com/rmarkdown_websites.html&#34;&gt;RMarkdown Websites&lt;/a&gt;, you can just put this script into an external footer or script file linked in your &lt;code&gt;_site.yml&lt;/code&gt; file (e.g., &lt;a href=&#34;rguppies.js&#34;&gt;rguppies.js&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up-the-input-boxes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set up the input boxes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Set up a basic input box like below. It needs to have &lt;code&gt;solveme&lt;/code&gt; as the class and the correct answer in &lt;code&gt;answer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;What is &lt;span class=&#34;math inline&#34;&gt;\(5 + 5\)&lt;/span&gt;?
&lt;input class=&#34;solveme&#34; answer=&#34;10&#34; &gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;input class=&amp;quot;solveme&amp;quot; answer=&amp;quot;10&amp;quot; &amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you don’t care about uppercase vs lowercase letters, add &lt;code&gt;ignorecase&lt;/code&gt; to the input style. You can also change the width of theinput box with &lt;code&gt;size&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;What is the letter after B?
&lt;input class=&#34;solveme ignorecase&#34; size=&#34;1&#34; answer=&#34;c&#34; &gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;input class=&amp;quot;solveme ignorecase&amp;quot; size=&amp;quot;1&amp;quot; answer=&amp;quot;c&amp;quot; &amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can also put multiple correct answer possibilities separated by &lt;code&gt;:or:&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Type a vowel
&lt;input class=&#34;solveme ignorecase&#34; size=&#34;1&#34; 
         answer=&#34;a :or: e :or: i :or: o :or: u&#34; &gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;input class=&amp;quot;solveme ignorecase&amp;quot; size=&amp;quot;1&amp;quot; 
       answer=&amp;quot;a :or: e :or: i :or: o :or: u&amp;quot; &amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you’re asking for simple code where the spaces don’t matter, add the class &lt;code&gt;nospaces&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Draw 10 random numbers from a normal distribution with a mean of 3 and SD of 2:&lt;/p&gt;
&lt;p&gt;&lt;input class=&#34;solveme nospaces&#34; 
    size=&#34;30&#34;
    answer=&#34;rnorm(10, 3, 2) :or:
            rnorm(10, mean = 3, sd = 2) :or:
            rnorm(10, 3, sd = 2) :or:
            rnorm(10, sd = 2, mean = 3) :or:
            rnorm(x = 10, mean = 3, sd = 2)&#34; &gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;input class=&amp;quot;solveme nospaces&amp;quot; 
  size=&amp;quot;30&amp;quot;
  answer=&amp;quot;rnorm(10, 3, 2) :or:
  rnorm(10, mean = 3, sd = 2) :or:
  rnorm(10, 3, sd = 2) :or:
  rnorm(10, sd = 2, mean = 3) :or:
  rnorm(x = 10, mean = 3, sd = 2)&amp;quot; &amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p class=&#34;alert alert-info&#34;&gt;
You can also skip a line after &lt;code&gt;:or:&lt;/code&gt;. Whitespace before and after your answer is trimmed off, so your coprrect answer can’t require that the student start or end with spaces.
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This can’t handle multiple-line answers, but you can embed several input boxes in a paragraph. The formatting can get tricky, though:&lt;/p&gt;
&lt;p&gt;Complete the following function for returning the scaled values of a vector, &lt;code&gt;v&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
  scale_function &lt;- function(v) { 
    the_mean &lt;- &lt;input class=&#34;solveme&#34; size=&#34;6&#34; answer=&#34;mean&#34;&gt;(&lt;input class=&#34;solveme&#34; size = &#34;2&#34; answer=&#34;v&#34;&gt;)
    the_sd &lt;- sd(&lt;input class=&#34;solveme&#34; size=&#34;1&#34; answer=&#34;v&#34;&gt;)

    (v &lt;input class=&#34;solveme&#34; size = &#34;1&#34; answer=&#34;-&#34;&gt; the_mean ) &lt;input class=&#34;solveme&#34; size = &#34;1&#34; answer=&#34;/&#34;&gt; the_sd
  }
  &lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;pre&amp;gt;
scale_function &amp;lt;- function(v) { 
  the_mean &amp;lt;- &amp;lt;input class=&amp;quot;solveme&amp;quot; size=&amp;quot;6&amp;quot; answer=&amp;quot;mean&amp;quot;&amp;gt;(&amp;lt;input class=&amp;quot;solveme&amp;quot;  size = &amp;quot;2&amp;quot; answer=&amp;quot;v&amp;quot;&amp;gt;)
  the_sd &amp;lt;- sd(&amp;lt;input class=&amp;quot;solveme&amp;quot; size=&amp;quot;1&amp;quot; answer=&amp;quot;v&amp;quot;&amp;gt;)

  (v &amp;lt;input class=&amp;quot;solveme&amp;quot; size = &amp;quot;1&amp;quot; answer=&amp;quot;-&amp;quot;&amp;gt; the_mean ) &amp;lt;input class=&amp;quot;solveme&amp;quot; size = &amp;quot;1&amp;quot; answer=&amp;quot;/&amp;quot;&amp;gt; the_sd
}
&amp;lt;/pre&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p class=&#34;alert alert-info&#34;&gt;
I wrapped the text in &lt;code&gt;&amp;lt;pre&amp;gt;&lt;/code&gt; tags to format it like code, while still rendering the input boxes. If you surround it with three backticks, it will just display the code for the input boxes, not the actual boxes.
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can also use this for a multiple choice drop-down menu.&lt;/p&gt;
&lt;p&gt;How would you model a distribution of coin flips?&lt;/p&gt;
&lt;p&gt;&lt;select class=&#34;solveme&#34; answer=&#34;rbinom&#34;&gt;
&lt;option&gt;&lt;/option&gt;
&lt;option&gt;rnorm&lt;/option&gt;
&lt;option&gt;runif&lt;/option&gt;
&lt;option&gt;rpois&lt;/option&gt;
&lt;option&gt;rbinom&lt;/option&gt;
&lt;/select&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;select class=&amp;quot;solveme&amp;quot; answer=&amp;quot;rbinom&amp;quot;&amp;gt;
  &amp;lt;option&amp;gt;&amp;lt;/option&amp;gt;
  &amp;lt;option&amp;gt;rnorm&amp;lt;/option&amp;gt;
  &amp;lt;option&amp;gt;runif&amp;lt;/option&amp;gt;
  &amp;lt;option&amp;gt;rpois&amp;lt;/option&amp;gt;
  &amp;lt;option&amp;gt;rbinom&amp;lt;/option&amp;gt;
&amp;lt;/select&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can include a line with automatically updating total correct using the following code:&lt;/p&gt;
&lt;p&gt;&lt;span id=&#34;total_correct&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;span id=&amp;quot;total_correct&amp;quot;&amp;gt;&amp;lt;/span&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Face Research Lab PreReg Example</title>
      <link>https://debruine.github.io/post/frl-prereg/</link>
      <pubDate>Mon, 23 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/frl-prereg/</guid>
      <description>


&lt;style&gt;
   input[type=text] { width: 100%; }
   textarea { width: 100%; height: 6em; }
   .example { background-color: #DDEEFF; border: 1px solid #BBCCDD; border-radius: 0.5em; padding: 0.5em 0.5em 0.5em 0.5em;}
   .eval { font-size: smaller; color: #666666; }
&lt;/style&gt;
&lt;p&gt;Here is my internal lab pre-registration form (including an example). It is based on the &lt;a href=&#34;https://cos.io/prereg/&#34;&gt;OSF Preregistration Challenge&lt;/a&gt; forms. We use this for student projects to train them about open science practices and to prepare formal preregistrations on the OSF for postgraduate students and other lab members.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;prereg.Rmd&#34;&gt;Download the Rmd notebook for this example&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;study-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Study Information&lt;/h2&gt;
&lt;div id=&#34;title&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Title&lt;/h3&gt;
&lt;p&gt;Provide the working title of your study.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;The effect of facial expression on third party kin recognition&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
The title should be a specific and informative description of a project. Vague titles such as ‘Faces preregistration plan’ are not appropriate.
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;research-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Research Questions&lt;/h3&gt;
&lt;p&gt;Please list each research question included in this study.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;Does facial expression influence the accuracy of third party kin recognition?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;The type of submissions for this question will vary widely by discipline, and we cannot determine if the question is worth asking. However, questions that are excessively vague so as to make understanding later sections difficult are not appropriate. For some studies, the research questions and hypotheses are extremely similar, so overlap between this and the subsequent question is expected.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hypotheses&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hypotheses&lt;/h3&gt;
&lt;p&gt;For each of the research questions listed in the previous section, provide one or multiple specific and testable hypotheses. Please state if the hypotheses are directional or non-directional. If directional, state the direction. A predicted effect is also appropriate here.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;Third party kinship recognition accuracy will be higher for stimuli displaying a smiling facial expression than a neutral facial expression.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;In this section, the submission should provide a prediction as to the outcome of the study or a statement that no specific prediction is expected.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling-plan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sampling Plan&lt;/h2&gt;
&lt;p&gt;In this section we’ll ask you to describe how you plan to collect samples, as well as the number of samples you plan to collect and your rationale for this decision. Please keep in mind that the data described in this section should be the actual data used for analysis, so if you are using a subset of a larger dataset, please describe the subset that will actually be used in your study.&lt;/p&gt;
&lt;div id=&#34;data-collection-procedures&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data collection procedures&lt;/h3&gt;
&lt;p&gt;Please describe the process by which you will collect your data. If you are using human subjects, this should include the population from which you obtain subjects, recruitment efforts, payment for participation, how subjects will be selected for eligibility from the initial pool (e.g. inclusion and exclusion rules), and your study timeline. For studies that don’t include human subjects, include information about how you will collect samples, duration of data gathering efforts, source or location of samples, or batch numbers you will use.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;Raters will take part in this study online at &lt;a href=&#34;http://faceresearch.org&#34;&gt;faceresearch.org&lt;/a&gt;. Raters need to be at least 16 years old and give consent to participate in the study. Data collection will start in January 2017 and continue until we obtain complete data from 50 people, with no further restrictions for sex, nationality, language, or other characteristics.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;The answer to this question requires a specific set of instructions so that another person could repeat the data collection procedures and recreate the study population. Alternatively, if the study population would be unable to be reproduced because it relies on a specific set of circumstances unlikely to be recreated (e.g., a community of people from a specific time and location), the criteria and methods for creating the group and the rationale for this unique set of subjects should be clear.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sample-size&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sample size&lt;/h3&gt;
&lt;p&gt;Describe the sample size of your study. How many units will be analyzed in the study? This could be the number of people, birds, classrooms, plots, interactions, or countries included. If the units are not individuals, then describe the size requirements for each unit. If you are using a clustered or multilevel design, how many units are you collecting at each level of the analysis?&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;Our target sample size is 50 raters. Due to constraints of online data collection, we may collect more than 50 raters if many people participate in a short period of time. Data will not be excluded if more than 50 people complete the study.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;For some studies, this will simply be the number of samples or the number of clusters. For others, this could be an expected range, minimum, or maximum number.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sample-size-rationale&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sample size rationale&lt;/h3&gt;
&lt;p&gt;This could include a power analysis or an arbitrary constraint such as time, money, or personnel.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;We performed a power calculation using simulated data (R script attached) to ascertain that our design has &amp;gt;90% power to detect a third party kin recognition ability difference of at least 5% between neutral and smiling faces.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;This gives you an opportunity to specifically state how the sample size will be determined. A wide range of possible answers is acceptable; remember that transparency is more important than principled justifications. If you state any reason for a sample size upfront, it is better than stating no reason and leaving the reader to “fill in the blanks.” Acceptable rationales include: a power analysis, an arbitrary number of subjects, or a number based on time or monetary constraints.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variables&lt;/h2&gt;
&lt;p&gt;In this section you can describe all variables (both manipulated and measured variables) that will later be used in your confirmatory analysis plan. In your analysis plan, you will have the opportunity to describe how each variable will be used. If you have variables which you are measuring for exploratory analyses, you are not required to list them, though you are permitted to do so.&lt;/p&gt;
&lt;div id=&#34;manipulated-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Manipulated variables&lt;/h3&gt;
&lt;p&gt;Describe all variables you plan to manipulate and the levels or treatment arms of each variable. For observational studies and meta-analyses, simply state that this is not applicable.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;Relatedness (related/unrelated): Half of the stimulus pairs are &lt;em&gt;related&lt;/em&gt;; half are &lt;em&gt;unrelated&lt;/em&gt; age- and sex-matched pairs.&lt;/p&gt;
&lt;p&gt;Facial expression (smiling/neutral): Each rater will see half of the stimulus pairs with a &lt;em&gt;smiling&lt;/em&gt; facial expression and the other half with a &lt;em&gt;neutral&lt;/em&gt; facial expression.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;For any experimental manipulation, you should give a precise definition of each manipulated variable. This must include a precise description of the levels at which each variable will be set, or a specific definition for each categorical treatment. For example, “loud or quiet,” should instead give either a precise decibel level or a means of recreating each level. ‘Presence/absence’ or ‘positive/negative’ is an acceptable description if the variable is precisely described.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;measured-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Measured variables&lt;/h3&gt;
&lt;p&gt;Describe each variable that you will measure. This will include outcome measures, as well as any predictors or covariates that you will measure. You do not need to include any variables that you plan on collecting if they are not going to be included in the confirmatory analyses of this study.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;The outcome variable will be the perceived kinship status of each presented stimulus pair (related/unrelated).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;Observational studies and meta-analyses will include only measured variables. As with the previous questions, the answers here must be precise. For example, ‘intelligence,’ ‘accuracy,’ ‘aggression,’ and ‘color’ are too vague. Acceptable alternatives could be ‘IQ as measured by Wechsler Adult Intelligence Scale’ ‘percent correct,’ ‘number of threat displays,’ and ‘percent reflectance at 400 nm.’&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;indices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Indices&lt;/h3&gt;
&lt;p&gt;If any measurements are going to be combined into an index (or even a mean), what measures will you use and how will they be combined? Include either a formula or a precise description of your method. If your are using a more complicated statistical method to combine measures (e.g. a factor analysis), you can note that here but describe the exact method in the analysis plan section.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;Our analyses do not require transformation beyond assigning 0/1 to the response labels (related = 1, unrelated = 0).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;If you are using multiple pieces of data to construct a single variable, how will this occur? Both the data that are included and the formula or weights for each measure must be specified. Standard summary statistics, such as “means” do not require a formula, though more complicated indices require either the exact formula or, if it is an established index in the field, the index must be unambiguously defined. For example, “biodiversity index” is too broad, whereas “Shannon’s biodiversity index” is appropriate.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;design-plan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Design Plan&lt;/h2&gt;
&lt;p&gt;In this section, you will be asked to describe the overall design of your study. Remember that this research plan is designed to register a single study, so if you have multiple experimental designs, please complete a separate preregistration.&lt;/p&gt;
&lt;div id=&#34;study-design&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Study design&lt;/h3&gt;
&lt;p&gt;Describe your study design. Examples include two-group, factorial, randomized block, and repeated measures. Is it a between (unpaired), within-subject (paired), or mixed design? Describe any counterbalancing required. Typical study designs for observation studies include cohort, cross sectional, and case-control studies.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;Each rater will be presented with 100 stimulus pairs, presented in a random order. Half of the stimulus pairs are related (full siblings) and half are unrelated age- and sex-matched pairs. Half of each group will be shown with smiling expressions, the other half with neutral expressions, ensuring that the same stimuli are never presented as both smiling and neutral to the same rater. Raters will be randomly assigned to one of two versions containing the same stimuli, but the opposite facial expression (e.g., the stimuli with a smiling expression in version A are neutral in version B).&lt;/p&gt;
&lt;p&gt;Raters will receive the following instructions before the study:&lt;/p&gt;
&lt;blockquote&gt;
In this experiment you will be shown 100 pairs of faces. Some are siblings, some are an unrelated pair. You will be asked to determine whether each pair is “unrelated” or “related”. After the experiment, you will be told how many of the 100 pairs you correctly determined and what the average performance on this task was.
&lt;/blockquote&gt;
&lt;p&gt;After this, they will see 100 trials in a randomised order. Each trial has the questions, “Please indicate if this pair of faces shows siblings or an unrelated pair”, two respinse buttons (“unrelated”, “related”) and the faces of the stimulus pair shown side-by-side below at a maximum of 600x800 pixels (or sized to fit screen width if the device screen is smaller than 1200px). Face images have hair and clothing masked in black.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;prereg/interface.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;An example trial&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;This question has a variety of possible answers. The key is for a researcher to be as detailed as is necessary given the specifics of their design. Be careful to determine if every parameter has been specified in the description of the study design. There may be some overlap between this question and the following questions. That is OK, as long as sufficient detail is given in one of the areas to provide all of the requested information. For example, if the study design describes a complete factorial, 2 X 3 design and the treatments and levels are specified previously, you do not have to repeat that information.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;randomization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Randomization&lt;/h3&gt;
&lt;p&gt;If you are doing a randomized study, how will you randomize, and at what level?&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;Each rater will be assigned to one of the 2 counterbalanced versions. Raters will be assigned to whichever version currently has fewer completions by their sex, or randomly when both versions have the same number (this is done by the online platform). The order of stimuli is randomized for each rater.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;Typical randomization techniques include: simple, block, stratified, and adaptive covariate randomization. If randomization is required for the study, the method should be specified here, not simply the source of random numbers.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis-plan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysis Plan&lt;/h2&gt;
&lt;p&gt;You may describe one or more confirmatory analysis in this preregistration. Please remember that all analyses specified below must be reported in the final article, and any additional analyses must be noted as exploratory or hypothesis-generating.&lt;/p&gt;
&lt;p&gt;A confirmatory analysis plan must state up front which variables are predictors (independent) and which are the outcomes (dependent), otherwise it is an exploratory analysis. You are allowed to describe any exploratory work here, but a clear confirmatory analysis is required.&lt;/p&gt;
&lt;div id=&#34;statistical-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Statistical models&lt;/h3&gt;
&lt;p&gt;What statistical model will you use to test each hypothesis? Please include the type of model (e.g. ANOVA, multiple regression, SEM, etc) and the specification of the model (this includes each variable that will be included as predictors, outcomes, or covariates). Please specify any interactions that will be tested and remember that any test not included here must be noted as an exploratory test in your final article.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;Binary relatedness judgments will be analyzed using binomial logistic mixed regression in &lt;code&gt;R&lt;/code&gt; using &lt;code&gt;lme4&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lmerTest)
model &amp;lt;- glmer(judgement ~ related * expression + 
                (1 + related | rater) + 
                (1 + expression | stimulus),
              data = my_data,
              family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Relatedness judgment (0 = unrelated, 1 = related) is the dependent variable, relatedness (0.5 = related, -0.5 = unrelated) and expression (0.5 = smiling, -0.5 = neutral) are the independent variables. We include the rater and stimulus id as random effects and use maximally specified slopes.&lt;/p&gt;
&lt;p&gt;See the attached RMarkdown file with the proposed analysis script.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;This is perhaps the most important and most complicated question within the preregistration. As with all of the other questions, the key is to provide a specific recipe for analyzing the collected data. Ask yourself: is enough detail provided to run the same analysis again with the information provided by the user? Be aware for instances where the statistical models appear specific, but actually leave openings for the precise test. See the following examples:&lt;/p&gt;
&lt;p&gt;If someone specifies a 2x3 ANOVA with both factors within subjects, there is still flexibility with the various types of ANOVAs that could be run. Either a repeated measures ANOVA (RMANOVA) or a multivariate ANOVA (MANOVA) could be used for that design, which are two different tests.&lt;/p&gt;
&lt;p&gt;If you are going to perform a sequential analysis and check after 50, 100, and 150 samples, you must also specify the p-values you’ll test against at those three points.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;transformations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Transformations&lt;/h3&gt;
&lt;p&gt;If you plan on transforming, centering, recoding the data, or will require a coding scheme for categorical variables, please describe that process.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;Relatedness judgments are coded as 1 for related and 0 for unrelated. Actual relatedness will be effect-coded as -0.5 for unrelated and 0.5 for related, and facial expression will be effect-coded as -0.5 for neutral and 0.5 for smiling.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;If any categorical predictors are included in a regression, indicate how those variables will be coded (e.g. dummy coding, summation coding, etc.) and what the reference category will be.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;follow-up-analyses&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Follow-up analyses&lt;/h3&gt;
&lt;p&gt;If not specified previously, will you be conducting any confirmatory analyses to follow up on effects in your statistical model, such as subgroup analyses, pairwise or complex contrasts, or follow-up tests from interactions? Remember that any analyses not specified in this research plan must be noted as exploratory.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;If there is an interaction between expression and relatedness, separate models will be run for smiling and neutral expressions to determine whether kin recognition is apparent for both expressions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_s &amp;lt;- glmer(judgement ~ related + 
                 (1 + related | rater) + 
                 (1 | stimulus),
              data = filter(my_data, expression == &amp;quot;smiling&amp;quot;),
              family = binomial)

model_n &amp;lt;- glmer(judgement ~ related + 
                 (1 + related | rater) + 
                 (1 | stimulus),
              data = filter(my_data, expression == &amp;quot;neutral&amp;quot;),
              family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;This is simply a place to allow entering in any additional analyses. The criteria for these follow up analyses are identical to any analyses listed in other sections. It is also fine to enter these follow up analyses in a separate analysis section. The purpose of this question is to allow entering of any follow up tests that naturally follow from a primary analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;inference-criteria&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Inference criteria&lt;/h3&gt;
&lt;p&gt;What criteria will you use to make inferences? Please describe the information you’ll use (e.g. specify the p-values, Bayes factors, specific model fit indices), as well as cut-off criterion, where appropriate. Will you be using one or two tailed tests for each of your analyses? If you are comparing multiple conditions or testing multiple hypotheses, will you account for this?&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
We will use an alpha criterion of .05, given that our specific predictions are pre-registered and our power analyses were calculated using this alpha.
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
P-values, confidence intervals, and effect sizes are standard means for making an inference, and any level is acceptable, though some criteria must be specified in this or previous fields. Bayesian analyses should specify a Bayes factor or a credible interval. If you are selecting models, then how will you determine the relative quality of each? In regards to multiple comparisons, this is a question with few “wrong” answers. In other words, transparency is more important than any specific method of controlling the false discovery rate or false error rate. One may state an intention to report all tests conducted or one may conduct a specific correction procedure; either strategy is acceptable.
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-exclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data exclusion&lt;/h3&gt;
&lt;p&gt;How will you determine which data points or samples (if any) to exclude from your analyses? How will outliers be handled?&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;Only responses of raters who have completed all 100 trials will be included in the analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;Any rule for excluding a particular set of data is acceptable. One may describe rules for excluding a rater or for identifying outlier data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;missing-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Missing data&lt;/h3&gt;
&lt;p&gt;How will you deal with incomplete or missing data?&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;If a rater does not complete all 100 trials they will be excluded from the analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;eval&#34;&gt;
&lt;p&gt;Any relevant explanation is acceptable. As a final reminder, remember that the final analysis must follow the specified plan, and deviations must be either strongly justified or included as a separate, exploratory analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-analysis-optional&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory analysis (optional)&lt;/h3&gt;
&lt;p&gt;If you plan to explore your data set to look for unexpected differences or relationships, you may describe those tests here. An exploratory test is any test where a prediction is not made up front, or there are multiple possible tests that you are going to use. A statistically significant finding in an exploratory test is a great way to form a new confirmatory hypothesis, which could be registered at a later time.&lt;/p&gt;
&lt;div class=&#34;example&#34;&gt;
&lt;p&gt;We will explore whether demographic traits of the raters are related to kin recognition. Therefore, we will look for relationships between demographic variables (age, sex, parental status) and third party kinship detection accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;script&gt;
/*
  document.body.onclick= function(e){
    e = window.event ? event.srcElement : e.target;
    if (e.className &amp;&amp; e.className.indexOf(&#39;example&#39;)!=-1) {
        var tbox = document.createElement(&#34;textarea&#34;);
        tbox.value = e.innerText.trim();
        e.parentNode.replaceChild(tbox, e);
    } else if (e.tagName != &#34;TEXTAREA&#34;) {
      var tboxes = document.getElementsByTagName(&#34;textarea&#34;)[0];
      var divbox = document.createElement(&#34;div&#34;);
      divbox.innerHTML = tboxes.value;
      divbox.className = &#34;example&#34;;
      e.parentNode.replaceChild(divbox, tboxes);
    }
  }
*/
&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hidden Solutions</title>
      <link>https://debruine.github.io/post/hidden-solutions/</link>
      <pubDate>Sat, 21 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/hidden-solutions/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;a href=&#34;http://swcarpentry.github.io/r-novice-gapminder/&#34;&gt;R for Reproducible Scientific Analysis&lt;/a&gt; pages at software carpentry have a really nice interface for hiding and showing solutions to exercises. I’ve created my own lightweight solution that you can use in any html file, including those made by RMarkdown (e.g., R notebooks).&lt;/p&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Graph the relationship between speed and distance for the &lt;code&gt;cars&lt;/code&gt; dataset.&lt;/p&gt;
&lt;div class=&#34;solution&#34;&gt;
&lt;button&gt;
Solution
&lt;/button&gt;
&lt;p&gt;You can put some text inside the solution, as well as code cunks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(cars, aes(speed, dist)) +
  geom_point(color = &amp;quot;purple&amp;quot;) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, color = &amp;quot;purple&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/hidden_solution_files/figure-html/cars-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Setting this up requires a few lines at the beginning and end of each file, plus surrounding your solutions with a line of html.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;put-this-at-the-beginning-of-your-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Put this at the beginning of your file&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;style&amp;gt;
  /* styles for hidden solutions */
  .solution {
    height: 2em;
    overflow-y: hidden;
    padding: 0.5em;
  }
  .solution.open { 
    height: auto; 
    background-color: rgba(0, 0, 0, 0.1);
    border-radius: 5px;
  }
  .solution button {
    height: 1.5em;
    margin-bottom: 0.5em;
  }
  .solution pre.sourceCode {
    border-color: green;
  }
&amp;lt;/style&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’re using &lt;a href=&#34;http://rmarkdown.rstudio.com/rmarkdown_websites.html&#34;&gt;RMarkdown Websites&lt;/a&gt;, you can just put these lines of css into an external stylesheet linked in your &lt;code&gt;_site.yml&lt;/code&gt; file (e.g., &lt;a href=&#34;rguppies.css&#34;&gt;rguppies.css&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;put-this-at-the-end-of-your-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Put this at the end of your file&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;script&amp;gt;
  window.onload = function(){
    var buttons = document.getElementsByTagName(&amp;quot;button&amp;quot;);
    for (var i = 0; i &amp;lt; buttons.length; i++) {
      buttons[i].onclick = function() {
        var cl = this.parentElement.classList;
        if (cl.contains(&amp;#39;open&amp;#39;)) {
          cl.remove(&amp;quot;open&amp;quot;);
        } else {
          cl.add(&amp;quot;open&amp;quot;);
        }
      }
    }
  }
&amp;lt;/script&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’re using &lt;a href=&#34;http://rmarkdown.rstudio.com/rmarkdown_websites.html&#34;&gt;RMarkdown Websites&lt;/a&gt;, you can just put this script into an external footer or script file linked in your &lt;code&gt;_site.yml&lt;/code&gt; file (e.g., &lt;a href=&#34;rguppies.js&#34;&gt;rguppies.js&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;surround-your-hidden-solutions-like-this&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Surround your hidden solutions like this&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;div class=&amp;quot;solution&amp;quot;&amp;gt;&amp;lt;button&amp;gt;Solution&amp;lt;/button&amp;gt;

PUT YOUR SOLUTION HERE (including r chunks)

&amp;lt;/div&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can change the text on the button to something else (e.g., &lt;code&gt;&amp;lt;button&amp;gt;View the Answer&amp;lt;/button&amp;gt;&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let me know if you have any suggestions or find this useful.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>psych::alpha()</title>
      <link>https://debruine.github.io/post/psych-alpha/</link>
      <pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/psych-alpha/</guid>
      <description>


&lt;p&gt;This is a tutorial in progress on how to calculate Cronbach’s alphas using the &lt;code&gt;psych&lt;/code&gt; package in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(psych)

disgust &amp;lt;- read_csv(&amp;quot;https://psyteachr.github.io/msc-data-skills/data/disgust.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;spss&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;SPSS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Under the &lt;code&gt;Analyze&lt;/code&gt; menu, choose &lt;code&gt;Scale&lt;/code&gt; and &lt;code&gt;Reliability Analysis...&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Choose the 7 moral disgust items&lt;/li&gt;
&lt;li&gt;Make sure the Model is “Alpha”&lt;/li&gt;
&lt;li&gt;Under &lt;code&gt;Statistics&lt;/code&gt;, add descriptives for Item, Scale, and Scale if Item Deleted&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/images/moral_alpha.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;psychalpha&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;psych::alpha()&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;alpha(x, keys=NULL, cumulative=FALSE, title=NULL, max=10, na.rm=TRUE,&lt;/code&gt;&lt;br /&gt;
      &lt;code&gt;check.keys=FALSE, n.iter=1, delete=TRUE, use=&#34;pairwise&#34;, warnings=TRUE, n.obs=NULL)&lt;/code&gt;&lt;/p&gt;
&lt;div id=&#34;arguments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Arguments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; A data.frame or matrix of data, or a covariance or correlation matrix&lt;/li&gt;
&lt;li&gt;&lt;code&gt;keys&lt;/code&gt; If some items are to be reversed keyed, then either specify the direction of all items or just a vector of which items to reverse&lt;/li&gt;
&lt;li&gt;&lt;code&gt;title&lt;/code&gt; Any text string to identify this run&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cumulative&lt;/code&gt; should means reflect the sum of items or the mean of the items. The default value is means.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max&lt;/code&gt; the number of categories/item to consider if reporting category frequencies. Defaults to 10, passed to link{response.frequencies}&lt;/li&gt;
&lt;li&gt;&lt;code&gt;na.rm&lt;/code&gt; The default is to remove missing values and find pairwise correlations&lt;/li&gt;
&lt;li&gt;&lt;code&gt;check.keys&lt;/code&gt; if TRUE, then find the first principal component and reverse key items with negative loadings. Give a warning if this happens.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n.iter&lt;/code&gt; Number of iterations if bootstrapped confidence intervals are desired&lt;/li&gt;
&lt;li&gt;&lt;code&gt;delete&lt;/code&gt; Delete items with no variance and issue a warning&lt;/li&gt;
&lt;li&gt;&lt;code&gt;use&lt;/code&gt; Options to pass to the cor function: “everything”, “all.obs”, “complete.obs”, “na.or.complete”, or “pairwise.complete.obs”. The default is “pairwise”&lt;/li&gt;
&lt;li&gt;&lt;code&gt;warnings&lt;/code&gt; By default print a warning and a message that items were reversed. Suppress the message if warnings = FALSE&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n.obs&lt;/code&gt; If using correlation matrices as input, by specify the number of observations, we can find confidence intervals&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;disgust %&amp;gt;%
  select(moral1:moral7) %&amp;gt;%
  psych::alpha(title = &amp;quot;moral&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Reliability analysis  moral  
## Call: psych::alpha(x = ., title = &amp;quot;moral&amp;quot;)
## 
##   raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r
##       0.85      0.85    0.84      0.45 5.8 0.0016  3.8 1.3     0.46
## 
##  lower alpha upper     95% confidence boundaries
## 0.85 0.85 0.85 
## 
##  Reliability if an item is dropped:
##        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r
## moral1      0.83      0.83    0.81      0.45 4.9   0.0019 0.0035  0.45
## moral2      0.82      0.82    0.80      0.43 4.6   0.0019 0.0033  0.42
## moral3      0.83      0.83    0.81      0.45 5.0   0.0019 0.0040  0.45
## moral4      0.84      0.84    0.82      0.47 5.3   0.0017 0.0023  0.48
## moral5      0.83      0.83    0.81      0.44 4.8   0.0019 0.0038  0.45
## moral6      0.84      0.84    0.82      0.47 5.4   0.0017 0.0033  0.49
## moral7      0.82      0.83    0.81      0.44 4.8   0.0019 0.0043  0.44
## 
##  Item statistics 
##            n raw.r std.r r.cor r.drop mean  sd
## moral1 19668  0.74  0.73  0.67   0.62  3.1 1.9
## moral2 19662  0.77  0.78  0.75   0.68  4.6 1.5
## moral3 19681  0.74  0.73  0.67   0.62  3.2 1.8
## moral4 19656  0.66  0.68  0.60   0.54  4.5 1.5
## moral5 19668  0.76  0.75  0.70   0.64  3.8 1.9
## moral6 19679  0.68  0.67  0.58   0.54  3.8 1.8
## moral7 19665  0.76  0.76  0.70   0.65  3.7 1.7
## 
## Non missing response frequency for each item
##           0    1    2    3    4    5    6 miss
## moral1 0.11 0.13 0.14 0.18 0.18 0.15 0.12 0.02
## moral2 0.03 0.03 0.05 0.09 0.18 0.28 0.34 0.02
## moral3 0.10 0.11 0.13 0.17 0.20 0.17 0.12 0.02
## moral4 0.03 0.03 0.06 0.11 0.19 0.29 0.30 0.02
## moral5 0.07 0.08 0.10 0.15 0.17 0.21 0.23 0.02
## moral6 0.07 0.07 0.10 0.14 0.20 0.22 0.20 0.02
## moral7 0.06 0.08 0.10 0.17 0.21 0.22 0.16 0.02&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;disgust %&amp;gt;%
  select(sexual1:sexual7) %&amp;gt;%
  psych::alpha(title = &amp;quot;sexual disgust&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Reliability analysis  sexual disgust  
## Call: psych::alpha(x = ., title = &amp;quot;sexual disgust&amp;quot;)
## 
##   raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r
##       0.81      0.81     0.8      0.38 4.3 0.0021  2.6 1.4      0.4
## 
##  lower alpha upper     95% confidence boundaries
## 0.8 0.81 0.81 
## 
##  Reliability if an item is dropped:
##         raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r
## sexual1      0.77      0.78    0.76      0.37 3.5   0.0025 0.0073  0.38
## sexual2      0.79      0.79    0.77      0.38 3.7   0.0023 0.0043  0.40
## sexual3      0.77      0.77    0.74      0.36 3.3   0.0025 0.0045  0.38
## sexual4      0.79      0.80    0.77      0.39 3.9   0.0023 0.0073  0.40
## sexual5      0.77      0.78    0.76      0.37 3.5   0.0025 0.0077  0.37
## sexual6      0.80      0.80    0.78      0.40 4.0   0.0022 0.0052  0.40
## sexual7      0.79      0.79    0.77      0.38 3.7   0.0024 0.0078  0.40
## 
##  Item statistics 
##             n raw.r std.r r.cor r.drop mean  sd
## sexual1 19693  0.71  0.72  0.66   0.59  2.4 1.9
## sexual2 19664  0.65  0.67  0.59   0.52  1.4 1.8
## sexual3 19690  0.74  0.75  0.71   0.63  1.6 1.9
## sexual4 19703  0.64  0.64  0.54   0.49  3.0 2.0
## sexual5 19695  0.73  0.72  0.66   0.59  2.7 2.1
## sexual6 19670  0.62  0.62  0.52   0.46  3.9 2.1
## sexual7 19684  0.69  0.67  0.59   0.53  2.9 2.2
## 
## Non missing response frequency for each item
##            0    1    2    3    4    5    6 miss
## sexual1 0.20 0.19 0.16 0.16 0.13 0.09 0.07 0.02
## sexual2 0.47 0.19 0.10 0.09 0.05 0.04 0.05 0.02
## sexual3 0.41 0.20 0.11 0.10 0.07 0.05 0.06 0.02
## sexual4 0.14 0.13 0.14 0.15 0.16 0.14 0.13 0.01
## sexual5 0.20 0.16 0.13 0.14 0.11 0.11 0.14 0.02
## sexual6 0.10 0.08 0.08 0.11 0.12 0.19 0.33 0.02
## sexual7 0.21 0.15 0.11 0.11 0.10 0.12 0.20 0.02&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;disgust %&amp;gt;%
  select(pathogen1:pathogen7) %&amp;gt;%
  psych::alpha(title = &amp;quot;pathogen disgust&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Reliability analysis  pathogen disgust  
## Call: psych::alpha(x = ., title = &amp;quot;pathogen disgust&amp;quot;)
## 
##   raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd median_r
##       0.74      0.74    0.72      0.29 2.9 0.0028  3.7 1.1      0.3
## 
##  lower alpha upper     95% confidence boundaries
## 0.73 0.74 0.74 
## 
##  Reliability if an item is dropped:
##           raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r
## pathogen1      0.71      0.71    0.69      0.29 2.5   0.0032 0.0042  0.26
## pathogen2      0.70      0.71    0.68      0.29 2.5   0.0032 0.0033  0.30
## pathogen3      0.70      0.70    0.67      0.28 2.4   0.0033 0.0028  0.26
## pathogen4      0.71      0.72    0.69      0.30 2.5   0.0032 0.0042  0.30
## pathogen5      0.70      0.70    0.67      0.28 2.4   0.0033 0.0030  0.26
## pathogen6      0.72      0.72    0.70      0.30 2.6   0.0031 0.0042  0.31
## pathogen7      0.71      0.72    0.69      0.30 2.6   0.0031 0.0037  0.30
## 
##  Item statistics 
##               n raw.r std.r r.cor r.drop mean  sd
## pathogen1 19668  0.60  0.63  0.53   0.45  4.4 1.5
## pathogen2 19683  0.64  0.63  0.54   0.46  3.3 1.7
## pathogen3 19687  0.65  0.66  0.58   0.49  3.2 1.6
## pathogen4 19683  0.62  0.62  0.52   0.44  3.7 1.8
## pathogen5 19678  0.64  0.67  0.59   0.50  4.3 1.4
## pathogen6 19655  0.61  0.59  0.48   0.41  3.8 1.9
## pathogen7 19692  0.63  0.61  0.50   0.43  3.5 1.9
## 
## Non missing response frequency for each item
##              0    1    2    3    4    5    6 miss
## pathogen1 0.01 0.04 0.07 0.11 0.22 0.26 0.29 0.02
## pathogen2 0.07 0.12 0.14 0.18 0.22 0.16 0.11 0.02
## pathogen3 0.05 0.14 0.17 0.19 0.24 0.14 0.08 0.02
## pathogen4 0.04 0.11 0.12 0.15 0.21 0.19 0.18 0.02
## pathogen5 0.01 0.04 0.08 0.13 0.24 0.27 0.23 0.02
## pathogen6 0.06 0.10 0.10 0.13 0.18 0.19 0.25 0.02
## pathogen7 0.08 0.12 0.12 0.13 0.17 0.17 0.20 0.02&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>nest() and irr::icc()</title>
      <link>https://debruine.github.io/post/nest/</link>
      <pubDate>Mon, 21 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/nest/</guid>
      <description>


&lt;p&gt;I’m going to use intra-class correlations to demonstrate how to run an analysis on subgroups of data (because I’m constantly forgetting exactly how to do it).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(irr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load the rating data for the open-source &lt;a href=&#34;https://figshare.com/articles/Face_Research_Lab_London_Set/5047666&#34;&gt;Face Research Lab London Set&lt;/a&gt;.
The data set contains 1-7 attractiveness ratings from 2513 raters for the 102 faces in the set (&lt;code&gt;X001:X173&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;london &amp;lt;- read_csv(&amp;quot;https://ndownloader.figshare.com/files/8542045&amp;quot;)

head(london)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 105
##   rater_sex rater_sexpref rater_age  X001  X002  X003  X004  X005  X006  X007
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 female    either             17       3     3     3     3     2     3     5
## 2 female    either             17       5     2     3     2     1     5     6
## 3 female    either             17.1     5     3     4     3     3     4     4
## 4 female    either             17.1     4     6     5     5     3     4     5
## 5 female    either             17.2     3     4     3     1     1     1     3
## 6 female    either             17.3     6     5     5     3     7     5     6
## # … with 95 more variables: X008 &amp;lt;dbl&amp;gt;, X009 &amp;lt;dbl&amp;gt;, X010 &amp;lt;dbl&amp;gt;, X011 &amp;lt;dbl&amp;gt;,
## #   X012 &amp;lt;dbl&amp;gt;, X013 &amp;lt;dbl&amp;gt;, X014 &amp;lt;dbl&amp;gt;, X016 &amp;lt;dbl&amp;gt;, X017 &amp;lt;dbl&amp;gt;, X018 &amp;lt;dbl&amp;gt;,
## #   X019 &amp;lt;dbl&amp;gt;, X020 &amp;lt;dbl&amp;gt;, X021 &amp;lt;dbl&amp;gt;, X022 &amp;lt;dbl&amp;gt;, X024 &amp;lt;dbl&amp;gt;, X025 &amp;lt;dbl&amp;gt;,
## #   X026 &amp;lt;dbl&amp;gt;, X027 &amp;lt;dbl&amp;gt;, X029 &amp;lt;dbl&amp;gt;, X030 &amp;lt;dbl&amp;gt;, X031 &amp;lt;dbl&amp;gt;, X032 &amp;lt;dbl&amp;gt;,
## #   X033 &amp;lt;dbl&amp;gt;, X034 &amp;lt;dbl&amp;gt;, X036 &amp;lt;dbl&amp;gt;, X037 &amp;lt;dbl&amp;gt;, X038 &amp;lt;dbl&amp;gt;, X039 &amp;lt;dbl&amp;gt;,
## #   X041 &amp;lt;dbl&amp;gt;, X042 &amp;lt;dbl&amp;gt;, X043 &amp;lt;dbl&amp;gt;, X044 &amp;lt;dbl&amp;gt;, X045 &amp;lt;dbl&amp;gt;, X061 &amp;lt;dbl&amp;gt;,
## #   X062 &amp;lt;dbl&amp;gt;, X063 &amp;lt;dbl&amp;gt;, X064 &amp;lt;dbl&amp;gt;, X066 &amp;lt;dbl&amp;gt;, X067 &amp;lt;dbl&amp;gt;, X068 &amp;lt;dbl&amp;gt;,
## #   X069 &amp;lt;dbl&amp;gt;, X070 &amp;lt;dbl&amp;gt;, X081 &amp;lt;dbl&amp;gt;, X082 &amp;lt;dbl&amp;gt;, X083 &amp;lt;dbl&amp;gt;, X086 &amp;lt;dbl&amp;gt;,
## #   X087 &amp;lt;dbl&amp;gt;, X090 &amp;lt;dbl&amp;gt;, X091 &amp;lt;dbl&amp;gt;, X092 &amp;lt;dbl&amp;gt;, X094 &amp;lt;dbl&amp;gt;, X096 &amp;lt;dbl&amp;gt;,
## #   X097 &amp;lt;dbl&amp;gt;, X099 &amp;lt;dbl&amp;gt;, X100 &amp;lt;dbl&amp;gt;, X101 &amp;lt;dbl&amp;gt;, X102 &amp;lt;dbl&amp;gt;, X103 &amp;lt;dbl&amp;gt;,
## #   X104 &amp;lt;dbl&amp;gt;, X105 &amp;lt;dbl&amp;gt;, X107 &amp;lt;dbl&amp;gt;, X108 &amp;lt;dbl&amp;gt;, X112 &amp;lt;dbl&amp;gt;, X113 &amp;lt;dbl&amp;gt;,
## #   X114 &amp;lt;dbl&amp;gt;, X115 &amp;lt;dbl&amp;gt;, X117 &amp;lt;dbl&amp;gt;, X118 &amp;lt;dbl&amp;gt;, X119 &amp;lt;dbl&amp;gt;, X120 &amp;lt;dbl&amp;gt;,
## #   X121 &amp;lt;dbl&amp;gt;, X122 &amp;lt;dbl&amp;gt;, X123 &amp;lt;dbl&amp;gt;, X124 &amp;lt;dbl&amp;gt;, X125 &amp;lt;dbl&amp;gt;, X126 &amp;lt;dbl&amp;gt;,
## #   X127 &amp;lt;dbl&amp;gt;, X128 &amp;lt;dbl&amp;gt;, X129 &amp;lt;dbl&amp;gt;, X130 &amp;lt;dbl&amp;gt;, X131 &amp;lt;dbl&amp;gt;, X132 &amp;lt;dbl&amp;gt;,
## #   X134 &amp;lt;dbl&amp;gt;, X135 &amp;lt;dbl&amp;gt;, X136 &amp;lt;dbl&amp;gt;, X137 &amp;lt;dbl&amp;gt;, X138 &amp;lt;dbl&amp;gt;, X139 &amp;lt;dbl&amp;gt;,
## #   X140 &amp;lt;dbl&amp;gt;, X141 &amp;lt;dbl&amp;gt;, X142 &amp;lt;dbl&amp;gt;, X143 &amp;lt;dbl&amp;gt;, X144 &amp;lt;dbl&amp;gt;, X172 &amp;lt;dbl&amp;gt;,
## #   X173 &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To calculate the ICC for ratings, first we need to get the data into a format where each column represents a rater and each row represents a stimulus. Select just the columns with ratings, then transpose (&lt;code&gt;t()&lt;/code&gt;) the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;london %&amp;gt;%
  select(X001:X173) %&amp;gt;%
  t() %&amp;gt;%
  irr::icc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Single Score Intraclass Correlation
## 
##    Model: oneway 
##    Type : consistency 
## 
##    Subjects = 102 
##      Raters = 2513 
##      ICC(1) = 0.24
## 
##  F-Test, H0: r0 = 0 ; H1: r0 &amp;gt; 0 
## F(101,256224) = 793 , p = 0 
## 
##  95%-Confidence Interval for ICC Population Values:
##   0.196 &amp;lt; ICC &amp;lt; 0.298&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what if you want to do this for several subsets of the raters or stimuli? One solution is to run the code above several times, once for each subset, adding code to select and filter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;london %&amp;gt;%
  filter(rater_sex == &amp;quot;male&amp;quot;) %&amp;gt;%
  select(X001:X173) %&amp;gt;%
  t() %&amp;gt;%
  irr::icc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Single Score Intraclass Correlation
## 
##    Model: oneway 
##    Type : consistency 
## 
##    Subjects = 102 
##      Raters = 955 
##      ICC(1) = 0.225
## 
##  F-Test, H0: r0 = 0 ; H1: r0 &amp;gt; 0 
## F(101,97308) = 279 , p = 0 
## 
##  95%-Confidence Interval for ICC Population Values:
##   0.183 &amp;lt; ICC &amp;lt; 0.281&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;london %&amp;gt;%
  filter(rater_sex == &amp;quot;female&amp;quot;) %&amp;gt;%
  select(X001:X173) %&amp;gt;%
  t() %&amp;gt;%
  irr::icc()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Single Score Intraclass Correlation
## 
##    Model: oneway 
##    Type : consistency 
## 
##    Subjects = 102 
##      Raters = 1552 
##      ICC(1) = 0.253
## 
##  F-Test, H0: r0 = 0 ; H1: r0 &amp;gt; 0 
## F(101,158202) = 526 , p = 0 
## 
##  95%-Confidence Interval for ICC Population Values:
##   0.207 &amp;lt; ICC &amp;lt; 0.313&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what if you want to calculate ICCs for lots of subdivisions? It’s tedious and error-prone to do each one by hand, but you can group your data into the subdivisions, nest the ratings, and map them onto a function.&lt;/p&gt;
&lt;p&gt;First, we have to write a function that takes the data and returns a table of the stats you’re interested in. The &lt;code&gt;irr::icc()&lt;/code&gt; function returns a list, which won’t play well with nesting later, so we &lt;code&gt;unlist()&lt;/code&gt; it, transpose it so it’s a row of values, not a column, turn it back into a tibble (transposing turns it into a matrix), and select just the columns you want.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_icc &amp;lt;- function(data) {
  data %&amp;gt;%
    select(X001:X173) %&amp;gt;% # select just the rating columns
    t() %&amp;gt;%               # transpose so columns are raters and rows are stimuli
    irr::icc() %&amp;gt;%        # calculate the ICC
    unlist() %&amp;gt;%          # turn the output list into a vector
    t() %&amp;gt;%               # transpose this vector
    as_tibble() %&amp;gt;%       # turn the vector into a table 
    select(               # select just the columns you want
      stimuli = subjects, # rename subjects to stimuli     
      raters, 
      icc = value,        # rename value to icc
      lbound, 
      ubound
    ) %&amp;gt;%
    # fix column modes (unlisting turned them all into characters)
    mutate_at(vars(stimuli, raters), as.integer) %&amp;gt;% 
    mutate_at(vars(icc:ubound), as.numeric)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Test the function on the whole dataset to check it gives you the right data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_icc(london)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 5
##   stimuli raters   icc lbound ubound
##     &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1     102   2513 0.240  0.196  0.298&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can group our full dataframe. Here I’ve created a new column of age group and filtered out age/sex groups with fewer than 10 raters. After you group your data, use the &lt;code&gt;nest()&lt;/code&gt; function to turn all the rest of the columns into a separate table for each group (stored in the column &lt;code&gt;data&lt;/code&gt;). Then you can map these tables onto your &lt;code&gt;my_icc&lt;/code&gt; function. Finally, unnest this new &lt;code&gt;icc&lt;/code&gt; column to re-expand your table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;london_icc_grouped &amp;lt;- london %&amp;gt;%
  mutate(age_group = round(rater_age / 10)*10) %&amp;gt;% # create age group by decade
  group_by(rater_sex, age_group) %&amp;gt;%               # group by rater age and sex
  filter(n() &amp;gt;= 10) %&amp;gt;%                            # remove groups smaller than 10
  nest() %&amp;gt;%                                       # nest the rest of the columns
  mutate(icc = map(data, my_icc)) %&amp;gt;%              # calculate ICC for each group
  unnest(icc) %&amp;gt;%                                  # expand the tables returned to icc
  select(-data)                                    # get rid of the data column
  
london_icc_grouped&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 7
## # Groups:   rater_sex, age_group [10]
##    rater_sex age_group stimuli raters   icc lbound ubound
##    &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;   &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 female           20     102   1035 0.253  0.207  0.313
##  2 female           30     102    317 0.257  0.211  0.319
##  3 female           40     102    123 0.264  0.216  0.327
##  4 female           50     102     54 0.255  0.206  0.319
##  5 female           60     102     20 0.271  0.215  0.342
##  6 male             20     102    478 0.211  0.171  0.265
##  7 male             30     102    253 0.252  0.206  0.312
##  8 male             40     102    119 0.217  0.175  0.274
##  9 male             50     102     74 0.267  0.218  0.332
## 10 male             60     102     27 0.245  0.194  0.311&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Cumulative p</title>
      <link>https://debruine.github.io/post/cumulative-p/</link>
      <pubDate>Thu, 27 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/cumulative-p/</guid>
      <description>
&lt;script src=&#34;https://debruine.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;EJ Wagenmakers started an interesting debate last night with a &lt;a href=&#34;https://twitter.com/EJWagenmakers/status/889987997046910978&#34;&gt;twitter poll on p-values&lt;/a&gt;. Some responses suggested you can multiply p-values from several tests to create a sort of &lt;a href=&#34;https://twitter.com/VladMalik/status/890246773515722752&#34;&gt;cumulative p-value&lt;/a&gt; that is the joint probability of the null hypothesis.&lt;/p&gt;
&lt;p&gt;I also used to believe that you could multiply p-values, but am now a bit embarassed at my misunderstanding, common as it is. The p-value is &lt;em&gt;not&lt;/em&gt; the probability that the null hypothesis is true, it is the probability of obtaining the current (or more extreme) values under the null hypothesis. This important distinction means you cannot multiply p-values to obtain the joint probability of several tests.&lt;/p&gt;
&lt;p&gt;First, I’ll write a simple function to generate two sets of &lt;code&gt;n&lt;/code&gt; samples from a normal distribution with the same mean and SD, then return the p-value for a t-test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nullp &amp;lt;- function(n) {
  a = rnorm(n)
  b = rnorm(n)
  t = t.test(a, b)
  t$p.value
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ll run this simulation 10000 times with samples of n = 1000 to get a good distribution of p-values under the null hypothesis. The histogram shows that this is a unifrom distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ps &amp;lt;- replicate(10000, nullp(1000))

hist(ps)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/cumulative-p_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then sample 1000 p-values from this distributions once, twice, thrice, and whatever the word is for four times. This should convince you that the cumulative p-value cannot provide the joint probability of the null hypothesis for multiple tests.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(
  x1 = sample(ps, 1000),
  x2 = sample(ps, 1000) * sample(ps, 1000),
  x3 = sample(ps, 1000) * sample(ps, 1000) * sample(ps, 1000),
  x4 = sample(ps, 1000) * sample(ps, 1000) * sample(ps, 1000) * sample(ps, 1000)
) %&amp;gt;%
  gather(&amp;quot;n_tests&amp;quot;, &amp;quot;cum_p&amp;quot;, x1:x4) %&amp;gt;%
  ggplot(aes(cum_p, fill=n_tests)) +
  geom_density(alpha = 0.5) +
  labs(x =&amp;quot;Cumulative p-value&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://debruine.github.io/post/cumulative-p_files/figure-html/unnamed-chunk-3-1.png&#34; alt=&#34;Cumulative p-value distribution under the null hypothesis&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Cumulative p-value distribution under the null hypothesis
&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>sample()</title>
      <link>https://debruine.github.io/post/sample/</link>
      <pubDate>Wed, 26 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/sample/</guid>
      <description>


&lt;p&gt;First, let’s make a data frame with two variables, &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; that are both sampled from a normal distribution with a mean of 0 and SD of 1. The variablle &lt;code&gt;n&lt;/code&gt; will be how many samples we’ll take (100). Then we can run a t-test to see if they are different.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ────────────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
## ✓ tibble  3.0.3     ✓ dplyr   1.0.2
## ✓ tidyr   1.1.1     ✓ stringr 1.4.0
## ✓ readr   1.3.1     ✓ forcats 0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ───────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n = 100

data &amp;lt;- data.frame(
  a = rnorm(n, 0, 1),
  b = rnorm(n, 0, 1)
)

t &amp;lt;- t.test(data$a,data$b)

t$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1527518&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s repeat that procedure 1000 times. The easiest way to do that is to make a function that returns the information you want.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tPower &amp;lt;- function() {
  n = 100

  data &amp;lt;- data.frame(
    a = rnorm(n, 0, 1),
    b = rnorm(n, 0, 1)
  )

  t &amp;lt;- t.test(data$a,data$b)
  
  return(t$p.value)
}

tPower()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7583175&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mySample &amp;lt;- data.frame(
  p = replicate(10000, tPower())
) 

mySample %&amp;gt;%
  ggplot(aes(p)) +
  geom_histogram(bins = 20, boundary = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/power_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(mySample$p &amp;lt; .05)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0528&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if you induced a small effect of 0.2 SD?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tPower2 &amp;lt;- function() {
  n = 100

  data &amp;lt;- data.frame(
    a = rnorm(n, 0, 1),
    b = rnorm(n, 0.2, 1)
  )

  t &amp;lt;- t.test(data$a,data$b)
  
  return(t$p.value)
}

tPower2()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9142489&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mySample2 &amp;lt;- data.frame(
  p = replicate(10000, tPower2())
) 

mySample2 %&amp;gt;%
  ggplot(aes(p)) +
  geom_histogram(bins = 20, boundary = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/power_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(mySample2$p &amp;lt; .05)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2929&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hmm, you only get a p-value less than .05 30% of the time. That means that your study would only have 30% power to detect an effect this big with 100 subjects. Let’s make a new function to give you the p-value of a study with any number of subjects (you put the N inside the parentheses of the function).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tPowerN &amp;lt;- function(n) {

  data &amp;lt;- data.frame(
    a = rnorm(n, 0, 1),
    b = rnorm(n, 0.2, 1)
  )

  t &amp;lt;- t.test(data$a,data$b)
  
  return(t$p.value)
}

tPowerN(200)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2969539&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mySampleN &amp;lt;- data.frame(
  p200 = replicate(10000, tPowerN(200))
) 

mySampleN %&amp;gt;%
  ggplot(aes(p200)) +
  geom_histogram(bins = 20, boundary = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/power_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(mySampleN$p200 &amp;lt; .05)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5137&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nlist &amp;lt;- seq(200, 1000, by = 100)

remove(mySampleN)
for (n in nlist) {
  temp &amp;lt;- data.frame(
    n = n,
    p = replicate(1000, tPowerN(n))
  ) 
  
  if (exists(&amp;quot;mySampleN&amp;quot;)) {
    mySampleN &amp;lt;- rbind(mySampleN, temp)
  } else {
    mySampleN &amp;lt;- temp
  }
  remove(temp)
  print(n)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 200
## [1] 300
## [1] 400
## [1] 500
## [1] 600
## [1] 700
## [1] 800
## [1] 900
## [1] 1000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mySampleN %&amp;gt;%
  ggplot(aes(p)) +
  geom_histogram(bins = 20, boundary = 0) +
  facet_wrap(~n, nrow = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/power_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>recode()</title>
      <link>https://debruine.github.io/post/recode/</link>
      <pubDate>Fri, 14 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/recode/</guid>
      <description>


&lt;p&gt;I often find myself needing to recode variables. I wrote previously about recoding a characters into a numbers using various &lt;a href=&#34;coding.html&#34;&gt;coding schemes&lt;/a&gt;. But sometimes I want to recode numeric values into characters; this is particularly useful for graphing and for double-checking the meaning of your variable levels.&lt;/p&gt;
&lt;p&gt;First, I’ll create a data frame with 50 subjects and randomly choose their genders from a list of 4 possibilities with the population proportions 40% male, 40% female, 10% non-binary, and 10% missing data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages( library(tidyverse) )

set.seed(12)  # for reproducibility; delete when running simulations

genders &amp;lt;- c(&amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;nonbinary&amp;quot;, NA)

df &amp;lt;- data.frame(
  id = rep(1:50),
  gender = sample(genders, 50, replace = TRUE, prob = c(.4, .4, .1, .1))
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ll graph it to make sure it looks like I expect. This is one of the few times a &lt;a href=&#34;https://barbarplots.github.io/&#34;&gt;bar plot&lt;/a&gt; is appropriate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;%
  ggplot(aes(gender)) +
  geom_bar(fill=&amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/recode_files/figure-html/recode-orig-data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now I’m going to transform the character variables into numbers and graph it again. As you can see, when a categorical variable is coded with numbers, the missing values are omitted from the graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df2 &amp;lt;- df %&amp;gt;%
  mutate(
    gender.num = recode(gender, &amp;quot;male&amp;quot; = 1, &amp;quot;female&amp;quot; = 2, &amp;quot;nonbinary&amp;quot; = 3)
  )

df2 %&amp;gt;%
  ggplot(aes(gender.num)) +
  geom_bar(fill=&amp;quot;darkorange&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 3 rows containing non-finite values (stat_count).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/recode_files/figure-html/recode-numbers-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now I’m going to recode the numeric column back into words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# this won&amp;#39;t work
df3 &amp;lt;- df2 %&amp;gt;%
  mutate(
    gender.cat = recode(gender.num, 1 = &amp;quot;male&amp;quot;, 2 = &amp;quot;female&amp;quot;, 3 = &amp;quot;nonbinary&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That didn’t work. You’ll get an error that looks like:&lt;/p&gt;
&lt;pre style=&#34;color: red;&#34;&gt;Error: unexpected &#39;=&#39; in:
&#34;  mutate(
    gender.cat = recode(gender.num, 1 =&#34;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;recode()&lt;/code&gt; requires that the left side of the equal sign be in quotes. Let’s try this again and graph it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df3 &amp;lt;- df2 %&amp;gt;%
  mutate(
    gender.cat = recode(gender.num, &amp;quot;1&amp;quot; = &amp;quot;male&amp;quot;, &amp;quot;2&amp;quot; = &amp;quot;female&amp;quot;, &amp;quot;3&amp;quot; = &amp;quot;nonbinary&amp;quot;)
  )

df3 %&amp;gt;%
  ggplot(aes(gender.cat)) +
  geom_bar(fill=&amp;quot;goldenrod&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/recode_files/figure-html/recode-characters-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What if you want your variables in a different order? You can use the &lt;code&gt;factor()&lt;/code&gt; function to set the order of the levels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df4 &amp;lt;- df3 %&amp;gt;%
  mutate(
    gender.ordered = factor(gender.cat, levels = c(&amp;quot;nonbinary&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;))
  )

df4 %&amp;gt;%
  ggplot(aes(gender.ordered)) +
  geom_bar(fill=&amp;quot;darkgreen&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/recode_files/figure-html/recode-ordered-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s put it all together to see how you can go from numeric to character values and get them in the order you want. We’ll start with an “original” dataframe of just the numerically coded genders from the previous data. Then we’ll make a new data frame by recoding the numeric column into words and then ordering this.&lt;/p&gt;
&lt;p&gt;Note that I’ve given the new column the name &lt;code&gt;gender.ordered&lt;/code&gt; and then redefined this column with the reordered levels. This is a nice feature of the tidyverse. You could put all that code on one line with complicated brackets, but it’s easier to manipulate a variable in steps and you can use previously created variables in subsequent steps of a &lt;code&gt;mutate()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.original &amp;lt;- df4 %&amp;gt;% select(gender.num)

data.recoded &amp;lt;- data.original %&amp;gt;%
  mutate(
    gender.ordered = recode(gender.num, &amp;quot;1&amp;quot; = &amp;quot;male&amp;quot;, &amp;quot;2&amp;quot; = &amp;quot;female&amp;quot;, &amp;quot;3&amp;quot; = &amp;quot;nonbinary&amp;quot;),
    gender.ordered = factor(gender.ordered, levels = c(&amp;quot;nonbinary&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;))
  )

data.recoded %&amp;gt;%
  ggplot(aes(gender.ordered)) +
  geom_bar(fill=&amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/recode_files/figure-html/recode-all-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PCA</title>
      <link>https://debruine.github.io/post/pca/</link>
      <pubDate>Mon, 26 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/pca/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;pca.Rmd&#34;&gt;Download the Rmd notebook for this example&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Putting together this page made me realise I still don’t know anything about PCA and factor analysis.&lt;/p&gt;
&lt;p&gt;I use the &lt;code&gt;psych&lt;/code&gt; package for SPSS-style PCA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(psych)
library(viridis)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, I’ll simulate some data with an underlying structure of three factors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(444)  # for reproducibility; delete when running simulations

a &amp;lt;- rnorm(100, 0, 1)
b &amp;lt;- rnorm(100, 0, 1)
c &amp;lt;- rnorm(100, 0, 1)

df &amp;lt;- data.frame(
  id = seq(1,100),
  a1 = a + rnorm(100, 0, 1),
  a2 = a + rnorm(100, 0, .8),
  a3 = a + rnorm(100, 0, .6),
  a4 = -a + rnorm(100, 0, .4),
  b1 = b + rnorm(100, 0, 1),
  b2 = b + rnorm(100, 0, .8),
  b3 = b + rnorm(100, 0, .6),
  b4 = -b + rnorm(100, 0, .4),
  c1 = c + rnorm(100, 0, 1),
  c2 = c + rnorm(100, 0, .8),
  c3 = c + rnorm(100, 0, .6),
  c4 = -c + rnorm(100, 0, .4)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Select just the columns you want for your PCA. You can visualise their correlations with &lt;code&gt;cor()&lt;/code&gt; and &lt;code&gt;ggplot()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traits &amp;lt;- df %&amp;gt;% select(-id) 

traits %&amp;gt;%
  cor() %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(var1 = rownames(.)) %&amp;gt;%
  gather(&amp;quot;var2&amp;quot;, &amp;quot;value&amp;quot;, a1:c4) %&amp;gt;%
  mutate(var1 = factor(var1), var1 = factor(var1, levels = rev(levels(var1)))) %&amp;gt;%
  ggplot(aes(var2, var1, fill=value)) +
  geom_tile() +
  scale_x_discrete(position = &amp;quot;top&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
  scale_fill_viridis(limits=c(-1, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/pca_files/figure-html/pca-cor-vars-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Determine the number of factors to extract. Here I use the SPSS-style default criterion of Eigenvalues &amp;gt; 1&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ev &amp;lt;- eigen(cor(traits));
nfactors &amp;lt;- length(ev$values[ev$values &amp;gt; 1]);
nfactors&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;principal-components-analysis-spss-style&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Principal components analysis (SPSS-style)&lt;/h2&gt;
&lt;div id=&#34;principalrotation-none&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;principal(rotation = “none”)&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traits.principal &amp;lt;- principal(traits, nfactors=nfactors, rotate=&amp;quot;none&amp;quot;, scores=TRUE)
traits.principal&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Principal Components Analysis
## Call: principal(r = traits, nfactors = nfactors, rotate = &amp;quot;none&amp;quot;, scores = TRUE)
## Standardized loadings (pattern matrix) based upon correlation matrix
##      PC1   PC2   PC3   h2   u2 com
## a1 -0.63  0.24  0.42 0.63 0.37 2.1
## a2 -0.72  0.12  0.50 0.77 0.23 1.8
## a3 -0.65  0.26  0.55 0.79 0.21 2.3
## a4  0.73 -0.23 -0.49 0.83 0.17 2.0
## b1  0.14  0.75 -0.19 0.62 0.38 1.2
## b2  0.09  0.83 -0.16 0.71 0.29 1.1
## b3  0.10  0.89 -0.16 0.83 0.17 1.1
## b4 -0.12 -0.88  0.15 0.81 0.19 1.1
## c1  0.64  0.04  0.50 0.66 0.34 1.9
## c2  0.77  0.10  0.42 0.78 0.22 1.6
## c3  0.70  0.08  0.54 0.79 0.21 1.9
## c4 -0.80 -0.04 -0.49 0.88 0.12 1.7
## 
##                        PC1  PC2  PC3
## SS loadings           4.05 3.02 2.03
## Proportion Var        0.34 0.25 0.17
## Cumulative Var        0.34 0.59 0.76
## Proportion Explained  0.45 0.33 0.22
## Cumulative Proportion 0.45 0.78 1.00
## 
## Mean item complexity =  1.6
## Test of the hypothesis that 3 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0.05 
##  with the empirical chi square  33.59  with prob &amp;lt;  0.44 
## 
## Fit based upon off diagonal values = 0.98&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scores.principal &amp;lt;- traits.principal$scores&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(scores.principal, traits) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(var1 = rownames(.)) %&amp;gt;%
  gather(&amp;quot;var2&amp;quot;, &amp;quot;value&amp;quot;, a1:c4) %&amp;gt;%
  mutate(var1 = factor(var1), var1 = factor(var1, levels = rev(levels(var1)))) %&amp;gt;%
  ggplot(aes(var2, var1, fill=value)) +
  geom_tile() +
  scale_x_discrete(position = &amp;quot;top&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
  scale_fill_viridis(limits=c(-1, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/pca_files/figure-html/pca-principal-none-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;principalrotation-varimax&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;principal(rotation = “varimax”)&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traits.varimax &amp;lt;- principal(traits, nfactors=nfactors, rotate=&amp;quot;varimax&amp;quot;, scores=TRUE)
traits.varimax&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Principal Components Analysis
## Call: principal(r = traits, nfactors = nfactors, rotate = &amp;quot;varimax&amp;quot;, 
##     scores = TRUE)
## Standardized loadings (pattern matrix) based upon correlation matrix
##      RC1   RC3   RC2   h2   u2 com
## a1 -0.15  0.78  0.06 0.63 0.37 1.1
## a2 -0.16  0.86 -0.09 0.77 0.23 1.1
## a3 -0.07  0.89  0.05 0.79 0.21 1.0
## a4  0.17 -0.90 -0.02 0.83 0.17 1.1
## b1  0.03 -0.05  0.79 0.62 0.38 1.0
## b2  0.02  0.03  0.84 0.71 0.29 1.0
## b3  0.03  0.05  0.91 0.83 0.17 1.0
## b4 -0.05 -0.03 -0.90 0.81 0.19 1.0
## c1  0.81 -0.08  0.00 0.66 0.34 1.0
## c2  0.85 -0.21  0.09 0.78 0.22 1.1
## c3  0.88 -0.09  0.04 0.79 0.21 1.0
## c4 -0.92  0.20 -0.02 0.88 0.12 1.1
## 
##                        RC1  RC3  RC2
## SS loadings           3.09 3.03 2.98
## Proportion Var        0.26 0.25 0.25
## Cumulative Var        0.26 0.51 0.76
## Proportion Explained  0.34 0.33 0.33
## Cumulative Proportion 0.34 0.67 1.00
## 
## Mean item complexity =  1
## Test of the hypothesis that 3 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0.05 
##  with the empirical chi square  33.59  with prob &amp;lt;  0.44 
## 
## Fit based upon off diagonal values = 0.98&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scores.varimax &amp;lt;- traits.varimax$scores&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(scores.varimax, traits) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(var1 = rownames(.)) %&amp;gt;%
  gather(&amp;quot;var2&amp;quot;, &amp;quot;value&amp;quot;, a1:c4) %&amp;gt;%
  mutate(var1 = factor(var1), var1 = factor(var1, levels = rev(levels(var1)))) %&amp;gt;%
  ggplot(aes(var2, var1, fill=value)) +
  geom_tile() +
  scale_x_discrete(position = &amp;quot;top&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
  scale_fill_viridis(limits=c(-1, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/pca_files/figure-html/pca-principal-varimax-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;here-are-some-other-functions-for-pcafactor-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Here are some other functions for PCA/Factor Analysis&lt;/h2&gt;
&lt;div id=&#34;princomp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;princomp()&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traits.princomp &amp;lt;- princomp(traits)
traits.princomp$loadings[,1:nfactors]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Comp.1       Comp.2      Comp.3
## a1  0.33006521  0.181709692  0.45906811
## a2  0.29151833  0.067518916  0.38086462
## a3  0.23950762  0.132057875  0.38289101
## a4 -0.25823681 -0.112938603 -0.32926216
## b1 -0.10631062  0.507810878 -0.12639281
## b2 -0.07370167  0.500188081 -0.09771759
## b3 -0.06894907  0.491029731 -0.08032005
## b4  0.07460353 -0.426344256  0.07374535
## c1 -0.44100481 -0.021914387  0.38825383
## c2 -0.42646359  0.010511567  0.23488306
## c3 -0.35757417 -0.001802824  0.29574993
## c4  0.38827130  0.026358392 -0.24163371&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scores.princomp &amp;lt;- traits.princomp$scores[,1:nfactors]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(scores.princomp, traits) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(var1 = rownames(.)) %&amp;gt;%
  gather(&amp;quot;var2&amp;quot;, &amp;quot;value&amp;quot;, a1:c4) %&amp;gt;%
  mutate(var1 = factor(var1), var1 = factor(var1, levels = rev(levels(var1)))) %&amp;gt;%
  ggplot(aes(var2, var1, fill=value)) +
  geom_tile() +
  scale_x_discrete(position = &amp;quot;top&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
  scale_fill_viridis(limits=c(-1, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/pca_files/figure-html/pca-princomp-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;factanalrotation-none&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;factanal(rotation = “none”)&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traits.fa &amp;lt;- factanal(traits, nfactors, rotation=&amp;quot;none&amp;quot;, scores=&amp;quot;regression&amp;quot;)
print(traits.fa, digits=2, cutoff=0, sort=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## factanal(x = traits, factors = nfactors, scores = &amp;quot;regression&amp;quot;,     rotation = &amp;quot;none&amp;quot;)
## 
## Uniquenesses:
##   a1   a2   a3   a4   b1   b2   b3   b4   c1   c2   c3   c4 
## 0.52 0.32 0.26 0.18 0.53 0.40 0.18 0.23 0.50 0.27 0.32 0.08 
## 
## Loadings:
##    Factor1 Factor2 Factor3
## a1 -0.46    0.26    0.45  
## a2 -0.55    0.17    0.60  
## a3 -0.47    0.32    0.64  
## a4  0.57   -0.30   -0.64  
## b1  0.09    0.63   -0.25  
## b2  0.07    0.74   -0.21  
## b3  0.08    0.87   -0.24  
## b4 -0.10   -0.84    0.24  
## c1  0.66    0.03    0.25  
## c2  0.83    0.09    0.19  
## c3  0.77    0.08    0.29  
## c4 -0.92   -0.05   -0.28  
## 
##                Factor1 Factor2 Factor3
## SS loadings       3.65    2.71    1.86
## Proportion Var    0.30    0.23    0.16
## Cumulative Var    0.30    0.53    0.68
## 
## Test of the hypothesis that 3 factors are sufficient.
## The chi square statistic is 22.21 on 33 degrees of freedom.
## The p-value is 0.923&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scores.fa &amp;lt;- traits.fa$scores&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(scores.fa, traits) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(var1 = rownames(.)) %&amp;gt;%
  gather(&amp;quot;var2&amp;quot;, &amp;quot;value&amp;quot;, a1:c4) %&amp;gt;%
  mutate(var1 = factor(var1), var1 = factor(var1, levels = rev(levels(var1)))) %&amp;gt;%
  ggplot(aes(var2, var1, fill=value)) +
  geom_tile() +
  scale_x_discrete(position = &amp;quot;top&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
  scale_fill_viridis(limits=c(-1, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/pca_files/figure-html/pca-factanal-none-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;factanalrotation-varimax&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;factanal(rotation = “varimax”)&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traits.fa.vm &amp;lt;- factanal(traits, nfactors, rotation=&amp;quot;varimax&amp;quot;, scores=&amp;quot;regression&amp;quot;)
print(traits.fa.vm, digits=2, cutoff=0, sort=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## factanal(x = traits, factors = nfactors, scores = &amp;quot;regression&amp;quot;,     rotation = &amp;quot;varimax&amp;quot;)
## 
## Uniquenesses:
##   a1   a2   a3   a4   b1   b2   b3   b4   c1   c2   c3   c4 
## 0.52 0.32 0.26 0.18 0.53 0.40 0.18 0.23 0.50 0.27 0.32 0.08 
## 
## Loadings:
##    Factor1 Factor2 Factor3
## a1 -0.16    0.67    0.06  
## a2 -0.18    0.80   -0.08  
## a3 -0.08    0.85    0.05  
## a4  0.17   -0.89   -0.03  
## b1  0.02   -0.04    0.68  
## b2  0.03    0.04    0.77  
## b3  0.04    0.05    0.91  
## b4 -0.05   -0.03   -0.87  
## c1  0.70   -0.10    0.00  
## c2  0.82   -0.21    0.09  
## c3  0.82   -0.11    0.04  
## c4 -0.94    0.19   -0.02  
## 
##                Factor1 Factor2 Factor3
## SS loadings       2.82    2.73    2.67
## Proportion Var    0.23    0.23    0.22
## Cumulative Var    0.23    0.46    0.68
## 
## Test of the hypothesis that 3 factors are sufficient.
## The chi square statistic is 22.21 on 33 degrees of freedom.
## The p-value is 0.923&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scores.fa.vm &amp;lt;- traits.fa.vm$scores&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(scores.fa.vm, traits) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(var1 = rownames(.)) %&amp;gt;%
  gather(&amp;quot;var2&amp;quot;, &amp;quot;value&amp;quot;, a1:c4) %&amp;gt;%
  mutate(var1 = factor(var1), var1 = factor(var1, levels = rev(levels(var1)))) %&amp;gt;%
  ggplot(aes(var2, var1, fill=value)) +
  geom_tile() +
  scale_x_discrete(position = &amp;quot;top&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
  scale_fill_viridis(limits=c(-1, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/pca_files/figure-html/pca-factanal-varimax-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-do-they-compare&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How do they compare?&lt;/h2&gt;
&lt;p&gt;Here, I’ll plot the absolute value of all the correlations (since the sign on factors/PCs is arbitrary).&lt;/p&gt;
&lt;p&gt;The functions principal(rotation = “varimax”) and factanal(rotation = “varimax”) are nearly (but not perfectly) identical.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scores.fa &amp;lt;- traits.fa$scores
colnames(scores.principal) &amp;lt;- c(&amp;quot;principal() 1&amp;quot;, &amp;quot;principal() 2&amp;quot;, &amp;quot;principal() 3&amp;quot;)
colnames(scores.varimax) &amp;lt;- c(&amp;quot;principal(vm) 1&amp;quot;, &amp;quot;principal(vm) 2&amp;quot;, &amp;quot;principal(vm) 3&amp;quot;)
colnames(scores.princomp) &amp;lt;- c(&amp;quot;princomp() 1&amp;quot;, &amp;quot;princomp() 2&amp;quot;, &amp;quot;princomp() 3&amp;quot;)
colnames(scores.fa) &amp;lt;- c(&amp;quot;factanal() 1&amp;quot;, &amp;quot;factanal() 2&amp;quot;, &amp;quot;factanal() 3&amp;quot;)
colnames(scores.fa.vm) &amp;lt;- c(&amp;quot;factanal(vm) 1&amp;quot;, &amp;quot;factanal(vm) 2&amp;quot;, &amp;quot;factanal(vm) 3&amp;quot;)

cbind(
  scores.princomp,
  scores.principal,
  scores.fa,
  scores.varimax,
  scores.fa.vm
) %&amp;gt;% 
  cor() %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(var1 = rownames(.)) %&amp;gt;%
  gather(&amp;quot;var2&amp;quot;, &amp;quot;value&amp;quot;, 1:15) %&amp;gt;%
  mutate(var1 = factor(var1), var1 = factor(var1, levels = rev(levels(var1)))) %&amp;gt;%
  mutate(value = abs(value)) %&amp;gt;%
  ggplot(aes(var2, var1, fill=value)) +
  geom_tile() +
  scale_x_discrete(position = &amp;quot;top&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
  scale_fill_viridis(limits=c(0, 1)) +
  geom_hline(yintercept = 3.5, color=&amp;quot;white&amp;quot;) +
  geom_hline(yintercept = 6.5, color=&amp;quot;white&amp;quot;) +
  geom_hline(yintercept = 9.5, color=&amp;quot;white&amp;quot;) +
  geom_hline(yintercept = 12.5, color=&amp;quot;white&amp;quot;) +
  geom_vline(xintercept = 3.5, color=&amp;quot;white&amp;quot;) +
  geom_vline(xintercept = 6.5, color=&amp;quot;white&amp;quot;) +
  geom_vline(xintercept = 9.5, color=&amp;quot;white&amp;quot;) +
  geom_vline(xintercept = 12.5, color=&amp;quot;white&amp;quot;) +
  theme(axis.text.x=element_text(angle=90,hjust=1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/posts/pca_files/figure-html/pca-compare-all-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Coding Schemes</title>
      <link>https://debruine.github.io/post/coding-schemes/</link>
      <pubDate>Fri, 23 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/coding-schemes/</guid>
      <description>
&lt;script src=&#34;https://debruine.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lmerTest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How you choose to code categorical variables changes how you can interpret the intercept and effects of those variables. My favourite &lt;a href=&#34;http://talklab.psy.gla.ac.uk/tvw/catpred/&#34;&gt;tutorial on coding schemes&lt;/a&gt; explains things in detail. I’m just adding some concrete examples below.&lt;/p&gt;
&lt;p&gt;First, I simulated a data frame of 100 raters rating 100 faces each. Female faces get ratings with a mean of 6; male faces get ratings with a mean of 5 (I know, ratings are usually ordinal integers, but let’s pretend we used something like a slider). To simulate random effects, both raters and faces have random intercepts with SDs of 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(555)  # for reproducibility; delete when running simulations

n_raters &amp;lt;- 100
n_faces &amp;lt;- 100

female_mean &amp;lt;- 6
male_mean &amp;lt;- 5

raters &amp;lt;- tibble(
  rater_id = 1:n_raters,
  rater_i = rnorm(n_raters)
)

faces &amp;lt;- tibble(
  face_id = 1:n_faces,
  face_i = rnorm(n_faces),
  face_sex = rep(c(&amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;), each = n_faces/2)
)

df &amp;lt;- expand.grid(
  face_id = faces$face_id,
  rater_id = raters$rater_id
) %&amp;gt;%
  left_join(faces, by = &amp;quot;face_id&amp;quot;) %&amp;gt;%
  left_join(raters, by = &amp;quot;rater_id&amp;quot;) %&amp;gt;%
  mutate(
    face_sex_i = ifelse(face_sex==&amp;quot;male&amp;quot;, male_mean, female_mean),
    error = rnorm(nrow(.)),
    rating = face_i + rater_i + face_sex_i + error
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Calculate the means and SDs of the female and male faces.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;face_sex&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SD&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.940&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.767&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.114&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.649&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Always graph your data to confirm you simulated it correctly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;% 
  ggplot(aes(face_sex, rating)) + 
  geom_violin() +
  geom_boxplot(width=0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://debruine.github.io/post/coding_files/figure-html/coding-orig-data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Recode face sex using treatment, sum, or effect coding.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df2 &amp;lt;- df %&amp;gt;%
  mutate(
    face_sex.tr = recode(face_sex, &amp;quot;female&amp;quot; = 1, &amp;quot;male&amp;quot; = 0),
    face_sex.sum = recode(face_sex, &amp;quot;female&amp;quot; = -1, &amp;quot;male&amp;quot; = 1),
    face_sex.e = recode(face_sex, &amp;quot;female&amp;quot; = -0.5, &amp;quot;male&amp;quot; = 0.5)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we analyse the data using each of the 4 styles of coding. I’m just going to show the table of fixed effects.&lt;/p&gt;
&lt;div id=&#34;categorical-coding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Categorical coding&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1 &amp;lt;- lmerTest::lmer(rating ~ face_sex + 
                       (1 | face_id) + 
                       (1 + face_sex | rater_id), 
                     data = df2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|t|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.940&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.174&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;173.360&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.080&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;face_sexmale&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.826&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.203&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;98.586&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-4.069&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note that the intercept coefficient is equal to the female mean (5.94) and the effect of face sex is how much less the male mean is (5.114 - 5.94 = -0.826).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;treatment-coding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Treatment coding&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m.tr &amp;lt;- lmerTest::lmer(rating ~ face_sex.tr + 
               (1 | face_id) + 
               (1 + face_sex.tr | rater_id), 
             data = df2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|t|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.114&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.172&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;169.515&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29.720&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;face_sex.tr&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.826&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.203&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;98.611&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.069&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Treatment coding is the same as categorical coding, but gives you more control over what the reference category is. Here, the reference category is &lt;code&gt;male&lt;/code&gt; and the “treatment” category is &lt;code&gt;female&lt;/code&gt;, so the intercept coefficient is equal to the male mean (5.114) and the effect of face sex is how much more the female mean is (5.94 - 5.114 = 0.826).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sum-coding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sum coding&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m.sum &amp;lt;- lmerTest::lmer(rating ~ face_sex.sum + 
                (1 | face_id) + 
                (1 + face_sex.sum | rater_id), 
              data = df2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|t|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.527&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.140&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;194.675&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39.387&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;face_sex.sum&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.413&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.102&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;98.601&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-4.069&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With sum coding, the intercept coefficient is equal to the overall mean ignoring face sex (i.e., (5.94 + 5.114)/2 = 5.527) and the effect of face sex is how much above and below that each of the two face sexes differ from the mean (i.e., (5.94 - 5.114)/2 = 0.413).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;effect-coding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Effect coding&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m.e &amp;lt;- lmerTest::lmer(rating ~ face_sex.e + 
              (1 | face_id) + 
              (1 + face_sex.e | rater_id), 
            data = df2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Std. Error&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;t value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Pr(&amp;gt;|t|)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;(Intercept)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.527&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.140&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;194.683&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39.387&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;face_sex.e&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.826&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.203&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;98.604&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-4.069&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With effect coding, the intercept coefficient is the same as sum coding and the effect of face sex is how much the two face sexes differ from each other (i.e., 5.94 - 5.114 = 0.826). Note that this coefficient is double that from the sum coding.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>scale()</title>
      <link>https://debruine.github.io/post/scale/</link>
      <pubDate>Thu, 22 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/scale/</guid>
      <description>


&lt;p&gt;
You can use &lt;code&gt;scale()&lt;/code&gt; to center and/or scale (i.e., Z-score) a vector of numbers.
&lt;/p&gt;
&lt;p&gt;Z-score a list of numbers&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(10, 12, 14, 16, 18)
scale(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]
## [1,] -1.2649111
## [2,] -0.6324555
## [3,]  0.0000000
## [4,]  0.6324555
## [5,]  1.2649111
## attr(,&amp;quot;scaled:center&amp;quot;)
## [1] 14
## attr(,&amp;quot;scaled:scale&amp;quot;)
## [1] 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, the result contains the mean and SD.
This can cause problems if you want to assign it to a new column in a data frame,
which you can fix using &lt;code&gt;as.vector()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.vector(scale(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1.2649111 -0.6324555  0.0000000  0.6324555  1.2649111&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I find it more straightforward to just use the equation for a Z-score&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;( x - mean(x) ) / sd(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1.2649111 -0.6324555  0.0000000  0.6324555  1.2649111&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can just center the numbers without scaling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.vector(scale(x, center=TRUE, scale=FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -4 -2  0  2  4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;( x - mean(x) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -4 -2  0  2  4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Scaling without centering divides numbers by their root mean square.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.vector(scale(x, center=FALSE, scale=TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6262243 0.7514691 0.8767140 1.0019589 1.1272037&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x / sqrt(sum(x^2)/(length(x)-1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6262243 0.7514691 0.8767140 1.0019589 1.1272037&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Set the scale to a number to divide by that number&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.vector(scale(x, center=FALSE, scale=3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.333333 4.000000 4.666667 5.333333 6.000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x / 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.333333 4.000000 4.666667 5.333333 6.000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create new columns in a dataframe with the scaled or centered variable&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages( library(tidyverse) )
df &amp;lt;- data.frame(id = seq(1,5), x = x)
df.s &amp;lt;- df %&amp;gt;%
  mutate(
    x.s = as.vector(scale(x)),
    x.c = as.vector(scale(x, scale=F)),
    x.z = (x - mean(x)) / sd(x)
  )
df.s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id  x        x.s x.c        x.z
## 1  1 10 -1.2649111  -4 -1.2649111
## 2  2 12 -0.6324555  -2 -0.6324555
## 3  3 14  0.0000000   0  0.0000000
## 4  4 16  0.6324555   2  0.6324555
## 5  5 18  1.2649111   4  1.2649111&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>rep()</title>
      <link>https://debruine.github.io/post/rep/</link>
      <pubDate>Wed, 21 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/post/rep/</guid>
      <description>


&lt;p&gt;&lt;code&gt;rep(x, times = 1, length.out = NA, each = 1)&lt;/code&gt; is pretty useful for simulating data. Here are some common recipes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Repeat a single number (1) a number of times (10)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(1, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 1 1 1 1 1 1 1 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Repeat a series of numbers (1:5) a number of times (2)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(1:5, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 2 3 4 5 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(1:5, times=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 2 3 4 5 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;If &lt;code&gt;times&lt;/code&gt; is not an integer, it is truncated (not rounded).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(1:5, times=2.9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 2 3 4 5 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Repeat a series of numbers (1:5) a number of times each (2)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(1:5, each=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 1 2 2 3 3 4 4 5 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(1:5, 1, NA, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 1 2 2 3 3 4 4 5 5&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Repeat a list of numbers (0, 3, 6) a number of times (2)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(c(0, 3, 6), times=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0 3 6 0 3 6&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Repeat a list of strings (“a”, “b”, “c”) a number of times (2)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;), times=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot; &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Repeat a list of strings (“a”, “b”, “c”) a number of times each (2)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;), each=2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot; &amp;quot;c&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Repeat a list of strings (“a”, “b”) a number of times each (2) a number of times (3)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;), each=2, times=3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;a&amp;quot; &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;b&amp;quot; &amp;quot;a&amp;quot; &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;b&amp;quot; &amp;quot;a&amp;quot; &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;b&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Repeat a series of numbers (1:5) until you have a specific total (12)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(1:5, length.out=12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 2 3 4 5 1 2 3 4 5 1 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep_len(1:5, 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 2 3 4 5 1 2 3 4 5 1 2&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;length.out&lt;/code&gt; overrides &lt;code&gt;times&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(1:5, length.out=12, times=500)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 2 3 4 5 1 2 3 4 5 1 2&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Repeat a sequence of numbers (0:10 by 5s) a number of times (3)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(seq(0, 10, by=5), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  0  5 10  0  5 10  0  5 10&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://debruine.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coding Club</title>
      <link>https://debruine.github.io/project/shiny/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/project/shiny/</guid>
      <description>&lt;p&gt;Do you want to take your R coding skills to the next level in a fun atmosphere where we learn by doing? Join the Coding Club!&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll be starting from the very basics to use the shiny R package to create an interactive web app that can read from a Google Form and display tables and plots describing your data.&lt;/p&gt;
&lt;p&gt;When: Every other Wednesday 14:00-15:00, starting 6 October 2021.&lt;/p&gt;
&lt;h3 id=&#34;faq&#34;&gt;FAQ&lt;/h3&gt;
&lt;h4 id=&#34;do-i-have-enough-experience&#34;&gt;Do I have enough experience?&lt;/h4&gt;
&lt;p&gt;Don&amp;rsquo;t worry if you&amp;rsquo;re new to R coding; as long as you can open RStudio and load the shiny package, you&amp;rsquo;ll be able to participate. All levels are welcome, including undergraduate students and staff.&lt;/p&gt;
&lt;h4 id=&#34;will-it-be-recorded&#34;&gt;Will it be recorded?&lt;/h4&gt;
&lt;p&gt;Yes, if you&amp;rsquo;re staff or a student at the University of Glasgow, join the Methods &amp;amp; Metascience Team with the code j2z7m4w in Teams to access recordings on the Coding Club channel.&lt;/p&gt;
&lt;h4 id=&#34;what-computing-resources-do-i-need&#34;&gt;What computing resources do I need?&lt;/h4&gt;
&lt;p&gt;You need to have access to R and RStudio. We will use these packages to start: shiny, shinydashboard, tidyverse. See &lt;a href=&#34;https://psyteachr.github.io/reprores-v2/installingr.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;instructions for installing on your own computer&lt;/a&gt; or use a university computer or &lt;a href=&#34;https://rstudio.cloud/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RStudio Cloud&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;i-have-more-questions&#34;&gt;I have more questions&lt;/h4&gt;
&lt;p&gt;Join the Methods &amp;amp; MetaScience Team (it&amp;rsquo;s open to all at UofG) with the join code j2z7m4w in Teams and post questions in the Coding Club channel or email &lt;a href=&#34;mailto:Lisa.DeBruine@glasgow.ac.uk&#34;&gt;Lisa.DeBruine@glasgow.ac.uk&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experimentum</title>
      <link>https://debruine.github.io/project/experimentum/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/project/experimentum/</guid>
      <description>&lt;p&gt;Experimentum is an open-source, online platform for psychology studies.&lt;/p&gt;
&lt;p&gt;Lisa DeBruine, Rebecca Lai, Benedict Jones, Rifah Abdullah, Gaby Mahrholz. (2020). Experimentum (Version v.0.2). Zenodo. doi:10.5281/zenodo.2634355&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Psychological Science Accelerator</title>
      <link>https://debruine.github.io/project/psa/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/project/psa/</guid>
      <description>&lt;p&gt;The PSA is a globally distributed network of psychological science laboratories that coordinates data collection for democratically selected studies.&lt;/p&gt;
&lt;p&gt;I was a founding member and was elected to a 3-year term as an Associate Director in 2021. I also was one of the lead researchers on the first project.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>psyTeachR</title>
      <link>https://debruine.github.io/project/psyteachr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://debruine.github.io/project/psyteachr/</guid>
      <description>&lt;p&gt;The psyTeachR team at the University of Glasgow School of Psychology and Institute of Neuroscience and Psychology has successfully made the transition to teaching reproducible research using R across all undergraduate and postgraduate levels. Our curriculum now emphasizes essential ‘data science’ graduate skills that have been overlooked in traditional approaches to teaching, including programming skills, data visualisation, data wrangling and reproducible reports. Students learn about probability and inference through data simulation as well as by working with real datasets.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
